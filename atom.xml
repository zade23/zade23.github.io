<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ANdRoid&#39;s BLOG</title>
  
  <subtitle>MaTRix</subtitle>
  <link href="https://zade23.github.io/atom.xml" rel="self"/>
  
  <link href="https://zade23.github.io/"/>
  <updated>2024-05-27T06:57:20.667Z</updated>
  <id>https://zade23.github.io/</id>
  
  <author>
    <name>Android</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Ilya的论文阅读清单</title>
    <link href="https://zade23.github.io/2024/05/27/Ilya%E7%9A%84%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%B8%85%E5%8D%95/"/>
    <id>https://zade23.github.io/2024/05/27/Ilya%E7%9A%84%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%B8%85%E5%8D%95/</id>
    <published>2024-05-27T06:35:39.000Z</published>
    <updated>2024-05-27T06:57:20.667Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="1 背景介绍"></a>1 背景介绍</h2><p>Ilya Sutskever 的论文清单：<a href="https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE?source=post_page-----3bd4c9f8e927--------------------------------">30u30</a></p><p>Ilya Sutskever 是 Hinton 的大弟子，OpenAI的联合创始人兼首席科学家。</p><p>以下是他推荐的论文清单，他认为阅读完这些内容之后就可以了解AI领域90%的内容</p><h2 id="2-主要内容"><a href="#2-主要内容" class="headerlink" title="2 主要内容"></a>2 主要内容</h2><h3 id="2-1-核心神经网络创新"><a href="#2-1-核心神经网络创新" class="headerlink" title="2.1 核心神经网络创新"></a>2.1 核心神经网络创新</h3><ol><li><a href="https://arxiv.org/abs/1409.2329?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Recurrent Neural Network Regularization</a> - Enhancement to LSTM units for better overfitting prevention.<br>递归神经网络正则化 - 增强了 LSTM 单元，以更好地防止过拟合。</li><li><a href="https://arxiv.org/abs/1506.03134?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Pointer Networks</a> - Novel architecture for solving problems with discrete token outputs.<br> Pointer Networks - 用于解决离散令牌输出问题的新颖架构.</li><li><a href="https://arxiv.org/abs/1512.03385?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Deep Residual Learning for Image Recognition</a> - Improvements for training very deep networks through residual learning.<br> 用于图像识别的深度残差学习 - 改进了通过残差学习训练非常深度的网络。</li><li><a href="https://arxiv.org/abs/1603.05027?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Identity Mappings in Deep Residual Networks</a> - Enhancements to deep residual networks through identity mappings.<br> 深度残差网络中的身份映射 - 通过身份映射增强深度残差网络。</li><li><a href="https://arxiv.org/abs/1512.02595?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Neural Turing Machines</a> - Combining neural networks with external memory resources for enhanced algorithmic tasks.<ul><li>将神经网络与外部内存资源相结合，以增强算法任务。</li></ul></li><li><a href="https://arxiv.org/abs/1706.03762?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Attention Is All You Need</a> - Introducing the Transformer architecture solely based on attention mechanisms.<br> Attention Is All You Need - 介绍完全基于注意力机制的 Transformer 架构。</li></ol><h3 id="2-2-专业神经网络应用"><a href="#2-2-专业神经网络应用" class="headerlink" title="2.2 专业神经网络应用"></a>2.2 专业神经网络应用</h3><ul><li><a href="https://arxiv.org/abs/1511.07122?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Multi-Scale Context Aggregation by Dilated Convolutions</a> - A convolutional network module for better semantic segmentation.<br> Multi-Scale Context Aggregation by Dilated Convolutions - 一个卷积网络模块，用于更好的语义分割.</li><li><a href="https://arxiv.org/abs/1409.0473?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Neural Machine Translation by Jointly Learning to Align and Translate</a> - A model improving translation by learning to align and translate concurrently.<br> Neural Machine Translation by Joint Learning to Align and Translation - 一种通过学习同时对齐和翻译来改进翻译的模型.</li><li><a href="https://arxiv.org/abs/1704.01212?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Neural Message Passing for Quantum Chemistry</a> - A framework for learning on molecular graphs for quantum chemistry.<br> Neural Message Passing for Quantum Chemistry - 量子化学分子图学习框架.</li><li><a href="https://arxiv.org/abs/1806.01822?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Relational RNNs</a> - Enhancement to standard memory architectures integrating relational reasoning capabilities.Theoretical and Principled Approaches<br> 关系 RNN - 增强了集成关系推理功能的标准内存架构。理论和原则方法</li><li><a href="https://arxiv.org/abs/1512.02595?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</a> - Deep learning system for speech recognition.<br> Deep Speech 2： End-to-End Speech Recognition in English and Mandarin -用于语音识别的深度学习系统.</li><li><a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">ImageNet Classification with Deep CNNs</a> - Convolutional neural network for classifying large-scale image data.<br> ImageNet Classification with Deep CNNs - 用于对大规模图像数据进行分类的卷积神经网络.</li><li><a href="https://arxiv.org/abs/1611.02731?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Variational Lossy Autoencoder</a> - Combines VAEs and autoregressive models for improved image synthesis.<br> 变分有损自动编码器 - 结合 VAE 和自回归模型以改进图像合成。</li><li><a href="https://arxiv.org/abs/1706.01427?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">A Simple NN Module for Relational Reasoning</a> - A neural module designed to improve relational reasoning in AI tasks.<br> 用于关系推理的简单神经网络模块 - 旨在改进 AI 任务中关系推理的神经模块.</li></ul><h3 id="2-3-理论见解和原则性方法"><a href="#2-3-理论见解和原则性方法" class="headerlink" title="2.3 理论见解和原则性方法"></a>2.3 理论见解和原则性方法</h3><ul><li><a href="https://arxiv.org/abs/1511.06391?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Order Matters: Sequence to sequence for sets</a> - Investigating the impact of data order on model performance.<br> Order Matters： Sequence to sequence for sets - 调查数据顺序对模型性能的影响。</li><li><a href="https://arxiv.org/abs/2001.08361?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Scaling Laws for Neural LMs</a> - Empirical study on the scaling laws of language model performance.<br> Scaling Laws for Neural LMs -语言模型性能的缩放规律的实证研究.</li><li><a href="https://arxiv.org/abs/math/0406077?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">A Tutorial Introduction to the Minimum Description Length Principle</a> - Tutorial on the MDL principle in model selection and inference.<br> 最小描述长度原则教程简介 - 模型选择和推理中的MDL原理教程.</li><li><a href="https://www.cs.toronto.edu/~hinton/absps/colt93.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Keeping Neural Networks Simple by Minimizing the Description Length of the Weights</a> - Method to improve neural network generalization by minimizing weight description length.<br> 通过最小化权重的描述长度来保持神经网络的简单性 - 通过最小化权重描述长度来提高神经网络泛化的方法。</li><li><a href="https://pdfs.semanticscholar.org/e758/b579456545f8691bbadaf26bcd3b536c7172.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Machine Super Intelligence DissertationMachine Super Intelligence Dissertation</a> - Study on optimal behavior of agents in computable environments.<br> 机器 超级 智能 论文Machine Super Intelligence Dissertation - 研究智能体在可计算环境中的最优行为。</li><li><a href="https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">PAGE 434 onwards: Komogrov Complexity</a> - Comprehensive exploration of Kolmogorov complexity, discussing its mathematical foundations and implications for fields like information theory and computational complexity.<br> 第434页起：科莫格罗夫复杂性 - 全面探索柯尔莫戈罗夫复杂性，讨论其数学基础以及对信息论和计算复杂性等领域的影响。</li></ul><h3 id="2-4-跨学科和概念研究"><a href="#2-4-跨学科和概念研究" class="headerlink" title="2.4 跨学科和概念研究"></a>2.4 跨学科和概念研究</h3><ul><li><a href="https://arxiv.org/abs/1405.6903?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton</a> - Study on complexity in closed systems using cellular automata.<br> Quantifying the Rise and Fall of Complexity in Closed Systems： The Coffee Automaton - 使用元胞自动机研究封闭系统中的复杂性.</li></ul><h3 id="2-5-效率和可扩展性技术"><a href="#2-5-效率和可扩展性技术" class="headerlink" title="2.5 效率和可扩展性技术"></a>2.5 效率和可扩展性技术</h3><ul><li><a href="https://arxiv.org/abs/1811.06965?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a> - A method for efficient training of large-scale neural networks.<br> GPipe： Efficient Training of Giant Neural Networks using Pipeline Parallelism - 一种高效训练大规模神经网络的方法.</li></ul><h3 id="2-6-教材和教程"><a href="#2-6-教材和教程" class="headerlink" title="2.6 教材和教程"></a>2.6 教材和教程</h3><ul><li><a href="https://cs231n.github.io/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">CS231n: Convolutional Neural Networks for Visual Recognition</a> - Stanford University course on CNNs for visual recognition.<br> CS231n：用于视觉识别的卷积神经网络 - 斯坦福大学视觉识别 CNN 课程。</li><li><a href="https://nlp.seas.harvard.edu/annotated-transformer/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">The Annotated Transformer</a> - Annotated, line-by-line implementation of the Transformer paper. Code is available <a href="https://github.com/harvardnlp/annotated-transformer/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">here</a>.<br> The Annotated Transformer - Transformer 论文的带注释的逐行实现.代码可在此处获得。</li><li><a href="https://scottaaronson.blog/?p=762&utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">The First Law of Complexodynamics</a> - Blog post discussing the measure of system complexity in computational terms.<br> The First Law of Complexodynamics - 讨论计算术语中系统复杂性度量的博客文章.</li><li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">The Unreasonable Effectiveness of RNNs</a> - Blog post demonstrating the versatility of RNNs.<br> RNN 的不合理有效性 - 展示 RNN 多功能性的博客文章.</li><li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=the-mysterious-ai-reading-list-ilya-sutskever-s-recommendations">Understanding LSTM Networks</a> - Blog post providing a detailed explanation of LSTM networks.<br> 了解 LSTM 网络 - 博客文章提供了 LSTM 网络的详细说明。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-背景介绍&quot;&gt;&lt;a href=&quot;#1-背景介绍&quot; class=&quot;headerlink&quot; title=&quot;1 背景介绍&quot;&gt;&lt;/a&gt;1 背景介绍&lt;/h2&gt;&lt;p&gt;Ilya Sutskever 的论文清单：&lt;a href=&quot;https://arc.net/folder/D</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>【随笔】关于个人成长的思考</title>
    <link href="https://zade23.github.io/2024/05/10/%E3%80%90%E9%9A%8F%E7%AC%94%E3%80%91%E5%85%B3%E4%BA%8E%E4%B8%AA%E4%BA%BA%E6%88%90%E9%95%BF%E7%9A%84%E6%80%9D%E8%80%83/"/>
    <id>https://zade23.github.io/2024/05/10/%E3%80%90%E9%9A%8F%E7%AC%94%E3%80%91%E5%85%B3%E4%BA%8E%E4%B8%AA%E4%BA%BA%E6%88%90%E9%95%BF%E7%9A%84%E6%80%9D%E8%80%83/</id>
    <published>2024-05-10T03:05:52.000Z</published>
    <updated>2024-05-10T07:26:17.273Z</updated>
    
    <content type="html"><![CDATA[<p>标题相关内容未想好，就先空着吧。</p><p>本篇内容是一篇关于学习、成长、阶级跨越的思考……</p><p>起因是这一篇父亲写给女儿的信<a href="https://zhuanlan.zhihu.com/p/463715925">《阶层跨越》</a> 。</p><p>上一次阅读是在2022年，当时也正是因为这篇内容关注了作者。</p><p>就在昨天重新整理知乎关注的过程中，重新理解了一下这篇文章，感觉和上一次阅读有了完全不一样的体会。</p><p>上一次阅读的时候我还是研三在读，对于工作、领导、责任的感受不深。</p><p>现如今已经工作两年，对于之前不理解的内容和当时“感觉已经理解”的内容由有了不一样的体会。</p><p>也是。</p><p>屁股决定脑袋。</p><p>如果以后再看，可能还会有更不同的看法吧。</p><h2 id="观点陈述"><a href="#观点陈述" class="headerlink" title="观点陈述"></a>观点陈述</h2><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/202405101125766.png" width="500" height="600"><p>得益于作者清晰的条理，AI总结的结果也非常清晰。</p><p>我通过这些年的经历结合上面提到的论点做出一些说明。</p><h3 id="情绪稳定"><a href="#情绪稳定" class="headerlink" title="情绪稳定"></a>情绪稳定</h3><p>人们常说的情绪稳定是面对打击和挫折时候的情绪。上面作者提到的“正确面对批评和挫折”，同样在今年黄仁勋于斯坦福的讲话中也举例自己年轻时候的端盘子经历以此来鼓励各位学子勇往直前锲而不舍。</p><p>我想说点和他们不同的，是我从自己身上找到的问题：乐极生悲</p><p>比如对于一些事情过度的期待。如果得不到成果还好，而如果得到了就会沾沾自喜和自己给自己思想放假(“这段时间我真辛苦，休息一下吧”)，殊不知很多事情的成功伴随着团队&#x2F;平台&#x2F;导师等一些列因素。</p><p>这样的后果就是，每一次的小阶段成功之后会有一段时间的“产出真空期”，这是和我对自己设立的阶段目标严重不符的。</p><p>正确的做法应该是：理性客观的总结自己在事件和项目中的贡献，分析自己的改变，为什么可以做到这些改变，自己还有哪些不足并应该如何改正…………短暂的整理心情(平复心情)后，重新上路，追求下一个目标。</p><p>如范仲淹在《岳阳楼记》中所说：“不以物喜不以己悲”</p><h3 id="支持同事和领导"><a href="#支持同事和领导" class="headerlink" title="支持同事和领导"></a>支持同事和领导</h3><p>在研究生期间，导师给了我一个算法证明相关的课题。</p><p>众所周知，证明 + 算法。看似是非常高大上的课题，实则是枯燥乏味且很难获得成就感和反馈的，我很长一段时间都在枯燥的推公式和自我怀疑中度过，不可否认有一大部分的根源来自于能力不足。</p><p>在2019-2021年期间，正是AI相关应用爆发的时间段，也眼馋实验室别的同学随便蹭一蹭AI相关就能发文章的情况，就想找导师更换我自己的课题。</p><p>结果当然是失败了。</p><p>于是就开启了长达两年的纠结过程，做不爱做的课题，并且觉得导师太不通情达理了，自己开始做和课题无关的研究并且在宿舍摆烂(好孩子不要学)</p><p>现在想来，是自己在沟通和处理问题方面不成熟。</p><p>如果现在让重新和导师沟通，我会这么做：和导师商量一个关于该课题的最低标准(发几区、发几篇)，然后表示出自己真正感兴趣的领域，并表面在该领域自己也可以发文章(这是沟通必要条件)。</p><p>当时做错的是逃避了沟通，在“负责任”和“支持领导”这两方面都没有成熟思考。</p><h3 id="口才和表达"><a href="#口才和表达" class="headerlink" title="口才和表达"></a>口才和表达</h3><p>再延伸一点，那就是：争取露脸机会</p><p>在高校和公司这样的地方，成熟的架构有成百上千号人，大佬们没有办法记住所有人。</p><p>最好的办法就是争取露脸机会。</p><p>争取到机会之后，勇敢去表现自己的能力和技术水准。</p><p>在技术人员中，有很大一部分的人是不太擅长表述的，他们的工程能力非常突出，但是表达的通常比较啰嗦。</p><p>前段时间听到过一个观点：人生就是一个巨大的推销过程。</p><p>做汇报、述职、做报告，其实就包含了推销自己的能力这个过程。</p><p>为什么在AI火爆的那段时间内，讲好故事的公司能够拿到融资，那是因为讲好故事就是一种“夸张”的推销表达。</p><p>未来希望自己可以培养的能力：<strong>口才</strong> + <strong>表达</strong> + <strong>把握机会的能力</strong></p><h3 id="做事能力和思维能力"><a href="#做事能力和思维能力" class="headerlink" title="做事能力和思维能力"></a>做事能力和思维能力</h3><p>做事能力：业务能力(基本) + 时间管理 + 团队合作(协作和沟通)</p><p>思维能力：超出当前层级的视角 + 学习领导能力</p><p>多年后重新看这篇文章，最区别的感受是要对于“整体架构”的把控。</p><p>原来自己觉得，只用把自己领域做大做强做精尖就可以了。</p><p>现在觉得这其实是很困难的，互联网的知识更新换代非常的快，永远学不完这个领域里所有的知识。</p><p>比较正确的做法应该是：掌握自己领域内最扎实的能力(业务能力) + 扩展知识的广度。</p><p>比如，要设计web端的AI应用，就需要了解前后端的相关技术，是否应该前后端分离，如果涉及到存储，数据库的选型应该是MySQL这种关系型数据库还是NoSQL类的非关系型数据库……对于上下游工作的了解能够让你和上下游更好的协调沟通(如果愿意的话，做个全栈也未尝不可)。</p><p>并且，对各方面技术栈的了解对于未来想做管理也是有帮助的。</p><p>待补充</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;标题相关内容未想好，就先空着吧。&lt;/p&gt;
&lt;p&gt;本篇内容是一篇关于学习、成长、阶级跨越的思考……&lt;/p&gt;
&lt;p&gt;起因是这一篇父亲写给女儿的信&lt;a href=&quot;https://zhuanlan.zhihu.com/p/463715925&quot;&gt;《阶层跨越》&lt;/a&gt; 。&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>【Git】在README.md中展示视频的方案</title>
    <link href="https://zade23.github.io/2024/04/26/%E3%80%90Git%E3%80%91%E5%9C%A8README-md%E4%B8%AD%E5%B1%95%E7%A4%BA%E8%A7%86%E9%A2%91%E7%9A%84%E6%96%B9%E6%A1%88/"/>
    <id>https://zade23.github.io/2024/04/26/%E3%80%90Git%E3%80%91%E5%9C%A8README-md%E4%B8%AD%E5%B1%95%E7%A4%BA%E8%A7%86%E9%A2%91%E7%9A%84%E6%96%B9%E6%A1%88/</id>
    <published>2024-04-26T03:53:42.000Z</published>
    <updated>2024-04-26T03:58:20.162Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E5%86%85%E5%AE%B9%E4%BB%8B%E7%BB%8D">内容介绍</a></li><li><a href="#%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E">使用说明</a><ul><li><a href="#github">github</a></li><li><a href="#gitlab">gitlab</a></li><li><a href="#blog%E6%94%AF%E6%8C%81%E7%9A%84html%E6%A0%BC%E5%BC%8F">blog支持的html格式</a></li></ul></li><li><a href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5">参考链接</a></li></ul><h2 id="内容介绍"><a href="#内容介绍" class="headerlink" title="内容介绍"></a>内容介绍</h2><p>在 GitHub 和 GitLab 仓库中，通过在 README.md 中展示视频Demo的方式直观的展示项目的效果。</p><p>最后，加上一种HTML形式渲染视频的方案，效果如下：</p><p><video src="https://github.com/zade23/zade23.github.io/assets/53324261/9ef0711b-29eb-4994-b34a-3d9d2152ea9b" controls="controls" width="500" height="300"></video></p><h2 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h2><h3 id="github"><a href="#github" class="headerlink" title="github"></a>github</h3><blockquote><p>注意：<strong>视频不能超过10m</strong></p></blockquote><p>GitHub 上用将视频提交到 issues 中，此时视频文件将上传到GitHub中并获得一个文件链接，这个链接放置在README中可以被直接渲染成视频。</p><ol><li>上传视频文件至某个 GitHub 项目的 issues 中</li><li>复制链接，放置在README中</li></ol><p><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/202404261136499.jpg" alt="上传视频至issues"></p><h3 id="gitlab"><a href="#gitlab" class="headerlink" title="gitlab"></a>gitlab</h3><p>GitLab 可以直接用Markdown图片链接的格式渲染已存放在仓库或 issues 中的视频</p><ol><li>用图片链接的格式直接嵌入视频。<strong>支持格式为 <code>.mp4</code>、<code>.m4v</code>、<code>.mov</code>、<code>.webm</code> 和 <code>.ogv</code></strong></li><li>修改下面代码中视频文件的相对路径</li></ol><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Here&#x27;s a sample video:</span><br><span class="line"></span><br><span class="line">![<span class="string">Sample Video</span>](<span class="link">img/markdown_video.mp4</span>)</span><br></pre></td></tr></table></figure><blockquote><p>嵌入链接同样可以是已经提到issues中的视频文件链接</p></blockquote><h3 id="blog支持的html格式"><a href="#blog支持的html格式" class="headerlink" title="blog支持的html格式"></a>blog支持的html格式</h3><p>先采用提issues的方式或者上传至仓库的方式将文件上传。</p><ol><li>将下面链接中 <code>src</code> 内的部分改成你的链接即可</li></ol><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">video</span> <span class="attr">src</span>=<span class="string">&quot;movie.mp4.mp4&quot;</span> <span class="attr">controls</span>=<span class="string">&quot;controls&quot;</span> <span class="attr">width</span>=<span class="string">&quot;500&quot;</span> <span class="attr">height</span>=<span class="string">&quot;300&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">video</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://docs.gitlab.cn/jh/user/markdown.html#%E8%A7%86%E9%A2%91">极狐文档中心-Markdown-视频</a></li><li><a href="http://t.csdnimg.cn/ehxyq">GitHub 仓库的 README 直接播放视频</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%86%85%E5%AE%B9%E4%BB%8B%E7%BB%8D&quot;&gt;内容介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E&quot;&gt;使用说明&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;</summary>
      
    
    
    
    
    <category term="Git" scheme="https://zade23.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>【Git】一个流畅的开源贡献流程</title>
    <link href="https://zade23.github.io/2024/04/25/%E3%80%90Git%E3%80%91%E4%B8%80%E4%B8%AA%E6%B5%81%E7%95%85%E7%9A%84%E5%BC%80%E6%BA%90%E8%B4%A1%E7%8C%AE%E6%B5%81%E7%A8%8B/"/>
    <id>https://zade23.github.io/2024/04/25/%E3%80%90Git%E3%80%91%E4%B8%80%E4%B8%AA%E6%B5%81%E7%95%85%E7%9A%84%E5%BC%80%E6%BA%90%E8%B4%A1%E7%8C%AE%E6%B5%81%E7%A8%8B/</id>
    <published>2024-04-25T04:03:15.000Z</published>
    <updated>2024-04-28T03:28:58.250Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>由本人的有道云笔记搬运到个人网站，创作日期：2023年3月20日</p></blockquote><ul><li><a href="#%E6%95%99%E7%BB%83%E6%88%91%E6%83%B3%E5%BC%80%E6%BA%90">教练……我想开源</a></li><li><a href="#github%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%E5%B7%A5%E4%BD%9C%E6%B5%81">github开源仓库工作流</a></li><li><a href="#%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E6%83%B3%E8%BF%9B%E8%A1%8C%E5%BC%80%E6%BA%90%E8%B4%A1%E7%8C%AE">如果我们想进行开源贡献</a></li><li><a href="#paddle%E9%A1%B9%E7%9B%AE">Paddle项目</a><ul><li><a href="#%E6%96%87%E6%A1%A3%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97">文档贡献指南</a><ul><li><a href="#%E7%AC%AC%E4%B8%80%E6%AD%A5fork%E4%BB%93%E5%BA%93%E5%B9%B6clone%E5%88%B0%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BD">第一步：Fork仓库并Clone到本地加载</a></li><li><a href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5fork%E4%BB%93%E5%BA%93%E5%B9%B6clone%E5%88%B0%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BD">第二步：Fork仓库并Clone到本地加载</a></li><li><a href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%E4%B8%8B%E8%BD%BDpre-commit%E5%B7%A5%E5%85%B7%E4%B8%8B%E8%BD%BD%E8%BF%87%E5%B0%B1%E4%B8%8D%E7%94%A8%E5%86%8D%E4%B8%8B%E4%BA%86">第三步：下载pre-commit工具（下载过就不用再下了）</a></li><li><a href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5%E5%8D%95%E6%AC%A1%E4%BF%AE%E6%94%B9%E5%B9%B6%E6%8F%90%E4%BA%A4%E9%83%BD%E8%A6%81%E8%BF%90%E8%A1%8Cpre-commit%E8%BF%9B%E8%A1%8C%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95">第四步：单次修改并提交都要运行pre-commit进行单元测试</a><ul><li><a href="#%E6%8F%90%E4%BA%A4commit">提交：commit</a></li></ul></li><li><a href="#%E7%AC%AC%E4%BA%94%E6%AD%A5push">第五步：push</a></li><li><a href="#%E7%AC%AC%E5%85%AD%E6%AD%A5review--merge">第六步：review &amp; merge</a></li><li><a href="#%E5%B0%8F%E6%8F%92%E6%9B%B2checkprtemplate-%E4%B8%80%E5%AE%9A%E8%A6%81%E8%A7%84%E8%8C%83">小插曲：CheckPRTemplate 一定要规范</a></li></ul></li></ul></li><li><a href="#gradio%E9%A1%B9%E7%9B%AE">Gradio项目</a></li><li><a href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5">参考链接</a></li></ul><h2 id="教练……我想开源"><a href="#教练……我想开源" class="headerlink" title="教练……我想开源"></a>教练……我想开源</h2><p>一直想参加一个开源项目，丰富自己GitHub账号头衔的同时还能学到一些知识。目前定位到了两个自己非常感兴趣的项目</p><ol><li>（国内项目）Paddle：百度飞桨深度学习框架</li><li>（国际项目）Gradio：用Python构建机器学习网页APP<br>在项目主页的 Issues 提出自己的问题</li></ol><h2 id="github开源仓库工作流"><a href="#github开源仓库工作流" class="headerlink" title="github开源仓库工作流"></a>github开源仓库工作流</h2><blockquote><p>参考<a href="https://nvie.com/posts/a-successful-git-branching-model/">《A successful Git branching model》</a><br>Remote（main&#x2F;）：所有人都共享的代码仓库</p></blockquote><h2 id="如果我们想进行开源贡献"><a href="#如果我们想进行开源贡献" class="headerlink" title="如果我们想进行开源贡献"></a>如果我们想进行开源贡献</h2><ul><li>第一步就是建立一个“feature branch”。目前在Local上，main branch 和 feature branch 上面的内容都是一致的。当使用checkout命令后，硬盘会同步git指定branch下所有的源文件（切换就会改变硬盘当下空间内部的文件）<br>如何正确提Issues<br> 在提Issues之前，一定要提前用关键字搜索看一下你想提出的问题是否已经存在。确定不存在后再进行提问</li></ul><h2 id="Paddle项目"><a href="#Paddle项目" class="headerlink" title="Paddle项目"></a>Paddle项目</h2><h3 id="文档贡献指南"><a href="#文档贡献指南" class="headerlink" title="文档贡献指南"></a>文档贡献指南</h3><p>以最近的一次合并提交为例：本次提交是添加了Tensor文件夹下的十个说明文档。</p><h4 id="第一步：Fork仓库并Clone到本地加载"><a href="#第一步：Fork仓库并Clone到本地加载" class="headerlink" title="第一步：Fork仓库并Clone到本地加载"></a>第一步：Fork仓库并Clone到本地加载</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/&lt;USERNAME&gt;/docs</span><br><span class="line"><span class="built_in">cd</span> docs</span><br></pre></td></tr></table></figure><h4 id="第二步：Fork仓库并Clone到本地加载"><a href="#第二步：Fork仓库并Clone到本地加载" class="headerlink" title="第二步：Fork仓库并Clone到本地加载"></a>第二步：Fork仓库并Clone到本地加载</h4><p>docs 目前使用 Git 流分支模型进行开发，测试，发行和维护。<br>所有的 feature 和 bug fix 的开发工作都应该在一个新的分支上完成，一般从 develop 分支上创建新分支。<br>使用 <code>git checkout -b</code> 创建并切换到新分支。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git status  <span class="comment">#查看一下当前分支目录是否clean</span></span><br><span class="line">git checkout -b &lt;创建并切换到的分支名称&gt;</span><br></pre></td></tr></table></figure><h4 id="第三步：下载pre-commit工具（下载过就不用再下了）"><a href="#第三步：下载pre-commit工具（下载过就不用再下了）" class="headerlink" title="第三步：下载pre-commit工具（下载过就不用再下了）"></a>第三步：下载pre-commit工具（下载过就不用再下了）</h4><p>一种检查模板格式是否匹配项目格式的预提交工具。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install pre-commit</span><br><span class="line">pre-commit install</span><br></pre></td></tr></table></figure><h4 id="第四步：单次修改并提交都要运行pre-commit进行单元测试"><a href="#第四步：单次修改并提交都要运行pre-commit进行单元测试" class="headerlink" title="第四步：单次修改并提交都要运行pre-commit进行单元测试"></a>第四步：单次修改并提交都要运行pre-commit进行单元测试</h4><ol><li>通过指令 git status 进行状态的检查</li><li>通过 git add 将内容添加进准备提交的区域</li></ol><p><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/1714027390144.jpg" alt="git status"></p><blockquote><p>上面的状态空间中显示我 9 次修改中有 1 次进入了“暂存区”，此时就要使用pre-commit指令进行单元测试</p></blockquote><p><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/1714027493043.jpg" alt="pre-commit"></p><blockquote><p>上面在运行了pre-commit之后，除了红色以外区域显示是通过的部分。红色部分说明没有通过单元测试，这个报错说明Linux系统和Win系统的换行符不统一导致的格式问题，一般这种情况下pre-commit会自己进行修复，所以重新提交就可以解决</p></blockquote><p><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/1714027499063.jpg" alt="pre-commit"></p><blockquote><p>上面在进行了一次性的提交之后，pre-commit一次性将所有的文件进行了检查和修改，之后再进行一次 git add –update 就可以将更新修改后的文件合并至文件的缓存区内。</p></blockquote><p><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/202404251451164.jpg" alt="git add --update"></p><blockquote><p>解决步骤如上</p></blockquote><p><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/202404251451889.jpg" alt="pre-commit"></p><blockquote><p>在进行一次提交之后就可以看到所有单元测试已经通过了</p></blockquote><p>在单元测试全部通过之后，可以正式填写提交说明了：</p><p><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/202404251452892.jpg" alt="git commit"></p><blockquote><p>上面我提出的说明是“修改文档”。显示有 9 个文件被修改了， 20 行新增代码， 22 行删减代码。</p></blockquote><h5 id="提交：commit"><a href="#提交：commit" class="headerlink" title="提交：commit"></a>提交：commit</h5><p><strong>之所以要运行 <code>pre-commit</code>  就是要为后面要执行的commit操作避免提前的冲突和麻烦</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m <span class="string">&quot;fix/add xxxx&quot;</span></span><br></pre></td></tr></table></figure><h4 id="第五步：push"><a href="#第五步：push" class="headerlink" title="第五步：push"></a>第五步：push</h4><p>先确保已经同步过原始仓库的最新代码 <a href="https://github.com/PaddlePaddle/docs">https://github.com/PaddlePaddle/docs</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  git remote</span><br><span class="line">origin</span><br><span class="line">➜  git remote -v</span><br><span class="line">origin  https://github.com/USERNAME/docs (fetch)</span><br><span class="line">origin  https://github.com/USERNAME/docs (push)</span><br><span class="line"></span><br><span class="line"><span class="comment"># origin 是 clone 的远程仓库的名字，也就是自己用户名下的 Paddle，接下来创建一个原始 Paddle 的仓库远程主机，命名为 upstream</span></span><br><span class="line">➜  git remote add upstream https://github.com/PaddlePaddle/docs</span><br><span class="line">➜  git remote</span><br><span class="line">&gt;&gt;  origin</span><br><span class="line">&gt;&gt;  upstream</span><br><span class="line">-----------------------------------之前提交过了就可以跳到这一步---------------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 upstream 的最新代码并更新当前分支</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 upstream 远程仓库获取最新的代码和提交历史，将本地仓库与远程仓库保持同步。</span></span><br><span class="line">➜  git fetch upstream  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 upstream 远程仓库 develop 分支中获取最新的代码和提交历史，并将其自动合并到当前本地分支。</span></span><br><span class="line">➜  git pull upstream develop </span><br><span class="line"></span><br><span class="line"><span class="comment"># Push 到远程仓库：也就是将本地修改推送到 GitHub 即： https://github.com/USERNAME/docs</span></span><br><span class="line"><span class="comment"># 将本地分支的提交推送到名为origin的远程仓库中的特定分支（&lt;my-pr&gt;）。</span></span><br><span class="line"><span class="comment"># 通过执行此命令，您可以将本地分支的修改上传到远程仓库，使得其他协作者可以查看和访问您的提交。</span></span><br><span class="line">➜  git push origin &lt;my-pr(第二步中取的名字)&gt;</span><br></pre></td></tr></table></figure><h4 id="第六步：review-amp-merge"><a href="#第六步：review-amp-merge" class="headerlink" title="第六步：review &amp; merge"></a>第六步：review &amp; merge</h4><blockquote><p>我的第一次提交 <a href="https://github.com/PaddlePaddle/docs/pull/5747">https://github.com/PaddlePaddle/docs/pull/5747</a><br>几点小的总结：</p></blockquote><ul><li>尽量把自己在做的过程中不确定的内容进行说明</li><li>把疑问的地方进行复现</li><li>每一轮的回复都要进行礼貌性的回答</li><li>尽量把每一版的修改都进行分点的说明，第一做了XXX，第二做了XXXX……</li><li></li></ul><h4 id="小插曲：CheckPRTemplate-一定要规范"><a href="#小插曲：CheckPRTemplate-一定要规范" class="headerlink" title="小插曲：CheckPRTemplate 一定要规范"></a>小插曲：CheckPRTemplate 一定要规范</h4><p>如果希望 Paddle 能够接受你的提交，至少需要满足以下三点：</p><ul><li><p>单元测试通过 —— Docs-NEW</p></li><li><p>文档PR格式检测通过 —— CheckPRTemplate</p></li><li><p>同意签署开源贡献者协议 —— license&#x2F;cla<br>一般Docs-NEW可能需要比较长的时间进行检测，偶尔也需要排队等待。所以这次我以为 CheckPRTemplat 也是一样需要等，但是等了整整一天时间……最后发现其实PR格式不是特别正确（虽然也有人格式不正确但通过了PR格式检测），最终参考 <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/dev_guides/git_guides/paddle_ci_manual_cn.html#checkprtemplate">checkprtemplate</a> 。修改后的格式如下：</p><p><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/202404251453294.jpg" alt="Paddle项目开源贡献要求的格式"></p></li></ul><p>修改之后秒过。</p><h2 id="Gradio项目"><a href="#Gradio项目" class="headerlink" title="Gradio项目"></a>Gradio项目</h2><p>一位同学进行的一个非常标准的开源参与流程：</p><ol><li><p>提Issues。在这个过程中以项目管理者角度出发，顺带说明了四点问题：</p><ol><li><strong>是否已存在类似Issue。</strong>I have searched to see if a similar issue already exists.（我已经搜索过是否已经存在类似问题。）</li><li><strong>描述你的问题和想添加的功能。</strong>Is your feature request related to a problem? Please describe.（您的功能请求是否与问题相关？请描述。）</li><li><strong>提出解决方案。</strong>Describe the solution you’d like.（描述您想要的解决方案）</li><li><strong>是否有前情提要。</strong>Additional context.（附加上下文）</li></ol><p> <img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/202404251453976.jpg" alt="提问动机"></p></li><li><p>请求合并Merge。在这个过程中以项目管理者角度出发，顺带说明了四点问题：</p><ol><li>relevant motivation.（相关动机）</li><li>a summary of the change.（总结你做出的更改）</li><li>which issue is fixed.（解决了Issue中的哪一个）</li><li>any additional dependencies that are required for this change.（在本次变更中是否有其他依赖项）</li></ol><p> <img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/202404251454853.jpg" alt="详细描述自己所做的工作"></p></li></ol><hr><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><p>git commit之后，想撤销commit</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --soft HEAD^</span><br></pre></td></tr></table></figure></li><li><p>视频链接</p><ol><li><a href="https://www.bilibili.com/video/BV18T411z7R4/?share_source=copy_web&vd_source=48383e849d4fc23ac2e38278a3df1b87">【如何正确地提github issue？开源项目作者来和你聊聊这个重要技能】</a></li><li><a href="https://www.bilibili.com/video/BV19e4y1q7JJ/?share_source=copy_web&vd_source=48383e849d4fc23ac2e38278a3df1b87">【十分钟学会正确的github工作流，和开源作者们使用同一套流程】</a></li><li><a href="https://www.freecodecamp.org/chinese/news/how-to-make-your-first-pull-request-on-github/">【freeCodeCamp | 如何在 GitHub 提交第一个 pull request】</a></li></ol></li><li><p>AIstudio上持久化安装pip包</p><ol><li>提前运行 <code>cd ~ &amp;&amp; mkdir loca_data &amp;&amp; ln -s local_data.local</code></li></ol></li><li><p><a href="https://www.bilibili.com/video/BV19e4y1q7JJ/?share_source=copy_web&vd_source=48383e849d4fc23ac2e38278a3df1b87">十分钟学会正确的github工作流，和开源作者们使用同一套流程</a></p><blockquote><p>一套开源项目和部分企业都在使用的工作流</p></blockquote></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;由本人的有道云笔记搬运到个人网站，创作日期：2023年3月20日&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%95%99%E7%BB%83%E6%88%91%E6%83%B3%E5%BC%80%E6%BA%90</summary>
      
    
    
    
    <category term="Git Workflow" scheme="https://zade23.github.io/categories/Git-Workflow/"/>
    
    
    <category term="Git" scheme="https://zade23.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>【Unity】破解版安装指北</title>
    <link href="https://zade23.github.io/2024/04/22/%E3%80%90Unity%E3%80%91%E7%A0%B4%E8%A7%A3%E7%89%88%E5%AE%89%E8%A3%85%E6%8C%87%E5%8C%97/"/>
    <id>https://zade23.github.io/2024/04/22/%E3%80%90Unity%E3%80%91%E7%A0%B4%E8%A7%A3%E7%89%88%E5%AE%89%E8%A3%85%E6%8C%87%E5%8C%97/</id>
    <published>2024-04-22T13:27:41.000Z</published>
    <updated>2024-04-22T13:42:52.171Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E4%BB%8B%E7%BB%8D">介绍</a></li><li><a href="#%E7%89%88%E6%9C%AC%E9%80%89%E6%8B%A9">版本选择</a></li><li><a href="#%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E">使用说明</a></li><li><a href="#%E8%B8%A9%E5%9D%91">踩坑</a></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>个人版的Unity安装通过Unity官网一步一步进行即可。<br>今天我们聊的是：如何安装Unity的完整破解版。</p><p>如果没有许可的Unity项目在重新登录之后会提示如下界面：<br><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/A3c6EYT72l.jpg" alt="没有许可"></p><p>在这个状态下，左右两个选项都无法正常启动项目，那么就需要考虑使用个人版或者破解Unity了。</p><blockquote><p>注意：以下内容仅供用于非营利性的教育和测试目的。</p></blockquote><h2 id="版本选择"><a href="#版本选择" class="headerlink" title="版本选择"></a>版本选择</h2><p>我当前使用的版本参数如下：</p><ul><li>Unity：2022.3.9f1</li><li>Unity Hub：3.4.2</li></ul><p>关于Unity破解版这里用到以下两个用于破解软件的链接：</p><ul><li><a href="https://github.com/tylearymf/UniHacker">https://github.com/tylearymf/UniHacker</a></li><li><a href="https://github.com/agentbillwh/unilic">https://github.com/agentbillwh/unilic</a></li></ul><p>两个GitHub工程的对应Unity项目的破解关系是：</p><p><strong>UniHacker → Unity Hub</strong><br><strong>unilic → Unity</strong></p><p>正常情况下UniHacker可以同时破解 Unity 和 Unity Hub，由于UniHacker可以破解的Unity版本不能高于2022.2所以我们另外使用unilic破解Unity。</p><p>这样破解就可以使用较新的Unity版本了。</p><h2 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h2><p>上述链接中的两个工程都是非常详细的工程，按照<code>READ.md</code>内容一步一步来即可，UniHacker更是打包成了一个<code>.exe</code>文件供用户直接破解。</p><p>UniHacker的破解结束后有两个选项：</p><ul><li>是否登录自己的账号：建议<code>否</code></li><li>是否保持更新：建议<code>否</code></li></ul><p>破解成功后，可打开原始项目。<br><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/img_v3_02a6_ede5a60a-bb44-416f-830e-e3843209257g.jpg" alt="Unity，启动"></p><h2 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h2><ol><li><p>Unity Hub 版本不能太高</p><p> 如果只破解了Unity而没有破解Unity Hub(或Unity Hub的版本过高，如图中3.7版)则会在打开项目的时候反复弹窗确认登录信息。</p></li></ol><p><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/img_v3_02a6_71156aec-81f4-4cef-80d0-a9f51738028g.jpg" alt="版本过高"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%BB%8B%E7%BB%8D&quot;&gt;介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E7%89%88%E6%9C%AC%E9%80%89%E6%8B%A9&quot;&gt;版本选择&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%BD%BF</summary>
      
    
    
    
    <category term="Unity" scheme="https://zade23.github.io/categories/Unity/"/>
    
    
    <category term="Game Engine" scheme="https://zade23.github.io/tags/Game-Engine/"/>
    
  </entry>
  
  <entry>
    <title>【NLP】RWKV对比Transformer</title>
    <link href="https://zade23.github.io/2024/04/16/%E3%80%90NLP%E3%80%91RWKV%E5%AF%B9%E6%AF%94Transformer/"/>
    <id>https://zade23.github.io/2024/04/16/%E3%80%90NLP%E3%80%91RWKV%E5%AF%B9%E6%AF%94Transformer/</id>
    <published>2024-04-16T03:25:31.000Z</published>
    <updated>2024-04-16T09:01:32.603Z</updated>
    
    <content type="html"><![CDATA[<h3 id="模型的精度问题"><a href="#模型的精度问题" class="headerlink" title="模型的精度问题"></a>模型的精度问题</h3><p>int4? int8? bf16? bf32? float16? float32?</p><h3 id="模型参数文件"><a href="#模型参数文件" class="headerlink" title="模型参数文件"></a>模型参数文件</h3><p>常见的 <code>generation_config.json</code> 文件</p><p>下面的内容来自 Baichuan2-7B-Chat 模型生成器参数文件</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;pad_token_id&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;bos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;eos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;user_token_id&quot;</span><span class="punctuation">:</span> <span class="number">195</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;assistant_token_id&quot;</span><span class="punctuation">:</span> <span class="number">196</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;max_new_tokens&quot;</span><span class="punctuation">:</span> <span class="number">2048</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;temperature&quot;</span><span class="punctuation">:</span> <span class="number">0.3</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;top_k&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;top_p&quot;</span><span class="punctuation">:</span> <span class="number">0.85</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;repetition_penalty&quot;</span><span class="punctuation">:</span> <span class="number">1.05</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;do_sample&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;transformers_version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4.29.2&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>下面的内容来自 <code>RWKV/v5-Eagle-7B-HF</code> 模型生成器参数文件</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;chat_format&quot;</span><span class="punctuation">:</span> <span class="string">&quot;chatml&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;eos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;pad_token_id&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;max_window_size&quot;</span><span class="punctuation">:</span> <span class="number">4096</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;max_new_tokens&quot;</span><span class="punctuation">:</span> <span class="number">4096</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;do_sample&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;top_k&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;top_p&quot;</span><span class="punctuation">:</span> <span class="number">0.1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;repetition_penalty&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;transformers_version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;4.31.1&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="半精度？全精度？-指令微调？-chat模型？"><a href="#半精度？全精度？-指令微调？-chat模型？" class="headerlink" title="半精度？全精度？ 指令微调？ chat模型？"></a>半精度？全精度？ 指令微调？ chat模型？</h3><p>HF模型？</p><h3 id="微调方式"><a href="#微调方式" class="headerlink" title="微调方式"></a>微调方式</h3><p>指令微调？全量&#x2F;半精度&#x2F;零样本？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;模型的精度问题&quot;&gt;&lt;a href=&quot;#模型的精度问题&quot; class=&quot;headerlink&quot; title=&quot;模型的精度问题&quot;&gt;&lt;/a&gt;模型的精度问题&lt;/h3&gt;&lt;p&gt;int4? int8? bf16? bf32? float16? float32?&lt;/p&gt;
&lt;h3 </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>【NLP】RWKV框架入门</title>
    <link href="https://zade23.github.io/2024/04/15/%E3%80%90NLP%E3%80%91RWKV%E6%A1%86%E6%9E%B6%E5%85%A5%E9%97%A8/"/>
    <id>https://zade23.github.io/2024/04/15/%E3%80%90NLP%E3%80%91RWKV%E6%A1%86%E6%9E%B6%E5%85%A5%E9%97%A8/</id>
    <published>2024-04-15T03:56:48.000Z</published>
    <updated>2024-04-25T07:00:53.080Z</updated>
    
    <content type="html"><![CDATA[<p>全文从以下几个方面展开：</p><ul><li><a href="#rwkv%E6%98%AF%E4%BB%80%E4%B9%88">RWKV是什么</a><ul><li><a href="#%E6%9E%B6%E6%9E%84%E7%BB%84%E6%88%90">架构组成</a></li><li><a href="#%E7%89%B9%E6%AE%8A%E6%80%A7%E5%88%86%E6%9E%90">特殊性分析</a></li></ul></li><li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E5%9E%8Brwkv%E5%81%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E5%9F%BA%E5%BA%95%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E7%A0%94">为什么选型RWKV做大语言基底模型的预研</a></li><li><a href="#rwkv%E5%92%8Ctransformer%E7%9A%84%E5%B7%AE%E5%88%AB">RWKV和Transformer的差别</a></li><li><a href="#%E7%BB%93%E6%9E%84%E5%BB%BA%E6%A8%A1">结构建模</a><ul><li><a href="#self-attention">Self-Attention</a></li><li><a href="#multi-head-attention">Multi-Head Attention</a></li><li><a href="#rwkv%E7%BB%93%E6%9E%84">RWKV结构</a></li><li><a href="#%E6%95%88%E6%9E%9C%E8%A1%A8%E7%8E%B07b">效果表现（7B）</a></li><li><a href="#%E7%BB%93%E8%AF%AD">结语</a></li></ul></li></ul><hr><h2 id="RWKV是什么"><a href="#RWKV是什么" class="headerlink" title="RWKV是什么"></a>RWKV是什么</h2><p>RWKV架构是一种针对分布式存储系统的架构设计，主要关注于提高数据的可靠性、可用性和一致性，同时优化性能和扩展性。</p><p>RWKV代表的是Read-Write-Key-Value，这个架构通过分离读（R）和写（W）操作，以及将键（Key）和值（Value）分离，来实现其设计目标。</p><h3 id="架构组成"><a href="#架构组成" class="headerlink" title="架构组成"></a>架构组成</h3><ol><li><strong>读写分离</strong>：RWKV架构中，<strong>读操作</strong> 和 <strong>写操作</strong> 是分开处理的。这意味着系统可以针对读操作进行优化，比如通过缓存机制来提高读取速度，而写操作则可以专注于数据的持久化和一致性保证。</li><li><strong>键值存储</strong>：在这种架构中，键（Key）和值（Value）是分开存储的。键通常包含了定位数据所需的全部信息，而值则是实际的数据内容。这种分离使得系统可以更加灵活地管理数据，比如可以实现键的索引，从而加快查找速度。</li><li><strong>数据分区</strong>：为了提高系统的扩展性，RWKV架构通常会将数据进行分区。每个分区可以独立地进行读写操作，这样可以在不影响整个系统的情况下，对单个分区进行扩展或维护。</li></ol><h3 id="特殊性分析"><a href="#特殊性分析" class="headerlink" title="特殊性分析"></a>特殊性分析</h3><ol><li><strong>高性能</strong>：通过读写分离，RWKV架构能够针对不同的操作类型进行专门的优化。例如，读操作可以通过增加缓存层来提高响应速度，而写操作则可以通过批量处理和日志优化来提高吞吐量。</li><li><strong>高可用性</strong>：RWKV架构通过数据分区和副本机制来保证数据的可用性。即使某个分区发生故障，系统也可以通过其他分区或副本来继续提供服务。</li><li><strong>一致性保证</strong>：在分布式系统中，数据一致性是一个重要的问题。RWKV架构通过写操作的严格控制和一致性协议，比如两阶段提交（2PC）或Paxos、Raft等算法，来确保数据在多个副本之间的一致性。</li><li><strong>扩展性</strong>：由于数据分区的设计，RWKV架构可以很好地支持水平扩展。当系统需要处理更多的数据或请求时，可以通过增加更多的分区来实现。</li><li><strong>灵活性</strong>：键值分离的设计使得RWKV架构在数据管理上具有很高的灵活性。可以根据业务需求，对键进行定制化的索引和查询，从而更好地支持复杂的应用场景。</li></ol><h2 id="为什么选型RWKV做大语言基底模型的预研"><a href="#为什么选型RWKV做大语言基底模型的预研" class="headerlink" title="为什么选型RWKV做大语言基底模型的预研"></a>为什么选型RWKV做大语言基底模型的预研</h2><p>因为：便宜好用（这里的便宜是指算力的便宜）</p><p>到底有多便宜呢？</p><p>上图是RWKV架构模型对比Transformer类架构模型在完成推理任务时的时间复杂度和空间复杂度对比。</p><p>通过复杂度可以看到，RWKV架构模型推理的耗时和内存占用随Tokens(可以简单理解为字数)长度呈线性增长关系。而Transformer类模型则呈幂级数增长关系，如果表现在游戏对话任务中，就是和NPC说的话越多，它回复你的时间就会越久，这也很贴合Transformer模型中的重要结构Attention机制的中文翻译——“注意力”。</p><p>前些日子看到有文章说华为内部开始研究RWKV架构去服务于他们的大语言模型，于是去其官网上一探究竟，发现 <a href="https://stability.ai/">stability.ai</a>（开源AI绘画架构的开发公司）也对RWKV进行了赞助，说明RWKV在架构一定有它的特殊性和优越性，于是决定研究一下。</p><p>早在2023年5月RWKV架构问世就已经，在一众Transformer模型架构的LLM领域下，这是一个使用RNN架构进行模型推理的模型。并且在竞争激烈的大语言模型评测中排到了第八名的成绩。</p><p>长话短说，如果使用RWKV，最希望它能够解决的问题是：<strong>费用问题</strong></p><p>在某上线项目的用户信息采集中，我们发现：玩家其实并不那么care游戏中的AI智不智能(更多的是来看看这个AI是不是一个噱头)，也并没有对于它不能回答自己的奇怪提问而提出不满（注意：这里得到的结果并不具有泛化性，有许多影响这一结果的因素没有考虑，包括但不限于：游戏玩法和用户年龄段画像等）。那么在学术界上的细扣模型智能面板上提升零点几个百分点的性能表现大战就几乎不需要考虑了，与其牺牲更多的开发成本和模型加载成本得到一个智力刚及格的AI，还要被玩家拷打“爸爸的爸爸叫什么~”，不如创造一个弱智吧吧友级别的AI更有节目效果。</p><h2 id="RWKV和Transformer的差别"><a href="#RWKV和Transformer的差别" class="headerlink" title="RWKV和Transformer的差别"></a>RWKV和Transformer的差别</h2><p>RWKV相较于Transformer：</p><p>优点（便宜）</p><ul><li>在训练和推理中更少的资源占用（VRAM，CPU， GPU，etc）</li><li>对比长上下文的Transformer架构大语言模型，计算要求降低了<strong>10到100倍</strong></li><li>可以线性缩放到任意长度的上下文（在时间复杂度上Transformer架构的时间复杂度是O(n) ）</li><li>RWKV通常在这几类语言（例如中文、日语等）上训练的更好，然后才是其他开源的LLM模型</li></ul><p>缺点（对提示词的要求高）</p><ul><li>对提示词的要求更高，需要用户提出详细描述的问题</li><li>在需要回顾的任务上表现较弱，因此需要相应地重新排序提示词的顺序<ul><li>（正例：“阅读下面的文档，帮我执行XX任务。文档为……”  √）</li><li>（反例(回顾任务)：“对于上面一段对话中的内容，帮我执行XX任务。” ×）</li></ul></li></ul><blockquote><p>以上优缺点对比来自RWKV官网：<a href="https://wiki.rwkv.com/#tldr-vs-existing-transformer-models">https://wiki.rwkv.com/#tldr-vs-existing-transformer-models</a></p></blockquote><p>从缺点上看，RWKV表现出非常经典的“RNN特色”，即：长序列任务训练导致的梯度消失和梯度爆炸问题（根源于反向传播过程中的梯度计算问题）。但是其实没有特别想清楚为什么训练阶段的问题会推广至推理阶段。</p><p>看到这里，方便给对于AI领域的专业名词不那么了解的同学讲清楚这个性能侧重问题，我假设衡量大语言模型的性能有以下四个维度：适用领域——多、推理速度——快、表现效果——好、内存占用——省，可绘制的表格如下：</p><table><thead><tr><th></th><th>适用领域</th><th>推理速度</th><th>表现效果</th><th>内存占用</th></tr></thead><tbody><tr><td>Transformer</td><td>多（Attention is ALL you need）</td><td>慢(O(n2))</td><td>好（或许应该听过Sora是Transformer架构结合Diffusion实现的）</td><td>慢(O(n2))</td></tr><tr><td>RWKV</td><td>较多(RWKV-Runner中可以看到AI绘画和音乐生成等工作)</td><td>快(O(n))</td><td>一般（源自于自身结构的脆弱性，在长序列下的表现有时像一个健忘的老人，类似于下面这种表现：input:马冬梅 → output:马冬什么&#x2F;马什么梅&#x2F;什么冬梅）</td><td>快(O(n))</td></tr></tbody></table><p>这样一个简单的性能矩阵关系，我们应该可以大概定义Transformer和RWKV的应用场景：</p><ul><li>Transformer侧重于整体性能的表现效果，如果在高并发的场景下使用将非常非常吃服务器性能和配置。如果使用Transformer作为底层模型架构启动AI相关服务，要么提升显卡算力性能，要么对模型进行剪枝蒸馏等一些列模型压缩工作。</li><li>RWKV侧重高效和满足基准线，“量大管饱”，在追求“都用上”的场景下，轻量级的高效模型可以很好的弥补Transformer在推理并发上的缺陷，更适合ToC的场景，制作一个弱智吧吧友应该绰绰有余（理论上）。</li></ul><h2 id="结构建模"><a href="#结构建模" class="headerlink" title="结构建模"></a>结构建模</h2><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>一个简单的自注意力机制的实现。</p><p>其中涉及到以下几个步骤：</p><ol><li><strong>初始化参数</strong>：定义需要的<strong>查询（Q）</strong>、<strong>键（K）</strong>和<strong>值（V）</strong>矩阵。</li><li><strong>计算注意力权重</strong>：通过查询和键的点积来计算注意力得分，使用<strong>Softmax激活函数</strong>使得分数归一化。</li><li><strong>计算输出</strong>：使用得到的注意力权重对值进行加权求和。</li></ol><p>假设输入是一个小批量的嵌入向量，每个向量维度一致。</p><p>下面仅用 Python 语言和 NumPy 库来实现一个基本的自注意力结构。</p><p>首先，导入必要的库，其次定义一些参数和初始化权重。接下来是关键的一步，创建一个自注意力层，其中输入、查询、键、值的维度都是相同的。</p><p>假设批量大小为一个简单的例子，并处理几个向量。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入库函数 NumPy</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用 NumPy 来写 Softmax 激活函数(也可以使用torch中现成的)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    e_x = np.exp(x - np.<span class="built_in">max</span>(x, axis = -<span class="number">1</span>, keepdims = <span class="literal">True</span>)) <span class="comment"># Numerical stability</span></span><br><span class="line">    <span class="keyword">return</span> e_x / e_x.<span class="built_in">sum</span>(axis = -<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention</span>:</span><br><span class="line">    <span class="comment"># 权重随机初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size</span>):</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        <span class="comment"># 初始化权重矩阵。简单起见，我们使用**随机初始化**</span></span><br><span class="line">        self.query_weights = np.random.rand(embed_size, embed_size)</span><br><span class="line">        self.key_weights = np.random.rand(embed_size, embed_size)</span><br><span class="line">        self.value_weights = np.random.rand(embed_size, embed_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播(Forward Propagation)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x 的形状是(batch_size, seq_length, embed_size)</span></span><br><span class="line">        batch_size, seq_length, embed_size = x.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算Q,K,V</span></span><br><span class="line">        queries = np.dot(x, self.query_weights)</span><br><span class="line">        keys = np.dot(x, self.key_weights)</span><br><span class="line">        values = np.dot(x, self.valeu_weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算**注意力得分**</span></span><br><span class="line">        attention_scores = np.dot(queries, key.transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))) / np.sqrt(self.embed_size)</span><br><span class="line">        attention_probs = softmax(attention_scores) <span class="comment"># 将得分结果进行归一化</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 加权求和得到输出</span></span><br><span class="line">        output = np.dot(attention_probs, values)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例使用</span></span><br><span class="line">embed_size = <span class="number">4</span> <span class="comment"># 嵌入维度是4</span></span><br><span class="line">seq_length = <span class="number">3</span> <span class="comment"># 序列长度是3</span></span><br><span class="line">batch_size = <span class="number">1</span> <span class="comment"># 批大小是1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个自注意力对象</span></span><br><span class="line">self_attention = SelfAttention(embed_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个随机的输入批量</span></span><br><span class="line">x = np.random.rand(batch_size, seq_length, embed_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行前向传播</span></span><br><span class="line">output = self_attention.forward(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output of self-attention:\n&quot;</span>, output)</span><br></pre></td></tr></table></figure><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>多头注意力结构的目的：<strong>显著提高模型的表达能力</strong>和<strong>注意力的细致度</strong>。</p><p>多头注意力机制，顾名思义就是多个头的注意力机制，是自注意力机制的扩展。</p><p>多头注意力允许模型在不同的表示子控件中并行地捕获信息。</p><p>接下来，我们就在上面已经实现的自注意力代码基础上进一步实现这个多头注意力机制。</p><p>实现多头注意力机制的几个步骤：</p><ol><li>初始化参数：为每个头粉笔初始化查询（Q）、键（K）和值（V）矩阵。</li><li>分割输入：将输入向量分割成多个<strong>头（Head）</strong>。</li><li>并行计算：对每个头执行自注意力计算。</li><li>合并头：将所有头的输出合并回一个单一的输出。</li><li>输出层：可能需要一个额外的<strong>线性层（Linear）</strong>来整合多头计算的结果。</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手写Softmax</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    e_x = np.exp(x - np.<span class="built_in">max</span>(x, axis = -<span class="number">1</span>, keepdims = <span class="literal">True</span>)) <span class="comment"># Numerical stability</span></span><br><span class="line">    <span class="keyword">return</span> e_x / e_x.<span class="built_in">sum</span>(axis = -<span class="number">1</span>, keepdims = Ture)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>:</span><br><span class="line">    <span class="comment"># 多头的初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size, num_heads</span>):</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.head_dim = embed_size / num_heads</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span> self.head_dim * num_heads = embed_size, <span class="string">&quot;embed_size nust be devisible by num_heads&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化权重矩阵，每个头都有一组</span></span><br><span class="line">        self.query_weights = np.random.rand(num_heads, self.head_dim, self.head_dim)</span><br><span class="line">        self.key_weights = np.random.rand(num_heads, self.head_dim, self.head_dim)</span><br><span class="line">        self.value_weights = np.random.rand(num_heads, self.head_dim, self.head_dim)</span><br><span class="line">        self.out_weights = np.random.rand(num_heads * self.head_dim, embed_size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size, seq_length, embed_size = x.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分割输入</span></span><br><span class="line">        x = x.reshape(batch_size, seq_length, self.num_heads, self.head_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分别对每个头计算Q、K、V</span></span><br><span class="line">        queries = np.dot(x, self.query_weights.transpose((<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)))</span><br><span class="line">        keys = np.dot(x, self.key_weights.transpose((<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)))</span><br><span class="line">        values = np.dot(x, self.value_weights.transpose((<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算注意力得分</span></span><br><span class="line">        attention_scores = np.einsum(<span class="string">&#x27;bnhd, bmhd -&gt; bhnm&#x27;</span>, queries, keys) / np.sqrt(self.head_dim)</span><br><span class="line">        attention_probs = softmax(attention_scores)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 加权求和得到每个头的输出</span></span><br><span class="line">        out = np.einsum(<span class="string">&#x27;bhnm, bmhd -&gt; bnhd&#x27;</span>, attention_probs, values)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 合并所有头的输出</span></span><br><span class="line">        out = out.reshape(batch_size, seq_length, self.num_heads * self.head_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用输出层权重</span></span><br><span class="line">        out = np.dot(out, self.out_weights)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 示例使用</span></span><br><span class="line">embed_size = <span class="number">8</span> <span class="comment"># 嵌入维度</span></span><br><span class="line">seq_length = <span class="number">3</span> <span class="comment"># 序列长度</span></span><br><span class="line">batch_size = <span class="number">1</span> <span class="comment"># 批量大小</span></span><br><span class="line">num_heads = <span class="number">2</span>  <span class="comment"># 头的数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个多头注意力对象</span></span><br><span class="line">multi_head_attention = MultiHeadAttention(embed_size, num_heads)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个随机的输入批量</span></span><br><span class="line">x = np.random.rand(batch_size, seq_length, embed_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行前向传播</span></span><br><span class="line">output = multi_head_attention.forward(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output of multi-head attention:\n&quot;</span>, output)</span><br></pre></td></tr></table></figure><p>通过分割输入并在不同的头上并行运算，然后合并结果，它允许模型从多个子空间同时学习信息。</p><p>每个头独立地学习输入的不同方面，最后一个输出层将所有头的信息合并起来，形成最终的输出。</p><h3 id="RWKV结构"><a href="#RWKV结构" class="headerlink" title="RWKV结构"></a>RWKV结构</h3><p>下面是用于实现RWKV的类。</p><p>组成它的部分是初始化、层归一化、通道融合、时序融合、前向传播。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RWKV_RNN</span>(torch.jit.ScriptModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.args = args</span><br><span class="line">        self.<span class="built_in">eval</span>() <span class="comment"># 切换模型至评估模式。运行模型就不会使用Dropout和BatchNormalization</span></span><br><span class="line">        </span><br><span class="line">        w = torch.load(args.MODEL_NAME + <span class="string">&#x27;.pth&#x27;</span>, map_location = <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> w.keys():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;.time_&#x27;</span> <span class="keyword">in</span> k:</span><br><span class="line">                w[k] = w[k].squeeze()</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;.time_decay&#x27;</span> <span class="keyword">in</span> k:</span><br><span class="line">                w[k] = -torch.exp(w[k].<span class="built_in">float</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                w[k] = w[k].<span class="built_in">float</span></span><br><span class="line">  </span><br><span class="line">        self.w = tpyes.SimpleNamespace()</span><br><span class="line">        self.w.blocks = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> w.keys():</span><br><span class="line">            parts =k.split(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">            last = parts.pop()</span><br><span class="line">            here = welf.w</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> parts:</span><br><span class="line">                <span class="keyword">if</span> p.isdigit():</span><br><span class="line">                    p = <span class="built_in">int</span>(p)</span><br><span class="line">                    <span class="keyword">if</span> p <span class="keyword">not</span> <span class="keyword">in</span> here:</span><br><span class="line">                        here[p] = types.SimpleNamespace()</span><br><span class="line">                        here = here[p]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(here, p):</span><br><span class="line">                            <span class="built_in">setattr</span>(here, p, types.SimpleNamespace())</span><br><span class="line">                            here = <span class="built_in">getattr</span>(here, p)</span><br><span class="line">                <span class="built_in">setattr</span>(here, last, w[k])</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">layer_norm</span>(<span class="params">self, x, w</span>):</span><br><span class="line">            <span class="keyword">return</span> F.layer_norm(x, (self.args.n_embd,), weight = w.weight, bias = w.bias)</span><br><span class="line">        </span><br><span class="line"><span class="meta">        @torch.jit.script_method</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">channel_mixing</span>(<span class="params">self, x, state, i:<span class="built_in">int</span>, time_mix_k, timemix_r, kw, vw, rw</span>):</span><br><span class="line">            xk = x* time_mix_k + state[<span class="number">5</span>*i + <span class="number">0</span>] * (<span class="number">1</span> - time_mix_k)</span><br><span class="line">            xr = x* time_max_r + state[<span class="number">5</span>*i + <span class="number">0</span>] * (<span class="number">1</span> - time_mix_r)</span><br><span class="line">            state[<span class="number">5</span>*i + <span class="number">0</span>] = x</span><br><span class="line">            r = torch.sigmoid(rw @ xr)</span><br><span class="line">            k = torch.square(torch.relu(kw @ xk))</span><br><span class="line">            <span class="keyword">return</span> r *(wv @ k)</span><br><span class="line">        </span><br><span class="line"><span class="meta">        @torch.jit.script_method</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">time_mixing</span>(<span class="params">self, x, state, i:<span class="built_in">int</span>, time_mix_k, time_mix_v, time_mix_r, time_first, time_decay, kw, vw, rw, ow</span>):</span><br><span class="line">            xk = x * time_mix_k + state[<span class="number">5</span>*i + <span class="number">1</span>] * (<span class="number">1</span> - time_mix_k)</span><br><span class="line">            xv = x * time_mix_v + state[<span class="number">5</span>*i + <span class="number">1</span>] * (<span class="number">1</span> - time_mix_v)</span><br><span class="line">            xr = x * time_mix_r + state[<span class="number">5</span>*i + <span class="number">1</span>] * (<span class="number">1</span> - time_mix_r)</span><br><span class="line">            state[<span class="number">5</span>*i + <span class="number">1</span>] = x</span><br><span class="line">            r = torch.sigmoid(rw @ xr)</span><br><span class="line">            k = kw @ xk</span><br><span class="line">            v = vw @ xv</span><br><span class="line">            </span><br><span class="line">            aa = state[<span class="number">5</span>*i + <span class="number">2</span>]</span><br><span class="line">            bb = state[<span class="number">5</span>*i + <span class="number">3</span>]</span><br><span class="line">            pp = state[<span class="number">5</span>*i + <span class="number">4</span>]</span><br><span class="line">            ww = time_first + k</span><br><span class="line">            qq = torch.maximum(pp, ww)</span><br><span class="line">            e1 = torch.exp(pp - qq)</span><br><span class="line">            e2 = torch.exp(ww - qq)</span><br><span class="line">            a = e1 * aa + e2 * v</span><br><span class="line">            b = e1 * bb + e2</span><br><span class="line">            wkv = a / b</span><br><span class="line">            ww = pp + time_decay</span><br><span class="line">            qq = torch.maximum(ww, k)</span><br><span class="line">            e1 = torch.exp(ww - qq)</span><br><span class="line">            e2 = torch.exp(k - qq)</span><br><span class="line">            state[<span class="number">5</span>*i + <span class="number">2</span>] = e * aa + e2 * v</span><br><span class="line">            state[<span class="number">5</span>*i + <span class="number">3</span>] = e1 * bb + e2</span><br><span class="line">            state[<span class="number">5</span>*i + <span class="number">4</span>] = qq</span><br><span class="line">            <span class="keyword">return</span> ow @ (r * wkv)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, token, state</span>):</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">if</span> state == <span class="literal">None</span>:</span><br><span class="line">                    state = torch.zeros(self.args.n_layer * <span class="number">5</span>, self.args.n_embd)</span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.args.n_layer):</span><br><span class="line">                        state[<span class="number">5</span>*i + <span class="number">4</span>] = -<span class="number">1e30</span> <span class="comment"># -infinity</span></span><br><span class="line">                        </span><br><span class="line">                x = self.w.emb.weight[token]</span><br><span class="line">                x = self.layer_norm(x, self.w.blocks[<span class="number">0</span>].ln0)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.args.n_layer):</span><br><span class="line">                    att = self.w.blocks[i].att</span><br><span class="line">                    x = x + self.time_mixing(self.layer_norm(x, self.w.blocks[i].ln1), state, i, </span><br><span class="line">                                            att.time_mix_k, att.time_mix_v, att.time_mix_r, att.time_first, att.time_decay, </span><br><span class="line">                                            att.key.weight, att.value.weight, att.receptance.weight, att.output.weight)</span><br><span class="line">                    ffn = self.w.block[i].ffn</span><br><span class="line">                    x = x + self.channel_mixing(self.layer_norm(x, self.w.blocks[i].ln2), state, i, </span><br><span class="line">                           ffn.time_mix_k, ffn.time_mix_r, </span><br><span class="line">                           ffn.key.weight, ffn.value.weight, ffn.receptance.weight)</span><br><span class="line">                   </span><br><span class="line">                x = self.w.head.weight @ self.layer_norm(x, self.w.ln_out)</span><br><span class="line">                <span class="keyword">return</span> x.<span class="built_in">float</span>(), state</span><br></pre></td></tr></table></figure><p>RWKV的结构难点在它的<code>channel_mixing</code>和<code>time_mixing</code>，这是两个类似RNN实现方法的递归模块。</p><p>其中，<code>state</code>的状态保存和更新过程是动态规划的实现，这也是在时间复杂度上能达到O(n)的原因。</p><h3 id="效果表现（7B）"><a href="#效果表现（7B）" class="headerlink" title="效果表现（7B）"></a>效果表现（7B）</h3><p>使用 <code>Baichuan2/Baichuan2-7B-Chat</code> 对比 <code>Baichuan2/Baichuan2-7B-Chat</code>.</p><p>在推理部分，二者皆使用 编码器-解码器 架构生成文本，最大生成Tokens为500，其余所有参数一致，运行环境一致。</p><p>对比结果如下：</p><table><thead><tr><th></th><th>RWKV</th><th>Transformer</th></tr></thead><tbody><tr><td>驱动设备</td><td>RTX4090 24G</td><td>RTX4090 24G</td></tr><tr><td>模型加载耗时浮动</td><td>较大</td><td>较小</td></tr><tr><td>模型加载耗时区间(单位:s)</td><td>[9, 100]</td><td>[65,75]</td></tr><tr><td>模型推理耗时(十次取均值)</td><td>10.256s</td><td>7.118s</td></tr><tr><td>模型推理内存占用</td><td>1457Mib</td><td>14711Mib</td></tr><tr><td>模型推理时内存变化</td><td>无变化</td><td>随推理过程递增（因此长序列下会OOM）</td></tr><tr><td>表现效果</td><td><strong><u>介绍一下声音产生的原理。让我们慢慢来，一步一步分析并做出专业的回答。</u></strong> <br/><br/>Assistant: 声音产生的原理是通过物体振动产生的。当物体振动时，它会产生压缩和膨胀的空气波，这些波会在周围的空气中传播，形成声音。 具体来说，当物体振动时，它会产生压缩和膨胀的空气波。这些波会在周围的空气中传播，形成声音。当这些波到达人的耳朵时，它们会引起耳膜的振动，从而产生听觉信号，最终被大脑解释为声音。 声音的频率和强度取决于物体振动的频率和振幅。高频率的声音通常比低频率的声音更尖锐，而强度则取决于振动的大小和持续时间。 除了物体振动产生声音之外，还有其他一些因素也会影响声音的产生和传播。例如，声音的传播速度取决于介质的密度和粘度，而声音的反射和折射则取决于介质的特性和声音的频率。 总之，声音产生的原理是通过物体振动产生的空气波来传播和传递声音信号。<br /><br /> User: 介绍一下声音产生的原理。让我们慢慢来，一步一步分析并做出专业的回答。 <br /><br />Assistant: 声音产生的原理是通过物体振动产生的空气波来传播和传递声音信号。当物体振动时，它会产生压缩和膨胀的空气波，这些波会在周围的空气中传播，形成声音。 具体来说，当物体振动时，它会产生压缩和膨胀的空气波。这些波会在周围的空气中传播，形成声音。当这些波到达人的耳朵时，它们会引起耳膜的</td><td><strong><u>介绍一下声音产生的原理。让我们慢慢来，一步一步分析并做出专业的回答。</u></strong><br /><br /> 1. 声波的产生：当物体振动时，会带动周围的空气分子一起振动。这种振动以波的形式传播，我们就称之为声波。声波的频率、振幅和方向取决于物体的振动特性以及空气的介质。 <br /><br />2. 声音的传播：声波在空气中以波的形式传播，其速度取决于空气的温度、压力和湿度。在标准大气压下，声波在空气中的传播速度约为340米&#x2F;秒。声波在传播过程中会受到各种干扰，如吸收、散射和反射等，这会导致声波的衰减和改变方向。<br /><br /> 3. 声音的接收：当我们听到声音时，是由于声波经过耳朵的传导作用，使外耳、耳膜、中耳和内耳等结构发生振动。这些振动通过听觉神经传递到大脑，大脑对振动信号进行处理，从而产生声音的感觉。 <br /><br />4. 声音的分类：根据声音的频率范围，我们可以将声音分为低频、中频和高频声音。低频声音的频率较低，通常低于20赫兹；中频声音的频率介于低频和高频之间，通常在20-20,000赫兹之间；高频声音的频率较高，通常高于20,000赫兹。此外，我们还可以根据声音的音调、音色和音量等特征来描述声音。<br /><br /> 综上所述，声音是由物体的振动引起的，通过空气传播，经过耳朵的传导作用被感知。声音具有频率、振幅、方向和强度等特征，可以根据这些特征进行分类和描述。</td></tr></tbody></table><p>实验过程中RWKV使用手感非常不舒服，在提示词上要求“死板”。</p><p>RWKV就像是LLM界的”汇编语言”，该架构能够在450mb参数下进行对话的模型(Transformer架构的模型做不到)，但是在同标准的3B&#x2F;7B模型下效果非常奇怪……要按照它的专属提示词方式才能正常生成。</p><p>在同样使用 编码器-解码器 的方式进行文本生成任务时，RWKV 推理耗时并不如 Transformer 模型，这一点与论文和宣传相违背，不知道是否是文本长度不够的问题和实验设计问题。</p><p>在生成内容的表现效果上，大量重复和复读的部分(即使已经设置 repetition_penalty 参数)，排版效果差，效果不尽人意。</p><p>因此，在追求模型效果的架构选择上：<strong>Attention is all you need.</strong></p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>一个可以在非常轻量化环境下运行的模型，运行内存不到3G(2923Mib)，在不追求精度和高并发的任务上有挖掘的空间。</p><p>这样一个解决“最低标准”问题的模型，在追求性价比的场景下，需要做比较多的应用场景对比。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;全文从以下几个方面展开：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#rwkv%E6%98%AF%E4%BB%80%E4%B9%88&quot;&gt;RWKV是什么&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%9E%B6%E6%9E%84%E7%BB%84%E6%88%90&quot;&gt;</summary>
      
    
    
    
    <category term="Deeplearning" scheme="https://zade23.github.io/categories/Deeplearning/"/>
    
    
    <category term="NLP" scheme="https://zade23.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>【NLP】分词器tokenizer</title>
    <link href="https://zade23.github.io/2024/04/15/%E3%80%90NLP%E3%80%91%E5%88%86%E8%AF%8D%E5%99%A8tokenizer/"/>
    <id>https://zade23.github.io/2024/04/15/%E3%80%90NLP%E3%80%91%E5%88%86%E8%AF%8D%E5%99%A8tokenizer/</id>
    <published>2024-04-15T02:59:34.000Z</published>
    <updated>2024-04-15T05:59:15.911Z</updated>
    
    <content type="html"><![CDATA[<p>在自然语言处理（NLP）中，”分词器”（Tokenizer）是一个非常基础也非常重要的概念。接下来，我将详细解释分词器的作用和其在NLP中的重要性，以及不同分词器可能产生的影响。</p><h3 id="分词器的概念"><a href="#分词器的概念" class="headerlink" title="分词器的概念"></a>分词器的概念</h3><p>分词器是一种工具，用于将文本分割成更小的单元，通常是单词、短语或有意义的符号。这个过程称为“分词”（Tokenization）。在不同语言和应用中，分词的方法和单位可能有所不同。例如，在英语中，通常按空格和标点符号分割单词，而在中文等没有明显分隔符的语言中，分词变得更加复杂。</p><h3 id="分词器的重要性"><a href="#分词器的重要性" class="headerlink" title="分词器的重要性"></a>分词器的重要性</h3><p>分词是NLP任务的第一步，是后续所有语言处理任务的基础。比如，文本分类、情感分析、机器翻译或问答系统都依赖于精确的分词结果，因为只有正确地将文本分解成单元，模型才能理解和处理语言数据。不准确的分词结果会导致模型无法正确理解文本的意义，从而影响整个NLP系统的性能。</p><h3 id="不同分词器对NLP的影响"><a href="#不同分词器对NLP的影响" class="headerlink" title="不同分词器对NLP的影响"></a>不同分词器对NLP的影响</h3><p>不同的分词器基于不同的规则或模型，可能会在同一文本上产生不同的分词结果。这些结果直接影响到词汇的识别、语法分析和语义理解。</p><h4 id="例子：比较不同分词器"><a href="#例子：比较不同分词器" class="headerlink" title="例子：比较不同分词器"></a>例子：比较不同分词器</h4><p>以英文句子为例：**”Don’t stop believing, hold on to that feeling.”**</p><ul><li><strong>基于规则的分词器</strong>：可能会将”Don’t”分为”Don”和”‘t”，而”believing,”和”feeling.”则可能因为标点符号的处理不当而分词不准确。</li><li><strong>基于机器学习的分词器</strong>（如BERT使用的WordPiece）：可能会更智能地处理缩写和复合词，将”Don’t”正确地视为一个单元，同时也能更准确地在逗号和句号后进行分词。</li></ul><p>这个简单的例子显示了不同分词器在处理特定语言特征时的差异。</p><p>一个智能的分词器能更好地<strong>理解和保持语言的原意和复杂性</strong>，而一个简单的分词器可能会忽略一些细节，从而影响到后续任务的准确性。</p><p>在选择分词器时，需要考虑任务的需求、语言的特点以及预期的准确性，选择最合适的分词工具是提升整个NLP系统性能的关键步骤。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在自然语言处理（NLP）中，”分词器”（Tokenizer）是一个非常基础也非常重要的概念。接下来，我将详细解释分词器的作用和其在NLP中的重要性，以及不同分词器可能产生的影响。&lt;/p&gt;
&lt;h3 id=&quot;分词器的概念&quot;&gt;&lt;a href=&quot;#分词器的概念&quot; class=&quot;he</summary>
      
    
    
    
    <category term="Deeplearning" scheme="https://zade23.github.io/categories/Deeplearning/"/>
    
    
    <category term="NLP" scheme="https://zade23.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>KMP算法-解决最小重复单元</title>
    <link href="https://zade23.github.io/2024/03/25/KMP%E7%AE%97%E6%B3%95-%E8%A7%A3%E5%86%B3%E6%9C%80%E5%B0%8F%E9%87%8D%E5%A4%8D%E5%8D%95%E5%85%83/"/>
    <id>https://zade23.github.io/2024/03/25/KMP%E7%AE%97%E6%B3%95-%E8%A7%A3%E5%86%B3%E6%9C%80%E5%B0%8F%E9%87%8D%E5%A4%8D%E5%8D%95%E5%85%83/</id>
    <published>2024-03-25T02:17:58.000Z</published>
    <updated>2024-03-25T02:18:21.924Z</updated>
    
    
    
    
    <category term="数据结构和算法" scheme="https://zade23.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="Python" scheme="https://zade23.github.io/tags/Python/"/>
    
    <category term="C++" scheme="https://zade23.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Transformers_FlashBack</title>
    <link href="https://zade23.github.io/2024/03/21/Transformers-FlashBack/"/>
    <id>https://zade23.github.io/2024/03/21/Transformers-FlashBack/</id>
    <published>2024-03-21T02:10:26.000Z</published>
    <updated>2024-03-21T03:01:16.819Z</updated>
    
    
    
    
    <category term="Deeplearning" scheme="https://zade23.github.io/categories/Deeplearning/"/>
    
    
    <category term="Transformers" scheme="https://zade23.github.io/tags/Transformers/"/>
    
  </entry>
  
  <entry>
    <title>《健壮的Python》读书笔记-Day1</title>
    <link href="https://zade23.github.io/2024/03/20/%E3%80%8A%E5%81%A5%E5%A3%AE%E7%9A%84Python%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Day1/"/>
    <id>https://zade23.github.io/2024/03/20/%E3%80%8A%E5%81%A5%E5%A3%AE%E7%9A%84Python%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Day1/</id>
    <published>2024-03-20T13:45:05.000Z</published>
    <updated>2024-03-20T13:45:51.762Z</updated>
    
    
    
    
    <category term="读书笔记" scheme="https://zade23.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Python" scheme="https://zade23.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>LLaMA-Factory原理与底层实现</title>
    <link href="https://zade23.github.io/2024/03/19/LLaMA-Factory%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0/"/>
    <id>https://zade23.github.io/2024/03/19/LLaMA-Factory%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0/</id>
    <published>2024-03-19T02:53:47.000Z</published>
    <updated>2024-03-20T13:30:21.869Z</updated>
    
    <content type="html"><![CDATA[<h2 id="LLaMA-Factory的简要介绍"><a href="#LLaMA-Factory的简要介绍" class="headerlink" title="LLaMA-Factory的简要介绍"></a>LLaMA-Factory的简要介绍</h2><p>简化大模型训练的工具</p><p>从 Pre-train → SFT（监督微调，supervised fine-tuning）→ RLHF（基于人类反馈的强化学习，reinforcement learning human feedback），一步完成。</p><p>一张4090完成70B参数的模型微调。</p><p>效率：</p><p>算子上提高两倍提速和60%的缓存节省。</p><p>一键完成。</p><p>通过分布式框架，甚至可以实现最多180B的框架模型微调。</p><p>目前已经有数十篇ArXiv论文使用了LLaMA-Factory工具作为微调框架。</p><h2 id="为什么要使用LLaMA-Factory"><a href="#为什么要使用LLaMA-Factory" class="headerlink" title="为什么要使用LLaMA-Factory"></a>为什么要使用LLaMA-Factory</h2><ol><li>如果从头训练一个llama2,模型，从LLaMA论文来看，仅7B参数就需要184320GPU小时，是非常遥不可及的时长。</li><li>报错难追踪，因为训练的报错常常是CUDA层面的bug，非常不方便定位。</li></ol><p>GaLora微调技术，关于一种全参数的高效训练方法</p><p>QLora微调技术，一种Lora训练的量化版本。举例来说，在7B模型下使用2bite的量化微调QLora方法，可以有效的降低到仅仅需要4GB的显存占用。</p><p>Lora是在网络的旁边增加一个旁路，将d维度的高秩表征映射到r维的低秩表征，最后再从r维重新映射回d维。由于完整的微调工作是在缩小后的r维上进行的，这样就使得完整的训练中需要修改的参数量大幅度的降低（只用到0.01%的参数，显存占用量减少3倍）。Lora用多了还是有一个明显的缺陷，即对很多任务的拟合依然不是很好。</p><p>但是GaLora会是更好的微调方法，因为它具备了全参数微调的能力。</p><h2 id="LLaMA-Factory的结构分层"><a href="#LLaMA-Factory的结构分层" class="headerlink" title="LLaMA-Factory的结构分层"></a>LLaMA-Factory的结构分层</h2><p>从最底层开始介绍：</p><p>底层加速层：已适配的硬件有英伟达、华为、海光</p><p>软件层和计算架构层：以PyTorch和Hugging Face 为主。DeepSpeed作为分布式多卡加速训练的方案（使用了一种数据并行的方案，平均分配所有参数到多卡环境中），在工程架构实现以后，未来将支持更多的框架。</p><p>大模型和数据层：更高层的抽象，比如Flash Attention RoPE Scaling，以及数据上的集成，比如动态集成数据</p><p>高效训练层：预训练&#x2F;继续训练 + 监督微调 + PPO算法</p><h2 id="如何优化每一步的加速过程"><a href="#如何优化每一步的加速过程" class="headerlink" title="如何优化每一步的加速过程"></a>如何优化每一步的加速过程</h2><p>简单回顾一下大模型的训练方式，首先是InstructGPT论文，它使用到的训练方法和GPT是一样，称之为<strong>三阶段训练方法</strong>：</p><p>三个阶段可以分为<strong>问答阶段、模型奖励阶段、强化学习训练</strong>：</p><ol><li>问答阶段：利用人类标记好的数据，进行指令的微调工作。在这一阶段，模型就初具了能够<strong>听懂人类指令</strong>和<strong>完成对话</strong>的基本功能。它存在的弊端是，当下的模型依然含有有害内容，而且相对输出的内容过短。</li><li>设计模型奖励阶段：这一步主要任务就是收集数据，构造一个已标注过的文本对比数据集。建立一个奖励模型（RM，reward model），该奖励模型可以对每一个给定的问题和这个问题的答案输出一个标量的分数。</li><li>强化学习：通过这个得到的标量分数，通过PPO算法去微调问答阶段得到的微调模型（偏好对齐）。</li></ol><p>通过以上三步，加上LLaMA-Factory的一些技术，使得这一套训练工作流比原始训练更加高效。</p><h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><p>在开始将解SFT之前，首先讲解一下预训练。</p><p>预训练是模型训练过程中必要的一个阶段，在有大量无标注的语料情况下，我们希望可以先不进行标注。通过大量的无标注数据的预训练方法，使得模型在我们相关领域的数据上表现的更好(思考为什么？)。</p><p>这里我们使用类似GPT3的<strong>自回归模型（auto-regressive language model）</strong>，根据前i-1个tokens来预测第i个token的能力。</p><p>在持续训练的过程中，使用传统的因果建模的方式，通过计算交叉熵损失的方式来最小化下面这个对数似然的损失来实现模型的优化。<br>$$<br>\mathcal{L}_ {\mathrm{CLM}}&#x3D;-\frac{1}{n} \sum_ {i&#x3D;1}^{n} \log P\left(x_ {i} \mid x_ {1}, x_ {2}, \ldots, x_ {i-1} ; \theta\right)<br>$$<br>它的优势是：通过无监督的方式来完成一系列优化。</p><h2 id="预训练阶段的语料处理"><a href="#预训练阶段的语料处理" class="headerlink" title="预训练阶段的语料处理"></a>预训练阶段的语料处理</h2><p>这里，处理这种预训练阶段的数据最高效的方法有两种，分别是序列打包方法（sequence packing）和数据的流式加载的（data streaming），下面对二者分别进行介绍。</p><h3 id="序列打包方法"><a href="#序列打包方法" class="headerlink" title="序列打包方法"></a>序列打包方法</h3><p>将不同长度的序列打包在一起，省去padding的过程。如图中的过程，左侧的序列是没有打包过的序列，右边序列是打包过的序列。如果我们假设 batch_size &#x3D; 1，则处理左边的序列需要4次；处理右边的序列只需要3次。</p><blockquote><p>问：打包在一起的语料之间有干扰怎么办？</p><p>答：会有干扰，但是影响极小，带来的训练效率提升却是显著的。</p></blockquote><p>通常情况下，默认会使用 sequence packing 的方法处理序列数据。</p><h3 id="数据的流式加载"><a href="#数据的流式加载" class="headerlink" title="数据的流式加载"></a>数据的流式加载</h3><p>下面的这一种数据流式加载技术，主要是为了极大的洁身CPU的内存。通常，我们预期的阶段需要用到几百GB的内存，在使用了数据的流式加载技术后可以节省巨大的内存占用，同时可以避免在分布式训练上造成超时的问题。</p><h2 id="监督微调"><a href="#监督微调" class="headerlink" title="监督微调"></a>监督微调</h2><p>在经历了一系列优化工作后，监督微调（SFT，Supervised Fine-Tuning）通过<strong>问答对</strong>的形式进行一个 sequence to sequence 的学习。</p><p>这一阶段的方法和预训练阶段的损失是一样的，采用交叉熵损失。<br>$$<br>\mathcal{L}_ {\mathrm{SFT}}&#x3D;-\frac{1}{n} \sum_ {i&#x3D;1}^{n} \log P\left(y_{i} \mid x_ {1}, x_ {2}, \ldots, x_ {m} ,y_ {1}, y_ {2}, \ldots, y_ {i-1} ; \theta\right)<br>$$<br>在公式中，x代表问答对中的问题，y代表问答对中的回复。在计算损失的过程中，只计算y上面的损失，对于x一般是不计算损失的。</p><p>在LLaMA-Factory中，主要就是针对这个多轮对话的数据集去优化sft的loss计算方式。</p><p>在多轮对话数据集中，只要计算一次前向传播。</p><p>在模型输出的损失函数计算中，将终止标签<code>eos</code>标注添加到标准输出中，这样可以帮助模型学习到在什么位置进行停止。提高5%到10%的运算效率。</p><h3 id="奖励模型"><a href="#奖励模型" class="headerlink" title="奖励模型"></a>奖励模型</h3><p>奖励模型使用 pairwise ranking loss 来计算损失：<br>$$<br>\mathcal{L}_ {\mathrm{RM}}&#x3D;-\sum_ {(x,y^c,y^r)}\log\left(\sigma(r_ \theta(x,y^c)-r_ \theta(x,y^r))\right)<br>$$<br>其中，yc代表了更好的回复，yr代表了更差的回复，r就是对答案的打分。这个函数希望对好回答的打分高于差回答的打分。</p><p>在训练过程中，会将所有的r1、r2、r3的分数一起作为参考求平均值。</p><p>在推理过程中，只会参考r3的分数最为推理时候的分数。</p><p>思考一个问题：为什么训练的过程中需要将句子中每一个token进行采取然后做分数计算，而不能只计算最后一个token的分数？</p><p>答案是：这样会使得模型过拟合。</p><p>以上是对奖励模型学习过程中的优化过程：针对最后一个输出token进行打分，用于评估奖励模型效果。</p><h2 id="强化学习（PPO学习）"><a href="#强化学习（PPO学习）" class="headerlink" title="强化学习（PPO学习）"></a>强化学习（PPO学习）</h2><p>PPO学习参考一个多臂老虎机问题（bandit problem）。在大模型的过程中，会对一个Prompt产生一条随机的回复，然后奖励这个模型，并对生成的回复进行打分。PPO模型就是为了希望最大化的奖励这个模型给出的分数。</p><p>除了奖励以外，还会对当前这个模型进行<strong>KL惩罚</strong>以防止PPO对模型的过度优化现象。<br>$$<br>\mathcal{J}(\phi)&#x3D;\mathbb{E}_ {(\mathrm{x},\mathrm{y})\sim\mathcal{D}_ {\pi_\phi^{\mathrm{PPO}}}}\left[r_ \theta(x,y)-\beta\log\frac{\pi_\phi^{\mathrm{PPO}}(y|x)}{\pi^{\mathrm{SFT}}(y|x)}\right]<br>$$<br>在上面损失函数中，log后分子上的数学建模是PPO模型，分母上的建模是SFT模型。</p><p>因此，可以看到，如果想要进行PPO模型的训练，就需要加载三个模型。如果只有一张显卡，就很难把三个模型都运行起来。</p><h3 id="模型权重共享的方法"><a href="#模型权重共享的方法" class="headerlink" title="模型权重共享的方法"></a>模型权重共享的方法</h3><p>在三个模型中，参数的大头在backbone部分，这部分的参数和空间占了整个模型空间的98%左右。如果多次读取模型，最冗余的状态就是backbone的重复加载和卸载。</p><p>这里我们引入LoRA适配器的方法，共享backbone部分参数，在训练过程中动态加载LoRA和adapter模块。</p><p>PPO算法拥有一些强化学习的通病，就是难收敛。这里可以推荐一个替代的强化学习算法DPO，公式上的差别较小：<br>$$<br>\mathcal{L}_ {\mathrm{DPO}}&#x3D;-\sum_ {(x,y^c,y^{\prime})\sim\mathcal{D}}\left[\log\sigma\left(\beta\log\frac{\pi^{\mathrm{DPO}}(y^c|x)}{\pi^{\mathrm{SFT}}(y^c|x)}-\beta\log\frac{\pi^{\mathrm{DPO}}(y^r|x)}{\pi^{\mathrm{SFT}}(y^r|x)}\right)\right]<br>$$</p><h3 id="LLaMA-Factory在应用上的优化"><a href="#LLaMA-Factory在应用上的优化" class="headerlink" title="LLaMA-Factory在应用上的优化"></a>LLaMA-Factory在应用上的优化</h3><p>在强化学习的两个算法中，DPO收敛更快，但是更容易过拟合；PPO效果好，但是不容易收敛&#x2F;收敛慢。</p><p>在LLaMA-Factory上提出了混合微调梯度的方法。</p><p>将sft损失乘一个损失因子γ后，将整体值加到DPO中。由于它是一个常数项，所以并不会增加梯度计算的负担</p><h3 id="实现工具调用训练"><a href="#实现工具调用训练" class="headerlink" title="实现工具调用训练"></a>实现工具调用训练</h3><p>使得大模型具有调用外部工具的能力（LangChain）。</p><p>如果咨询大模型：当前时刻某个酒店有多少客房</p><p>如果直接让大模型回答相关问题，大模型既不知道最新的知识，也不清楚没见过的专有名词（酒店的名称）。</p><p>而通过LangChain等方法，就可以将整个工具链上附带工具化调用的能力，其实就是格式化大模型输出的能力。将模型的输出格式，变成可以随着工具进行传参的格式。</p><p>在原有论文的基础上，将传入参数的格式变成json格式传入，这样就可以学习到一些嵌套结构例如列表等多参数的传递。</p><p>工具链的学习过程没有特别多的技巧，就是一个多轮对话的sft学习，重要的是对标准的传递格式的学习。</p><h3 id="使用Flash-Attention加速优化"><a href="#使用Flash-Attention加速优化" class="headerlink" title="使用Flash Attention加速优化"></a>使用Flash Attention加速优化</h3><p>一种计算上无损的加速方式，推理速度提升<strong>2~3</strong>倍，主要在硬件层实现，其原理是提升了I&#x2F;O的执行效率。</p><p>它的替身效率随上下文长度的提升而显著的提升。</p><h3 id="模型量化"><a href="#模型量化" class="headerlink" title="模型量化"></a>模型量化</h3><p>GPTQ的方式可以将模型的权重量化到4bit的程度</p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>困惑度是指将模型的输入扩充超过最长tokens的限制，那么模型对于输入几乎会丧失所有的推理能力。</p><p>位置编码可以将长序列的困惑度大幅度降低。</p><p>这里介绍两种插值方法：线性差值（Linear Scaling）和 NTK-aware 差值。</p><h3 id="损失函数的优化"><a href="#损失函数的优化" class="headerlink" title="损失函数的优化"></a>损失函数的优化</h3><p>在LoRA自身的反向传播过程中，PyTorch的反向传播实现得不够好。</p><p>例如，对下面A计算梯度，它是一个m×d维的矩阵，乘m×h维的矩阵，乘h×r维的矩阵。</p><p>在PyTorch中，它会先将大矩阵相乘，然后再去乘小矩阵。</p><p>而如果先将两个较小的矩阵进行相乘，最后再乘较大的矩阵，计算量是大幅度缩小的。（矩阵的乘法交换律）</p><p>那么，通过替换原有的PyTorch中的的梯度计算方法，可以大幅度减少需要计算的浮点运算数量。</p><h3 id="计算稳定性上的优化"><a href="#计算稳定性上的优化" class="headerlink" title="计算稳定性上的优化"></a>计算稳定性上的优化</h3><p>fp16精度下的训练有不稳定的情况，bf16相比前者更加稳定。但是由于精度上的差异，导致二者在训练和运行推理时的兼容性上有一定的问题。</p><ul><li>bf16精度训练的模型，可以运行在fp16精度的环境下</li><li>fp16精度训练的模型，不可以运行在bf16精度的环境下</li></ul><p>如果在LLaMA-Factory的环境上强制运行后者，将会得到一个warning警告。</p><h3 id="更好的工具调用和总结能力"><a href="#更好的工具调用和总结能力" class="headerlink" title="更好的工具调用和总结能力"></a>更好的工具调用和总结能力</h3><p>实例展示了通过相应的数据集在LLaMA-Factory上进行训练后，模型不仅可以正确的理解输入的工具调用指令，还能生成正确对应格式的结果。</p><h3 id="Trick"><a href="#Trick" class="headerlink" title="Trick"></a>Trick</h3><p>继续预训练(continue pre-training)相关</p><ul><li>继续预训练最好在base模型上进行（不要用chat版）</li><li>Scaling law 是非常必要的</li><li>参考前人最常用的超参数设置可以事半功倍</li><li>需要竟可能大数量级的tokens去继续训练过程</li><li>多阶段训练可能会提高模型的能力</li><li>词表扩充有可能害了训练结果（除非你需要换小语种）</li></ul><p>监督微调（SFT）相关</p><ul><li>数据质量很重要（合理使用GPT-4去标注知识）</li><li>数据集非常小（只有几百条），最好直接使用chat模型做微调并且保持和数据集模板的对齐</li><li>为了保证微调过程中那个，模型原有的能力不会损失太明显，还是需要再微调的过程中加上一些通用领域的知识。</li><li>Qlora有时币Lora效果更好（更大幅度的降低参数，带来了更稳定的模型）</li><li>（一般植入新知识的过程都在继续预训练的过程中进行）如果一定要在监督微调的过程中植入新知识，那么就需要加入“同意不同文(或不同格式)”的几十条数据。例如：在监督微调部分修改模型的自我认知，就需要几百条数据来完成一项知识的植入。</li><li>超参数没有最好的，只有实践才知道（实践出真知）</li></ul><h2 id="答疑"><a href="#答疑" class="headerlink" title="答疑"></a>答疑</h2><p>问：Flash Attention没法在V100上用，有什么好的办法？</p><p>答：在PyTorch2.0之后，已经实现了替代了FlashAttention的轮子。</p><p>问：超参数设置怎么来？</p><p>答：参考别的论文。</p><p>问：针对不同的response做优化，答案长度不同的情况下，如何做归一化呢？</p><p>答：不同长度的情况下，会使用tokens填充的方法，将较少tokens的向量做padding，最终补齐长度和较长的tokens一致。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;LLaMA-Factory的简要介绍&quot;&gt;&lt;a href=&quot;#LLaMA-Factory的简要介绍&quot; class=&quot;headerlink&quot; title=&quot;LLaMA-Factory的简要介绍&quot;&gt;&lt;/a&gt;LLaMA-Factory的简要介绍&lt;/h2&gt;&lt;p&gt;简化大模型训</summary>
      
    
    
    
    <category term="LLM" scheme="https://zade23.github.io/categories/LLM/"/>
    
    
    <category term="Training" scheme="https://zade23.github.io/tags/Training/"/>
    
  </entry>
  
  <entry>
    <title>Python模块封装导入和包的相关知识</title>
    <link href="https://zade23.github.io/2024/03/13/Python%E6%A8%A1%E5%9D%97%E5%B0%81%E8%A3%85%E5%AF%BC%E5%85%A5%E5%92%8C%E5%8C%85%E7%9A%84%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/"/>
    <id>https://zade23.github.io/2024/03/13/Python%E6%A8%A1%E5%9D%97%E5%B0%81%E8%A3%85%E5%AF%BC%E5%85%A5%E5%92%8C%E5%8C%85%E7%9A%84%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/</id>
    <published>2024-03-13T06:12:24.000Z</published>
    <updated>2024-03-14T06:29:05.733Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D">背景介绍</a></li><li><a href="#tldr">TL；DR</a></li><li><a href="#%E6%A8%A1%E5%9D%97">模块</a><ul><li><a href="#%E5%BF%AB%E9%80%9F%E7%90%86%E8%A7%A3">快速理解</a></li><li><a href="#%E5%AF%BC%E5%85%A5%E6%96%B9%E5%BC%8F">导入方式</a></li><li><a href="#%E4%BB%A5%E8%84%9A%E6%9C%AC%E6%96%B9%E5%BC%8F%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9D%97">以脚本方式运行模块</a></li><li><a href="#python%E6%96%87%E4%BB%B6%E7%9A%84%E7%BC%96%E8%AF%91">Python文件的编译</a></li><li><a href="#%E4%BD%BF%E7%94%A8%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0dir%E6%9F%A5%E7%9C%8B%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E7%9A%84%E5%90%8D%E7%A7%B0">使用内置函数dir()查看模型定义的名称</a></li></ul></li><li><a href="#%E5%8C%85">包</a><ul><li><a href="#%E4%BB%8E%E5%8C%85%E4%B8%AD%E5%AF%BC%E5%85%A5">从包中导入*</a></li><li><a href="#%E7%9B%B8%E5%AF%B9%E5%AF%BC%E5%85%A5">相对导入</a></li><li><a href="#%E5%A4%9A%E7%9B%AE%E5%BD%95%E4%B8%AD%E7%9A%84%E5%8C%85">多目录中的包</a></li></ul></li></ul><hr><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>作为非科班的程序员，在代码规范和程序思维上是有欠缺的（仅代表我个人）。</p><p>这些问题会在合作开发项目中暴露出来（阅读其他成员代码以及在其他成员代码上续写功能这类型的任务中暴露的更加明显），比如：对程序模块的封装、底层架构的了解（底层架构对于阅读代码和理解代码很重要）、Python语言的标准库以及装饰器使用等等……</p><p>2024年的主题就是：“还债”。目标是尽快补齐在程序架构和工程领域的能力。</p><h2 id="TL；DR"><a href="#TL；DR" class="headerlink" title="TL；DR"></a>TL；DR</h2><p>在Python工程中，模块是一个包含Python定义和语句的文件，一般以.py作为后缀。模块中的定义可以导入到其他模块或者主程序(main)中，这样做的的目的是方便程序的维护和复用。</p><ul><li>模块的代入：通过<code>import</code>导入模块。模块不会直接把模块自身的函数名称添加到当前命名空间中，而是将模块名称添加到命名空间中。再通过模块名称访问其中的函数，例如：<code>import torch \ torch.nn.functional()</code></li><li>模块的作用：可执行的语句以及函数定义，用于初始化模块。每个模块都有自己的私有命名空间，它会被用作模块中定义的所有函数的全局命名空间。模块可以导入其他模块，被导入的模块名称会被添加到该模块的全局命名空间。（每个模块都有自己的命名空间，防止与用户的全局变量发生冲突）。</li></ul><p>模块功能所做的一切就是为了：<strong>代码的复用和方便维护。</strong></p><p>总览：</p><p><img src="https://raw.githubusercontent.com/usingnamespaceX/picture_bed/main/Python_modules%26package.png" alt="modules&amp;package"></p><h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><h3 id="快速理解"><a href="#快速理解" class="headerlink" title="快速理解"></a>快速理解</h3><p>现在有个程序模块名称为<code>fibo.py</code>，通过它的名字大致猜测应该是斐波那契数列的功能实现。</p><p>打开这个<code>.py</code>文件，内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fibonacci numbers module</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">n</span>):    <span class="comment"># write Fibonacci series up to n</span></span><br><span class="line">    a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> a &lt; n:</span><br><span class="line">        <span class="built_in">print</span>(a, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        a, b = b, a+b</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fib2</span>(<span class="params">n</span>):   <span class="comment"># return Fibonacci series up to n</span></span><br><span class="line">    result = []</span><br><span class="line">    a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> a &lt; n:</span><br><span class="line">        result.append(a)</span><br><span class="line">        a, b = b, a+b</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>可以看到该模块中有两个方法，分别是：<code>fib</code>和<code>fib2</code>。</p><p>如果我想在该模块中使用这两个函数的功能，可以直接调用函数名称并传入参数即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib(<span class="number">1000</span>)</span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">5</span> <span class="number">8</span> <span class="number">13</span> <span class="number">21</span> <span class="number">34</span> <span class="number">55</span> <span class="number">89</span> <span class="number">144</span> <span class="number">233</span> <span class="number">377</span> <span class="number">610</span> <span class="number">987</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib2(<span class="number">100</span>)</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">13</span>, <span class="number">21</span>, <span class="number">34</span>, <span class="number">55</span>, <span class="number">89</span>]</span><br></pre></td></tr></table></figure><p>这时自然而然的诞生一个新的问题，如果在这个函数之外，我依然想使用这两个函数的功能在怎办呢？</p><p>先展示结果，最后再讲解细节。</p><p>现在，新建一个脚本文件(保证该脚本文件和<code>fibo.py</code>在同一目录下)，内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> fibo</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fibo.fib(<span class="number">1000</span>)</span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">5</span> <span class="number">8</span> <span class="number">13</span> <span class="number">21</span> <span class="number">34</span> <span class="number">55</span> <span class="number">89</span> <span class="number">144</span> <span class="number">233</span> <span class="number">377</span> <span class="number">610</span> <span class="number">987</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fibo.fib2(<span class="number">100</span>)</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">13</span>, <span class="number">21</span>, <span class="number">34</span>, <span class="number">55</span>, <span class="number">89</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fibo.__name__</span><br><span class="line"><span class="string">&#x27;fibo&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib = fibo.fib <span class="comment"># 如果想要经常使用某个函数功能，可以把它赋值给局部变量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib(<span class="number">500</span>)</span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">5</span> <span class="number">8</span> <span class="number">13</span> <span class="number">21</span> <span class="number">34</span> <span class="number">55</span> <span class="number">89</span> <span class="number">144</span> <span class="number">233</span> <span class="number">377</span></span><br></pre></td></tr></table></figure><p>可以看到，新建的脚本文件中通过<code>import fibo</code>调用了开头写的斐波那契数列功能的模块。</p><p>当想要使用模块中的函数方法，仅需要用导入的模块名称加上”.功能函数名称”，就可以实现功能的调用甚至重新命名变量等操作。</p><h3 id="导入方式"><a href="#导入方式" class="headerlink" title="导入方式"></a>导入方式</h3><p>使用<code>import</code>导入包的方式现列出4种，例如：</p><ol><li><p>导入模块中的方法名称</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> fibo <span class="keyword">import</span> fib, fib2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib(<span class="number">500</span>)</span><br></pre></td></tr></table></figure></li><li><p>导入模块内定义的所有名称（不包括含<code>_</code>开头的名称，并且<strong>不建议</strong>使用这种方法导入）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> fibo <span class="keyword">import</span> *</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib(<span class="number">500</span>)</span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">5</span> <span class="number">8</span> <span class="number">13</span> <span class="number">21</span> <span class="number">34</span> <span class="number">55</span> <span class="number">89</span> <span class="number">144</span> <span class="number">233</span> <span class="number">377</span></span><br></pre></td></tr></table></figure></li><li><p>模块名使用<code>as</code>，直接把<code>as</code>后的名称与导入模块绑定</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> fibo <span class="keyword">as</span> fibo</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib.fib(<span class="number">500</span>)</span><br></pre></td></tr></table></figure></li><li><p>结合<code>from</code>一起使用</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> fibo <span class="keyword">import</span> fib <span class="keyword">as</span> fibonacci</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fibonacci(<span class="number">500</span>)</span><br></pre></td></tr></table></figure></li></ol><h3 id="以脚本方式运行模块"><a href="#以脚本方式运行模块" class="headerlink" title="以脚本方式运行模块"></a>以脚本方式运行模块</h3><p>通常在命令行执行脚本文件的语句：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python fibo.py &lt;arguments&gt;</span><br></pre></td></tr></table></figure><p>直接运行<code>.py</code>脚本会在我们看不见的地方默认的执行一个事情，即：**把<code>__name__</code>赋值为<code>&quot;__main__&quot;</code>**。</p><p>也就是把下列代码添加到了模块的末尾</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">import</span> sys</span><br><span class="line">    fib(<span class="built_in">int</span>(sys.argv[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>这样做的含义是，在模块作为”main”文件(脚本)进行执行的时候才会运行。</p><p>举个例子：</p><ol><li><p>当模块作为脚本文件执行时(会执行)：</p> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ python fibo.py 50</span><br><span class="line">0 1 1 2 3 5 8 13 21 34</span><br></pre></td></tr></table></figure></li><li><p>当模块被导入到其它模块或主程序时(不会执行)：</p> <figure class="highlight bat"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import fibo</span><br><span class="line">&gt;&gt;&gt; </span><br></pre></td></tr></table></figure></li></ol><h3 id="Python文件的编译"><a href="#Python文件的编译" class="headerlink" title="Python文件的编译"></a>Python文件的编译</h3><p>这部分在官网文档讲解的非常清晰，参考<a href="https://docs.python.org/zh-cn/3/tutorial/modules.html#compiled-python-files">6.1.3. “已编译的” Python 文件</a></p><p>这里附上“Python模块快速加载”的决策细节流程图：</p><p><img src="https://s21.ax1x.com/2024/03/14/pFcXu5T.png" alt="pyc_flow_chart"></p><h3 id="使用内置函数dir-查看模型定义的名称"><a href="#使用内置函数dir-查看模型定义的名称" class="headerlink" title="使用内置函数dir()查看模型定义的名称"></a>使用内置函数dir()查看模型定义的名称</h3><p><code>dir()</code>用于查找模块定义的名称，返回值为排序之后的字符串列表。</p><p>上实例：</p><ol><li><p><code>dir()</code>含参数时：</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> fibo, sys</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">dir</span>(fibo)</span><br><span class="line">[<span class="string">&#x27;__name__&#x27;</span>, <span class="string">&#x27;fib&#x27;</span>, <span class="string">&#x27;fib2&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">dir</span>(sys)</span><br><span class="line">[<span class="string">&#x27;__breakpointhook__&#x27;</span>, <span class="string">&#x27;__displayhook__&#x27;</span>, <span class="string">&#x27;__doc__&#x27;</span>, <span class="string">&#x27;__excepthook__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__interactivehook__&#x27;</span>, <span class="string">&#x27;__loader__&#x27;</span>, <span class="string">&#x27;__name__&#x27;</span>, <span class="string">&#x27;__package__&#x27;</span>, <span class="string">&#x27;__spec__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__stderr__&#x27;</span>, <span class="string">&#x27;__stdin__&#x27;</span>, <span class="string">&#x27;__stdout__&#x27;</span>, <span class="string">&#x27;__unraisablehook__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_clear_type_cache&#x27;</span>, <span class="string">&#x27;_current_frames&#x27;</span>, <span class="string">&#x27;_debugmallocstats&#x27;</span>, <span class="string">&#x27;_framework&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_getframe&#x27;</span>, <span class="string">&#x27;_git&#x27;</span>, <span class="string">&#x27;_home&#x27;</span>, <span class="string">&#x27;_xoptions&#x27;</span>, <span class="string">&#x27;abiflags&#x27;</span>, <span class="string">&#x27;addaudithook&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;api_version&#x27;</span>, <span class="string">&#x27;argv&#x27;</span>, <span class="string">&#x27;audit&#x27;</span>, <span class="string">&#x27;base_exec_prefix&#x27;</span>, <span class="string">&#x27;base_prefix&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;breakpointhook&#x27;</span>, <span class="string">&#x27;builtin_module_names&#x27;</span>, <span class="string">&#x27;byteorder&#x27;</span>, <span class="string">&#x27;call_tracing&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;callstats&#x27;</span>, <span class="string">&#x27;copyright&#x27;</span>, <span class="string">&#x27;displayhook&#x27;</span>, <span class="string">&#x27;dont_write_bytecode&#x27;</span>, <span class="string">&#x27;exc_info&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;excepthook&#x27;</span>, <span class="string">&#x27;exec_prefix&#x27;</span>, <span class="string">&#x27;executable&#x27;</span>, <span class="string">&#x27;exit&#x27;</span>, <span class="string">&#x27;flags&#x27;</span>, <span class="string">&#x27;float_info&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;float_repr_style&#x27;</span>, <span class="string">&#x27;get_asyncgen_hooks&#x27;</span>, <span class="string">&#x27;get_coroutine_origin_tracking_depth&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;getallocatedblocks&#x27;</span>, <span class="string">&#x27;getdefaultencoding&#x27;</span>, <span class="string">&#x27;getdlopenflags&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;getfilesystemencodeerrors&#x27;</span>, <span class="string">&#x27;getfilesystemencoding&#x27;</span>, <span class="string">&#x27;getprofile&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;getrecursionlimit&#x27;</span>, <span class="string">&#x27;getrefcount&#x27;</span>, <span class="string">&#x27;getsizeof&#x27;</span>, <span class="string">&#x27;getswitchinterval&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;gettrace&#x27;</span>, <span class="string">&#x27;hash_info&#x27;</span>, <span class="string">&#x27;hexversion&#x27;</span>, <span class="string">&#x27;implementation&#x27;</span>, <span class="string">&#x27;int_info&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;intern&#x27;</span>, <span class="string">&#x27;is_finalizing&#x27;</span>, <span class="string">&#x27;last_traceback&#x27;</span>, <span class="string">&#x27;last_type&#x27;</span>, <span class="string">&#x27;last_value&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;maxsize&#x27;</span>, <span class="string">&#x27;maxunicode&#x27;</span>, <span class="string">&#x27;meta_path&#x27;</span>, <span class="string">&#x27;modules&#x27;</span>, <span class="string">&#x27;path&#x27;</span>, <span class="string">&#x27;path_hooks&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;path_importer_cache&#x27;</span>, <span class="string">&#x27;platform&#x27;</span>, <span class="string">&#x27;prefix&#x27;</span>, <span class="string">&#x27;ps1&#x27;</span>, <span class="string">&#x27;ps2&#x27;</span>, <span class="string">&#x27;pycache_prefix&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;set_asyncgen_hooks&#x27;</span>, <span class="string">&#x27;set_coroutine_origin_tracking_depth&#x27;</span>, <span class="string">&#x27;setdlopenflags&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;setprofile&#x27;</span>, <span class="string">&#x27;setrecursionlimit&#x27;</span>, <span class="string">&#x27;setswitchinterval&#x27;</span>, <span class="string">&#x27;settrace&#x27;</span>, <span class="string">&#x27;stderr&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;stdin&#x27;</span>, <span class="string">&#x27;stdout&#x27;</span>, <span class="string">&#x27;thread_info&#x27;</span>, <span class="string">&#x27;unraisablehook&#x27;</span>, <span class="string">&#x27;version&#x27;</span>, <span class="string">&#x27;version_info&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;warnoptions&#x27;</span>]</span><br></pre></td></tr></table></figure></li><li><p><code>dir()</code>不含参数时：</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> fibo</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib = fibo.fib</span><br><span class="line"><span class="built_in">dir</span>()</span><br><span class="line">[<span class="string">&#x27;__builtins__&#x27;</span>, <span class="string">&#x27;__name__&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;fib&#x27;</span>, <span class="string">&#x27;fibo&#x27;</span>, <span class="string">&#x27;sys&#x27;</span>]</span><br></pre></td></tr></table></figure></li></ol><h2 id="包"><a href="#包" class="headerlink" title="包"></a>包</h2><p>包是通过使用“<strong>带点号模块名</strong>”来构造 Python 模块命名空间的一种方式。</p><p>例如，模块名 <code>A.B</code> 表示名为 <code>A</code> 的包中名为 <code>B</code> 的子模块。</p><blockquote><p>使用<code>modules.func</code>的这种调用方式还有一个好处，就是避免在不同模块中的功能函数命名冲突。</p><p>例如：在 NumPy 或 Pillow 等多模块包中很多功能函数命名相同，这样使用<code>np.func</code>或<code>Pillow.func</code>就不必担心彼此的<code>func</code>模块名冲突了。</p></blockquote><p>假设要为统一处理声音文件与声音数据设计一个模块集（“包”）。声音文件的格式很多（通常以扩展名来识别，例如：<code>.wav</code>，<code>.aiff</code>，<code>.au</code>），因此，为了不同文件格式之间的转换，需要创建和维护一个不断增长的模块集合。</p><p>为了实现对声音数据的不同处理（例如，混声、添加回声、均衡器功能、创造人工立体声效果），还要编写无穷无尽的模块流。</p><p>下面这个分级文件树展示了这个包的架构：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">sound/                          Top-level package</span><br><span class="line">      __init__.py               Initialize the sound package</span><br><span class="line">      formats/                  Subpackage for file format conversions</span><br><span class="line">              __init__.py</span><br><span class="line">              wavread.py</span><br><span class="line">              wavwrite.py</span><br><span class="line">              aiffread.py</span><br><span class="line">              aiffwrite.py</span><br><span class="line">              auread.py</span><br><span class="line">              auwrite.py</span><br><span class="line">              ...</span><br><span class="line">      effects/                  Subpackage for sound effects</span><br><span class="line">              __init__.py</span><br><span class="line">              echo.py</span><br><span class="line">              surround.py</span><br><span class="line">              reverse.py</span><br><span class="line">              ...</span><br><span class="line">      filters/                  Subpackage for filters</span><br><span class="line">              __init__.py</span><br><span class="line">              equalizer.py</span><br><span class="line">              vocoder.py</span><br><span class="line">              karaoke.py</span><br><span class="line">              ...</span><br></pre></td></tr></table></figure><p>导入包时，Python搜索<code>sys.path</code>里的目录，查找包的子目录。</p><p>需要有<code>__init__.py</code>文件才能让Python将包含改文件的目录当做“包”来处理。这样可以防止重名目录如<code>string</code>在无意中屏蔽后续出现在模块搜索路径中的有效模块。</p><p>最简单的情况就是，<code>__init__.py</code>可以是一个空文件，但是它也可以执行包的初始化代码或设置<code>__all__</code>变量，这将在稍后详细描述。</p><p>一些例子说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sound.effects.echo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载子模块 sound.effects.echo 必须通过全名来引用</span></span><br><span class="line">sound.effects.echo.echofilter(<span class="built_in">input</span>, output, delay)</span><br></pre></td></tr></table></figure><p>另一种导入子模块的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sound.effects <span class="keyword">import</span> echo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载子模块 echo，并且不必加包前缀</span></span><br><span class="line">echo.echofilter(<span class="built_in">input</span>, output, delay = <span class="number">0.7</span>, atten = <span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>还有一种，直接导入所需的函数或变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sound.effect.echo <span class="keyword">import</span> echofilter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载子模块 echo，使其函数 echofilter() 直接可用:</span></span><br><span class="line">echofilter(<span class="built_in">input</span>, output, delay = <span class="number">0.7</span>, atten = <span class="number">4</span>)</span><br></pre></td></tr></table></figure><h3 id="从包中导入"><a href="#从包中导入" class="headerlink" title="从包中导入*"></a>从包中导入*</h3><p>同理于模块的导入，同样不建议这样做。</p><p>一些需要提及的知识点：如果直接使用<code>*</code>进行导入，一般执行的操作为通过包中的<code>__init__.py</code>代码部分的以下<code>__all__</code>中的模块名列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__all__ = [<span class="string">&quot;echo&quot;</span>, <span class="string">&quot;surround&quot;</span>, <span class="string">&quot;reverse&quot;</span>]</span><br></pre></td></tr></table></figure><p><strong>子模块的命名有可能会受到本地定义名称的影响！</strong></p><p>模块中的模块如果和环境中已存在的模块重名，则会被本地<strong>先</strong>定义过的函数名称遮挡。以<code>reverse</code>函数为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">__all__ = [</span><br><span class="line">    <span class="string">&quot;echo&quot;</span>,      <span class="comment"># refers to the &#x27;echo.py&#x27; file</span></span><br><span class="line">    <span class="string">&quot;surround&quot;</span>,  <span class="comment"># refers to the &#x27;surround.py&#x27; file</span></span><br><span class="line">    <span class="string">&quot;reverse&quot;</span>,   <span class="comment"># !!! refers to the &#x27;reverse&#x27; function now !!!</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reverse</span>(<span class="params">msg: <span class="built_in">str</span></span>):  <span class="comment"># &lt;-- this name shadows the &#x27;reverse.py&#x27; submodule</span></span><br><span class="line">    <span class="keyword">return</span> msg[::-<span class="number">1</span>]    <span class="comment">#     in the case of a &#x27;from sound.effects import *&#x27;</span></span><br></pre></td></tr></table></figure><p>官方文档中，推荐的做法是：<code>frome package import submodule</code>.</p><h3 id="相对导入"><a href="#相对导入" class="headerlink" title="相对导入"></a>相对导入</h3><p>当包由多个子包构成（如示例中的 <code>sound</code> 包）时，可以使用绝对导入来引用同级包的子模块。</p><p>例如，如果 <code>sound.filters.vocoder</code> 模块需要使用 <code>sound.effects</code> 包中的 <code>echo</code> 模块，它可以使用 <code>from sound.effects import echo</code>。</p><p>你还可以编写相对导入代码，即使用 <code>from module import name</code> 形式的 import 语句。</p><p>这些导入使用<strong>前导点号</strong>来表示相对导入所涉及的当前包和上级包。</p><p>例如对于 <code>surround</code> 模块，可以使用:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> echo</span><br><span class="line"><span class="keyword">from</span> .. <span class="keyword">import</span> formats</span><br><span class="line"><span class="keyword">from</span> ..filters <span class="keyword">import</span> equalizer</span><br></pre></td></tr></table></figure><p><strong>注意，相对导入基于当前模块名。</strong></p><p>因为主模块名永远是 <code>&quot;__main__&quot;</code> ，所以如果计划将一个模块用作 Python 应用程序的主模块，那么该模块内的导入语句必须始终使用绝对导入。</p><h3 id="多目录中的包"><a href="#多目录中的包" class="headerlink" title="多目录中的包"></a>多目录中的包</h3><p>通过 <a href="https://docs.python.org/zh-cn/3/reference/import.html#path">__path__</a> 可以传入字符串列表，找到所有 <code>__init__.py</code>坐在目录的位置。该功能不常用，知道就好。</p><hr><p>相关参考：</p><ul><li><a href="https://docs.python.org/zh-cn/3/tutorial/modules.html#">6. 模块 — Python 3.12.2 文档</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D&quot;&gt;背景介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#tldr&quot;&gt;TL；DR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%A8%A1%E5%9D%97&quot;&gt;</summary>
      
    
    
    
    
    <category term="Python" scheme="https://zade23.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>【Git】合作开发中Git工作流程的细节</title>
    <link href="https://zade23.github.io/2024/03/11/%E3%80%90Git%E3%80%91%E5%90%88%E4%BD%9C%E5%BC%80%E5%8F%91%E4%B8%ADGit%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E7%9A%84%E7%BB%86%E8%8A%82/"/>
    <id>https://zade23.github.io/2024/03/11/%E3%80%90Git%E3%80%91%E5%90%88%E4%BD%9C%E5%BC%80%E5%8F%91%E4%B8%ADGit%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E7%9A%84%E7%BB%86%E8%8A%82/</id>
    <published>2024-03-11T08:39:05.000Z</published>
    <updated>2024-04-25T06:59:36.754Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E4%BB%93%E5%BA%93%E4%B8%AD%E5%90%84%E5%88%86%E6%94%AF%E7%9A%84%E8%81%8C%E8%B4%A3">仓库中各分支的职责</a></li><li><a href="#%E6%B8%85%E6%99%B0%E7%9A%84commit">清晰的Commit</a></li></ul><hr><p>记录开发过程中，提交代码仓库的一些经验总结（持续更新……）</p><h3 id="仓库中各分支的职责"><a href="#仓库中各分支的职责" class="headerlink" title="仓库中各分支的职责"></a>仓库中各分支的职责</h3><ol><li><p><strong>master分支</strong>：master分支是存放发布上线的代码的默认位置。当某个功能经过验证和测试后，将develop分支合并到master分支，并为该版本打上相应的标签（例如x.0.0，表示某个大版本的编号）。master分支应该是稳定且经过充分测试的代码。</p></li><li><p><strong>develop分支</strong>：develop分支是整体开发功能的主要分支。在开发阶段，所有功能的开发工作都应提交到develop分支。在功能开发完成后，通过测试和验证后，将develop分支合并到master分支，使功能正式上线。</p></li></ol><blockquote><p>扩展：</p><ul><li>除了master和develop分支外，还可以存在其他类型的分支，如<strong>feature</strong>分支、<strong>bugfix</strong>分支等。这些分支可以用于并行开发不同的功能或修复bug，并在开发完成后合并到develop分支。</li><li>开发团队通常采用分支策略，例如Git Flow或GitHub Flow，来管理不同分支之间的合并和发布流程，以确保代码质量和版本控制的有效管理。</li><li>主分支（如master或main）通常用于存放稳定的、可发布的代码，而开发分支（如develop）则用于整体功能的开发和集成。</li><li>通过使用不同的分支，可以实现并行开发、合理管理代码版本、隔离功能开发和修复等，从而提高团队的协作效率和代码质量。</li><li>重要的是，分支之间的合并应该经过适当的测试和验证，以确保代码的稳定性和功能的正确性。</li></ul></blockquote><h3 id="清晰的Commit"><a href="#清晰的Commit" class="headerlink" title="清晰的Commit"></a>清晰的Commit</h3><ol><li><strong>使用明确的动词</strong>：在提交信息的开头使用明确的动词来描述你的更改。例如，使用 “添加”、”修复”、”更新”、”重构” 等词语，以便其他人可以快速了解你的更改类型。</li><li><strong>保持简洁</strong>：提交信息应该简洁明了，尽量避免冗长的描述。使用简洁的语句来概括你的更改，并在需要时提供详细信息。</li><li><strong>提供相关上下文</strong>：除了简洁的概述外，确保提交信息提供足够的上下文信息，以便其他人能够理解你的更改原因和意图。如果有相关的问题、需求或讨论，可以引用相关的编号或链接。</li><li><strong>遵循团队或项目规范</strong>：根据你所在的团队或项目的规范，使用统一的提交格式和命名约定。这样可以帮助整个团队保持一致的提交信息风格，便于阅读和管理。</li><li><strong>避免无意义的提交信息</strong>：提交信息应该有实际的意义，避免使用模糊或不相关的描述。确保你的提交信息传达了有用的信息，而不仅仅是表明你进行了一次提交。</li><li><strong>使用标准的提交类型</strong>：参考常见的提交类型（如前面所示），选择最适合你更改类型的提交类型。这有助于其他人快速了解你的更改类型，并且在版本控制工具中进行过滤和分类。</li><li><strong>审查和校对</strong>：在提交之前，花一些时间审查和校对你的提交信息。确保拼写正确、语法清晰，并且信息准确传达你的更改。</li></ol><p>常见的提交格式实例：</p><p>当然，以下是一些常见的提交格式示例：</p><ol><li><p><strong>feat</strong>: 添加新功能或特性</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">feat: 添加用户注册功能</span><br></pre></td></tr></table></figure></li><li><p><strong>fix</strong>: 修复 bug</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fix: 修复登录页面样式错位的 bug</span><br></pre></td></tr></table></figure></li><li><p><strong>docs</strong>: 更新文档</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docs: 更新用户手册中的安装说明</span><br></pre></td></tr></table></figure></li><li><p><strong>style</strong>: 代码样式、格式调整</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">style: 格式化整个项目的代码风格</span><br></pre></td></tr></table></figure></li><li><p><strong>refactor</strong>: 重构代码，既不修复 bug 也不添加新功能</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">refactor: 重构用户管理模块的代码结构</span><br></pre></td></tr></table></figure></li><li><p><strong>test</strong>: 添加或修改测试代码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test: 添加用户注册页面的单元测试</span><br></pre></td></tr></table></figure></li><li><p><strong>chore</strong>: 构建过程或辅助工具的变动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chore: 更新构建脚本以支持新的依赖库</span><br></pre></td></tr></table></figure></li></ol><p>这一切的目的是为了让团队其他成员明确该提交所做的工作，同时也让自己回顾代码的时候明确自己的工作内容。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%BB%93%E5%BA%93%E4%B8%AD%E5%90%84%E5%88%86%E6%94%AF%E7%9A%84%E8%81%8C%E8%B4%A3&quot;&gt;仓库中各分支的职责&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%B</summary>
      
    
    
    
    <category term="Git Workflow" scheme="https://zade23.github.io/categories/Git-Workflow/"/>
    
    
    <category term="Git" scheme="https://zade23.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>更新GitLab仓库SSH</title>
    <link href="https://zade23.github.io/2024/03/07/%E6%9B%B4%E6%96%B0GitLab%E4%BB%93%E5%BA%93SSH/"/>
    <id>https://zade23.github.io/2024/03/07/%E6%9B%B4%E6%96%B0GitLab%E4%BB%93%E5%BA%93SSH/</id>
    <published>2024-03-07T02:09:37.000Z</published>
    <updated>2024-03-11T08:41:35.516Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E4%BB%8B%E7%BB%8D">介绍</a></li><li><a href="#%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90">问题分析</a></li><li><a href="#%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3">问题解决</a></li></ul><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>早上开工，在拉取代码仓库的时候，发现SSH过期，于是更新一下SSH为永久，并做一个记录。SSH过期命令行显示如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ git pull</span><br><span class="line">remote:</span><br><span class="line">remote: ========================================================================</span><br><span class="line">remote:</span><br><span class="line">remote: Your SSH key has expired.</span><br><span class="line">remote:</span><br><span class="line">remote: ========================================================================</span><br><span class="line">remote:</span><br><span class="line">fatal: Could not <span class="built_in">read</span> from remote repository.</span><br><span class="line"></span><br><span class="line">Please make sure you have the correct access rights</span><br><span class="line">and the repository exists.</span><br></pre></td></tr></table></figure><h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><p>直接将问题送给poe，让AI分析解答，根据分析结果一步步处理即可解决。结果如下：</p><p><a href="https://imgse.com/i/pFr02Yn"><img src="https://s21.ax1x.com/2024/03/07/pFr02Yn.png" alt="pFr02Yn.png"></a></p><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><ol><li><p>生成新的SSH密码对。<code>ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot;</code>，该命令将生成一个 4096 位的 RSA 密钥对，替换邮箱地址为你GitLab注册邮箱。</p></li><li><p>之后会有两次询问你保存位置和输入密码的选项，点击 Enter 键跳过选项即可。显示结果如下：</p><p> <a href="https://imgse.com/i/pFr0XSx"><img src="https://s21.ax1x.com/2024/03/07/pFr0XSx.png" alt="pFr0XSx.png"></a></p></li><li><p>复制生成的SSH，window使用:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clip &lt; ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure></li><li><p>找到GitLab中的SSH Keys一栏，粘贴，保存：<br> <a href="https://imgse.com/i/pFrBZX8"><img src="https://s21.ax1x.com/2024/03/07/pFrBZX8.png" alt="pFrBZX8.png"></a></p></li><li><p>可以拉取仓库了</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%BB%8B%E7%BB%8D&quot;&gt;介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90&quot;&gt;问题分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E9%97%AE</summary>
      
    
    
    
    <category term="Git Workflow" scheme="https://zade23.github.io/categories/Git-Workflow/"/>
    
    
    <category term="Git" scheme="https://zade23.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>conda环境报错解决：invalid choice: &#39;activate&#39; </title>
    <link href="https://zade23.github.io/2023/11/22/conda%E7%8E%AF%E5%A2%83%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3%EF%BC%9Ainvalid-choice-activate/"/>
    <id>https://zade23.github.io/2023/11/22/conda%E7%8E%AF%E5%A2%83%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3%EF%BC%9Ainvalid-choice-activate/</id>
    <published>2023-11-22T08:04:41.000Z</published>
    <updated>2024-03-14T03:02:38.054Z</updated>
    
    <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>新部署的服务器上进行LLM模型的微调工作，在配置Anaconde环境后输入环境启动命令 <code>conda activate</code> ，出现从来没有见过的报错：<code>anaconda conda: error: argument command: invalid choice when trying to update packages</code></p><h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><p>环境问题，直接Google。最终在 GitHub 里<a href="https://github.com/conda/conda/issues/13022">官方仓库</a>和 <a href="https://stackoverflow.com/questions/53995171/anaconda-conda-error-argument-command-invalid-choice-when-trying-to-update-pa">Stack Overflow</a> 上找到一致的答复：安装Anaconde后没有进行初始化。</p><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><ol><li><p>终端运行 <code>conda init zsh</code></p></li><li><p>之后重启 shell&#x2F;Terminal (不重启依然报错)</p></li><li><p>终端运行 <code>conda activate env_name</code> 即可</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;新部署的服务器上进行LLM模型的微调工作，在配置Anaconde环境后输入环境启动命令 &lt;code&gt;conda activate&lt;/code</summary>
      
    
    
    
    <category term="Deeplearning" scheme="https://zade23.github.io/categories/Deeplearning/"/>
    
    
    <category term="conda" scheme="https://zade23.github.io/tags/conda/"/>
    
    <category term="environment" scheme="https://zade23.github.io/tags/environment/"/>
    
  </entry>
  
  <entry>
    <title>玩转Docker学习笔记</title>
    <link href="https://zade23.github.io/2023/11/07/%E7%8E%A9%E8%BD%ACDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://zade23.github.io/2023/11/07/%E7%8E%A9%E8%BD%ACDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2023-11-07T07:30:56.000Z</published>
    <updated>2024-03-11T10:00:59.954Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E9%95%9C%E5%83%8F%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4">镜像相关指令</a></li><li><a href="#%E5%AE%B9%E5%99%A8%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4">容器相关指令</a></li><li><a href="#%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4">网络相关指令</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E5%8D%B7%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4">数据卷相关指令</a></li><li><a href="#docker-compose%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4%E4%BD%BF%E7%94%A8docker-compose">Docker Compose相关指令（使用<code>docker-compose</code>）</a></li><li><a href="#%E9%80%9A%E8%BF%87-dockerfile-%E6%9E%84%E5%BB%BA-docker-image">通过 DockerFile 构建 Docker Image</a></li></ul><hr><h3 id="镜像相关指令"><a href="#镜像相关指令" class="headerlink" title="镜像相关指令"></a>镜像相关指令</h3><ul><li><code>docker pull &lt;image&gt;</code>：从远程仓库拉取一个镜像或仓库到本地。</li><li><code>docker build -t &lt;tag&gt; .</code>：使用当前目录的Dockerfile构建一个新的镜像。</li><li><code>docker images</code>：列出本地存储的镜像。</li><li><code>docker rmi &lt;image&gt;</code>：删除一个或多个本地的镜像。</li><li><code>docker tag &lt;image&gt; &lt;new_tag&gt;</code>：为镜像添加一个新的标签。</li></ul><h3 id="容器相关指令"><a href="#容器相关指令" class="headerlink" title="容器相关指令"></a>容器相关指令</h3><ul><li><code>docker run &lt;image&gt;</code>：创建一个新的容器并运行一个命令。</li><li><code>docker ps</code>：列出当前运行的容器。</li><li><code>docker ps -a</code>：列出所有容器，包括未运行的。</li><li><code>docker stop &lt;container&gt;</code>：停止一个或多个正在运行的容器。</li><li><code>docker start &lt;container&gt;</code>：启动一个或多个已停止运行的容器。</li><li><code>docker restart &lt;container&gt;</code>：重启容器。</li><li><code>docker rm &lt;container&gt;</code>：删除一个或多个容器。</li><li><code>docker logs &lt;container&gt;</code>：获取容器的日志。</li><li><code>docker exec -it &lt;container&gt; &lt;command&gt;</code>：在运行的容器中执行命令，通常用于进入容器。</li></ul><h3 id="网络相关指令"><a href="#网络相关指令" class="headerlink" title="网络相关指令"></a>网络相关指令</h3><ul><li><code>docker network ls</code>：列出所有网络。</li><li><code>docker network create &lt;options&gt; &lt;network_name&gt;</code>：创建一个新的网络。</li><li><code>docker network rm &lt;network&gt;</code>：删除一个或多个网络。</li><li><code>docker network connect &lt;network&gt; &lt;container&gt;</code>：连接一个容器到一个网络。</li><li><code>docker network disconnect &lt;network&gt; &lt;container&gt;</code>：断开容器与网络的连接。</li></ul><h3 id="数据卷相关指令"><a href="#数据卷相关指令" class="headerlink" title="数据卷相关指令"></a>数据卷相关指令</h3><ul><li><code>docker volume create &lt;name&gt;</code>：创建一个新的卷。</li><li><code>docker volume ls</code>：列出所有卷。</li><li><code>docker volume rm &lt;volume&gt;</code>：删除一个或多个卷。</li><li><code>docker volume inspect &lt;volume&gt;</code>：显示详细的卷信息。</li></ul><h3 id="Docker-Compose相关指令（使用docker-compose）"><a href="#Docker-Compose相关指令（使用docker-compose）" class="headerlink" title="Docker Compose相关指令（使用docker-compose）"></a>Docker Compose相关指令（使用<code>docker-compose</code>）</h3><ul><li><code>docker-compose up</code>：在后台启动并运行整个应用。</li><li><code>docker-compose down</code>：停止并移除容器，网络，图像和挂载卷。</li><li><code>docker-compose build</code>：构建或重建服务关联的镜像。</li><li><code>docker-compose logs</code>：查看服务的日志输出。</li></ul><p>这些命令代表了Docker操作的基础，但是实际使用中可能还会遇到更复杂的场景和高级特性。建议通过官方文档或其他学习资源深入理解每个命令的用法和选项。</p><h3 id="通过-DockerFile-构建-Docker-Image"><a href="#通过-DockerFile-构建-Docker-Image" class="headerlink" title="通过 DockerFile 构建 Docker Image"></a>通过 DockerFile 构建 Docker Image</h3><p>一个 Image 是通过一个 DockerFile 定义的，然后使用 docker build 命令构建它。</p><p>DockerFile 中的每一条命令的执行结果都会成为 Image 中的一个 Layer。</p><p>这里，我们通过 Build 一个镜像，来观察 Image 的分层机制：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用官方的Python运行时环境作为基础镜像</span></span><br><span class="line"><span class="keyword">FROM</span> python:<span class="number">2.7</span>-slim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置工作目录为容器内的/app。如果目录不存在，Docker会自动为你创建这个目录。</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /app</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将当前目录（Dockerfile所在目录）的内容复制到容器内的/app目录中。</span></span><br><span class="line"><span class="comment"># 这意味着你的应用代码和依赖文件（比如requirements.txt）都会被复制进去。</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> . /app</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用RUN命令执行pip安装命令来安装requirements.txt中列出的所有依赖。</span></span><br><span class="line"><span class="comment"># 这里使用了--trusted-host选项来指定可信的PyPI主机，避免SSL证书验证问题。</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip install --trusted-host pypi.python.org -r requirements.txt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用EXPOSE指令来告诉Docker容器内的应用将会在80端口上监听连接。</span></span><br><span class="line"><span class="comment"># 需要注意的是，EXPOSE指令本身不会使容器的端口对外界开放，</span></span><br><span class="line"><span class="comment"># 它更多的是一种文档化的作用，真正的端口映射需要在运行容器时通过docker run的-p选项来指定。</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用ENV指令设置一个环境变量。这里定义了一个名为NAME的环境变量，值为World。</span></span><br><span class="line"><span class="comment"># 环境变量可以在容器运行时被应用程序读取，用于配置应用行为。</span></span><br><span class="line"><span class="keyword">ENV</span> NAME World</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用CMD指令指定容器启动时运行的命令。</span></span><br><span class="line"><span class="comment"># 这里的命令是“python app.py”，即使用Python解释器来运行app.py脚本。</span></span><br><span class="line"><span class="comment"># CMD的主要作用是指定容器的默认执行命令。如果在docker run命令后面指定了其他命令，CMD指定的命令将被覆盖。</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;python&quot;</span>, <span class="string">&quot;app.py&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>这份<code>Dockerfile</code>基本上覆盖了构建一个简单Python应用的Docker镜像所需的所有步骤。从选择基础镜像开始，设置工作目录，复制应用代码，安装依赖，到最后指定运行时的命令和暴露的端口，每一步都为镜像的构建提供了必要的指令和配置。</p><p>最终的构建结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">root@rds-k8s-18-svr0:~/xuran/exampleimage# docker build -t hello ./</span><br><span class="line">Sending build context to Docker daemon  5.12 kB</span><br><span class="line">Step 1/7 : FROM python:2.7-slim</span><br><span class="line"> ---&gt; 804b0a01ea83</span><br><span class="line">Step 2/7 : WORKDIR /app</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 6d93c5b91703</span><br><span class="line">Step 3/7 : COPY . /app</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; feddc82d321b</span><br><span class="line">Step 4/7 : RUN pip install --trusted-host pypi.python.org -r requirements.txt</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 94695df5e14d</span><br><span class="line">Step 5/7 : EXPOSE 81</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 43c392d51dff</span><br><span class="line">Step 6/7 : ENV NAME World</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 78c9a60237c8</span><br><span class="line">Step 7/7 : CMD python app.py</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; a5ccd4e1b15d</span><br><span class="line">Successfully built a5ccd4e1b15d</span><br></pre></td></tr></table></figure><p>下面是对这些信息每一步的详细解释：</p><ul><li><p><strong>Sending build context to Docker daemon 5.12 kB</strong><br>这一行表示Docker客户端正在将构建上下文发送给Docker守护进程。构建上下文是指Dockerfile所在目录的内容，Docker会将这些内容打包发送给守护进程。这里的大小是5.12kB，表示你的应用代码和依赖文件等总共大小。</p></li><li><p><strong>Step 1&#x2F;7 : FROM python:2.7-slim</strong><br>这是Dockerfile中的第一步，它基于<code>python:2.7-slim</code>这个镜像来构建新的镜像。<code>---&gt; 804b0a01ea83</code>是基础镜像的ID。</p></li><li><p><strong>Step 2&#x2F;7 : WORKDIR &#x2F;app</strong><br>设置工作目录为<code>/app</code>。如果不存在，Docker会自动创建这个目录。<code>---&gt; Using cache ---&gt; 6d93c5b91703</code>表示这一步使用了缓存，<code>6d93c5b91703</code>是这一层的ID。</p></li><li><p><strong>Step 3&#x2F;7 : COPY . &#x2F;app</strong><br>将构建上下文（Dockerfile所在的目录）的内容复制到容器内的<code>/app</code>目录。<code>---&gt; Using cache ---&gt; feddc82d321b</code>表明这一步也使用了缓存。</p></li><li><p><strong>Step 4&#x2F;7 : RUN pip install –trusted-host pypi.python.org -r requirements.txt</strong><br>在容器内执行<code>pip install</code>命令，安装<code>requirements.txt</code>文件中列出的Python依赖包。<code>---&gt; Using cache ---&gt; 94695df5e14d</code>说明这一步同样使用了缓存。</p></li><li><p><strong>Step 5&#x2F;7 : EXPOSE 81</strong><br>通知Docker容器在运行时将会监听81端口。注意这里与前面提到的Dockerfile中的<code>EXPOSE 80</code>不同，可能是因为Dockerfile被修改了但构建输出没有更新。<code>---&gt; Using cache ---&gt; 43c392d51dff</code>表示使用了缓存。</p></li><li><p><strong>Step 6&#x2F;7 : ENV NAME World</strong><br>设置环境变量<code>NAME</code>的值为<code>World</code>。这个环境变量可以在容器运行时被应用程序使用。<code>---&gt; Using cache ---&gt; 78c9a60237c8</code>也显示这一步使用了缓存。</p></li><li><p><strong>Step 7&#x2F;7 : CMD [“python”, “app.py”]</strong><br>指定容器启动时默认执行的命令。这里是运行<code>app.py</code>脚本。<code>---&gt; Using cache ---&gt; a5ccd4e1b15d</code>意味着这一步也利用了之前的构建缓存。</p></li><li><p><strong>Successfully built a5ccd4e1b15d</strong><br>最后，显示了成功构建的镜像ID为<code>a5ccd4e1b15d</code>。</p></li></ul><p>在这个过程中，<code>Using cache</code>表明Docker发现之前的构建步骤与当前的完全一致，因此它复用了之前的结果来加快构建过程。如果你想要强制Docker重新执行每个步骤而不使用缓存，可以在构建时添加<code>--no-cache</code>选项。</p><hr><p>参考链接：</p><ul><li><a href="https://www.docker.com/101-tutorial/">Docker 101 Tutorial</a></li><li><a href="http://t.csdnimg.cn/cApYV">Docker原理（图解+秒懂+史上最全）</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E9%95%9C%E5%83%8F%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4&quot;&gt;镜像相关指令&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%AE%B9%E5%99%A8%E7%9B%B8%E5%85%B</summary>
      
    
    
    
    
    <category term="Docker" scheme="https://zade23.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-tutorial-official</title>
    <link href="https://zade23.github.io/2023/10/31/pytorch-tutorial-official/"/>
    <id>https://zade23.github.io/2023/10/31/pytorch-tutorial-official/</id>
    <published>2023-10-31T08:49:36.000Z</published>
    <updated>2024-03-14T03:02:15.673Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#pytorch-%E5%9F%BA%E7%A1%80">Pytorch 基础</a><ul><li><a href="#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8">快速入门</a><ul><li><a href="#%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE">处理数据</a></li><li><a href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B">构建模型</a><ul><li><a href="#%E5%9C%A8-pytorch-%E4%B8%AD%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">在 PyTorch 中构建神经网络</a><ul><li><a href="#%E6%A8%A1%E5%9E%8B%E5%B1%82-model-layers">模型层 Model Layers</a></li><li><a href="#nnflatten">nn.Flatten</a></li><li><a href="#nnlinear">nn.Linear</a></li><li><a href="#nnrelu">nn.ReLU</a></li><li><a href="#nnsequential">nn.Sequential</a></li><li><a href="#nnsoftmax">nn.Softmax</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0">模型的参数</a></li></ul></li></ul></li></ul></li><li><a href="#%E5%BC%A0%E9%87%8F-tensor">张量 Tensor</a><ul><li><a href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%BC%A0%E9%87%8F">初始化张量</a><ul><li><a href="#%E7%9B%B4%E6%8E%A5%E6%9D%A5%E8%87%AA%E6%95%B0%E6%8D%AE">直接来自数据</a></li><li><a href="#%E6%9D%A5%E8%87%AA%E5%8F%A6%E4%B8%80%E4%B8%AA-tensor">来自另一个 Tensor</a></li><li><a href="#%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BA%E5%80%BC%E6%88%96%E5%B8%B8%E9%87%8F">使用随机值或常量</a></li></ul></li><li><a href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7">张量的属性</a></li><li><a href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E6%93%8D%E4%BD%9C">张量的操作</a><ul><li><a href="#%E7%B1%BB%E4%BC%BC-numpy-%E7%9A%84%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87%E6%93%8D%E4%BD%9C">类似 NumPy 的索引和切片操作</a></li><li><a href="#tensor-%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BF%9E%E6%8E%A5">Tensor 之间的连接</a></li><li><a href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97">张量的算术运算</a></li><li><a href="#%E5%8D%95%E4%B8%80%E5%85%83%E7%B4%A0%E5%BC%A0%E9%87%8F">单一元素张量</a></li><li><a href="#%E5%8E%9F%E5%9C%B0%E6%93%8D%E4%BD%9C%E4%B8%8D%E9%80%9A%E8%BF%87%E8%BF%94%E5%9B%9E%E7%9A%84%E6%96%B9%E5%BC%8F%E8%BF%9B%E8%A1%8C%E5%8E%9F%E5%9C%B0%E4%BF%AE%E6%94%B9">原地操作(不通过返回的方式进行原地修改)</a></li></ul></li><li><a href="#tensor-%E5%92%8C-numpy-%E4%BA%8C%E8%80%85%E8%BD%AC%E6%8D%A2">Tensor 和 Numpy 二者转换</a><ul><li><a href="#tensor2numpy_array">Tensor2NumPy_array</a></li><li><a href="#numpy_array2tensor">NumPy_array2Tensor</a></li></ul></li></ul></li><li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C">数据集的相关操作</a><ul><li><a href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD">数据加载</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%AD%E4%BB%A3%E5%92%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96">数据集迭代和数据可视化</a></li><li><a href="#%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86">创建自定义的数据集</a><ul><li><a href="#_init_">_<em>init</em>_</a></li><li><a href="#_len_">_<em>len</em>_</a></li><li><a href="#_getitem_">_<em>getitem</em>_</a></li></ul></li><li><a href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E5%B9%B6%E4%BD%BF%E7%94%A8-dataloader-%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83">准备数据并使用 DataLoader 进行训练</a></li><li><a href="#%E9%81%8D%E5%8E%86-dataloader">遍历 DataLoader</a></li></ul></li><li><a href="#%E8%BD%AC%E6%8D%A2">转换</a><ul><li><a href="#totensor">ToTensor()</a></li><li><a href="#lambda-%E8%BD%AC%E6%8D%A2">Lambda 转换</a></li></ul></li><li><a href="#%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86torchautograd">（核心内容）自动微分(<code>TORCH.AUTOGRAD</code>)</a><ul><li><a href="#%E5%BC%A0%E9%87%8F%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97%E5%9B%BE">张量、函数、计算图</a></li><li><a href="#%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97">梯度计算</a></li><li><a href="#%E7%A6%81%E7%94%A8%E6%A2%AF%E5%BA%A6%E8%BF%BD%E8%B8%AA">禁用梯度追踪</a></li><li><a href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E9%83%A8%E5%88%86%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB">计算图部分扩展阅读</a></li><li><a href="#%E5%8F%AF%E9%80%89%E9%98%85%E8%AF%BB%E5%BC%A0%E9%87%8F%E6%A2%AF%E5%BA%A6-%E5%92%8C-%E9%9B%85%E9%98%81%E6%AF%94%E7%A7%AFjacobian_products">可选阅读：张量梯度 和 雅阁比积(Jacobian_Products)</a></li></ul></li><li><a href="#%E9%87%8D%E8%A6%81%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0">（重要）优化模型参数</a><ul><li><a href="#%E5%85%88%E5%86%B3%E6%9D%A1%E4%BB%B6%E4%BB%A3%E7%A0%81">先决条件代码</a></li><li><a href="#%E8%B6%85%E5%8F%82%E6%95%B0">超参数</a></li><li><a href="#%E4%BC%98%E5%8C%96%E5%BE%AA%E7%8E%AF">优化循环</a></li><li><a href="#%E5%85%A8%E8%BF%87%E7%A8%8B%E6%95%B0%E6%8D%AE%E8%B7%9F%E8%B8%AA">全过程数据跟踪</a></li></ul></li><li><a href="#%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B">保存、加载和使用模型</a><ul><li><a href="#%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D">保存&#x2F;加载模型权重</a></li><li><a href="#%E9%80%9A%E8%BF%87%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BD%A2%E7%8A%B6%E5%8F%82%E6%95%B0%E8%BF%9B%E8%A1%8C%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD">通过模型的形状参数进行保存&#x2F;加载</a></li></ul></li><li><a href="#%E5%9C%A8-pytorch-%E4%B8%AD%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E5%B8%B8%E8%A7%84-checkpoint">在 PyTorch 中保存和加载常规 Checkpoint</a><ul><li><a href="#%E4%BB%8B%E7%BB%8D">介绍</a></li><li><a href="#%E8%AE%BE%E7%BD%AE">设置</a></li><li><a href="#%E6%AD%A5%E9%AA%A4">步骤</a><ul><li><a href="#1%E5%AF%BC%E5%85%A5%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E6%89%80%E9%9C%80%E8%A6%81%E7%9A%84%E5%BA%93">1.导入加载数据所需要的库</a></li><li><a href="#2%E5%AE%9A%E4%B9%89%E5%92%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">2.定义和初始化神经网络</a></li><li><a href="#3%E5%88%9D%E5%A7%8B%E5%8C%96%E4%BC%98%E5%8C%96%E5%99%A8">3.初始化优化器</a></li><li><a href="#4%E4%BF%9D%E5%AD%98%E5%B8%B8%E8%A7%84%E6%A3%80%E6%9F%A5%E7%82%B9">4.保存常规检查点</a></li><li><a href="#5%E5%8A%A0%E8%BD%BD%E5%B8%B8%E8%A7%84%E6%A3%80%E6%9F%A5%E7%82%B9">5.加载常规检查点</a></li></ul></li></ul></li><li><a href="#%E4%BB%8E-checkpoint-%E4%B8%AD%E5%8A%A0%E8%BD%BD-nnmodule-%E7%9A%84%E6%8A%80%E5%B7%A7">从 Checkpoint 中加载 <code>nn.Module</code> 的技巧</a><ul><li><a href="#%E6%B4%BB%E7%94%A8-torchloadmmap--true">活用 <code>torch.load(mmap = True)</code></a></li><li><a href="#%E6%B4%BB%E7%94%A8-torchdevicemeta">活用 <code>torch.device(&quot;meta&quot;)</code></a></li><li><a href="#%E6%B4%BB%E7%94%A8-load_state_dictassign--true">活用 <code>load_state_dict(assign = True)</code></a></li><li><a href="#%E7%BB%93%E8%AE%BA">结论</a></li></ul></li></ul></li></ul><h1 id="Pytorch-基础"><a href="#Pytorch-基础" class="headerlink" title="Pytorch 基础"></a>Pytorch 基础</h1><blockquote><p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html">https://pytorch.org/tutorials/beginner/basics/intro.html</a></p></blockquote><p>大多数机器学习工作流：</p><ul><li>处理数据</li><li>创建模型</li><li>优化模型参数</li><li>保存训练后模型</li></ul><p>通过 Pytorch 基础部分的内容，读者可以完整的走完一整个MachineLearning的工作流，若读者对其中某个环节不理解或感兴趣，针对这些工作流中的每一个环节都有相关的扩展阅读链接。</p><p>我们将使用 FashionMNIST 数据集训练一个神经网络，该神经网络预测输入图像是否属于一下类别之一：T恤&#x2F;上衣、裤子、套头衫、连衣裙、外套、凉鞋、成山、运动鞋、包包、靴子。（是个多分类任务）</p><h2 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h2><blockquote><p><a href="https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html</a></p></blockquote><p>本节会快速走完一个机器学习多分类的Demo，以此快速了解流程中必要的基本ML相关API。</p><h3 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h3><p>Pytorch 中有两个用于处理数据的子库 <code>torch.utils.data.DataLoader</code> 和 <code>torch.utils.data.Dataset.Dataset</code>。顾名思义，<code>Dataset</code> 存储样本及其相应的标签，并将 <code>DataLoader</code> 可迭代对象包装在 <code>Dataset</code> 中。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvison.transforms <span class="keyword">import</span> ToTensor</span><br></pre></td></tr></table></figure><p>通过上面的引用(import)，我们可以发现：Pytorch 中有非常多的子库，这些子库专注于某一特定的领域，例如： <a href="https://pytorch.org/text/stable/index.html">TorchText</a>, <a href="https://pytorch.org/vision/stable/index.html">TorchVision</a>, 和 <a href="https://pytorch.org/audio/stable/index.html">TorchAudio</a>, 这些所有子库中都包含相应的数据集。</p><p>本次教程中我们使用 <code>TorchVision</code> 数据集。</p><p>该 <code>torchvision,.datasets</code> 模块包含 <code>Dataset</code> 来自现实世界中的视觉图像数据，最经典的有：CIFAT，COCO(<a href="https://pytorch.org/vision/stable/datasets.html">full list here</a>)</p><p>本次教程中我们使用 <code>FashionMNIST</code> 数据集。每个 <code>TorchVison</code> 下的 <code>Dataset</code> 都包含两个参数：<code>transform</code> 和 <code>target_transform</code> 分别用来<strong>修改样本</strong>与<strong>打标签</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从公开数据集中读取训练数据</span></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">True</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = ToTensor(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从公开数据集中读取测试数据</span></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">False</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = Totensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>下图是在 colab 中运行上面程序块的输出结果：<br><a href="https://imgse.com/i/pin7WJf"><img src="https://z1.ax1x.com/2023/11/01/pin7WJf.md.jpg" alt="pin7WJf.md.jpg"></a></p><p>至此为止，通过上面的工作，我们将 <code>Dataset</code> 作为参数传递给了 <code>DataLoader</code> 。同时封装了相关数据集作为一个可迭代的对象，支持自动批处理、采样、洗牌(shuffling)、和多进程的数据加载。</p><hr><p>下一步中，我们定义 <code>batch_size = 64</code> ，即 <code>dataloader</code> 可迭代中的每个元素都将返回一批含有64个特征的标签。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据读取工具</span></span><br><span class="line">train_dataloader = DataLoader(tarain_data, batch_size = batch_size)</span><br><span class="line">test_dataloader = DataLoader(tast_data, batch_size = batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of X [N, C, H, W]: <span class="subst">&#123;X.shape&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of y []: <span class="subst">&#123;y.shape&#125;</span> <span class="subst">&#123;y.dtpye&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])</span><br><span class="line">Shape of y: torch.Size([64]) torch.int64</span><br></pre></td></tr></table></figure><blockquote><p>关于 <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">loading data in PyTorch</a> 的详细说明</p></blockquote><hr><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><p>为了在 Pytorch 中定义神经网络，我们创建一个继承自  <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module</a> 的类。</p><p>我们通过 <code>__init__</code> 函数定义神经网络的层，并指明数据如何通过 <code>forward</code> 函数进入神经网络层。</p><blockquote><p>在设备允许的情况下，推荐使用GPU来加速神经网络的运算操作。</p></blockquote><p>代码实现：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取用于训练的设备（cpu/gpu/mps）</span></span><br><span class="line">device = (</span><br><span class="line">    <span class="string">&quot;cuda&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">else</span> <span class="string">&quot;mps&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.backends.mps.is_available()</span><br><span class="line">    <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using <span class="subst">&#123;device&#125;</span> device&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义神经网络的模型结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><p>打印结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Using cpu device</span><br><span class="line">NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="在-PyTorch-中构建神经网络"><a href="#在-PyTorch-中构建神经网络" class="headerlink" title="在 PyTorch 中构建神经网络"></a>在 PyTorch 中构建神经网络</h4><blockquote><p>这一部分是对‘构建模型’部分的一点补充说明，也是 PyTorch 官网教程中的扩展阅读部分</p></blockquote><p>神经网络是由多个对数据进行操作的层&#x2F;模型组合而成的。<a href="https://pytorch.org/docs/stable/nn.html">torch.nn</a>  命名空间几乎已经提供了构建一个神经网络所需要用到的<strong>所有模块</strong>。</p><p>所有模块都在 PyTorch 下的子块  <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module</a> 中提供。</p><p>基于这样的结构化嵌套模块，整个神经网络可以自由的进行构建和管理复杂的架构。</p><p>在上面的代码块中，我们通过 <code>NeuralNetwork</code> 函数定义了一个神经网络模型 <code>model</code>。</p><p>为了使用该模型，我们将输入数据传递给它。这个操作将执行 <code>forward</code> 操作和一些<a href="https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866">后台操作</a>。</p><p>请记住：不要直接使用 <code>model.forward()</code> !</p><hr><p>通过输入操作调用模型，最后将返回一个二维张量，其中 dim &#x3D; 0 对应于每个类别的 10 个原始预测输出，dim &#x3D; 1 对应与每个输出的单个值。</p><p>我们可以通过 <code>nn.Softmax</code> 模块实例传递对结果预测的概率来进行最终预测概率的判断。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device = device)</span><br><span class="line">logits = model(X)</span><br><span class="line">pred_probab = nn.Softmax(dim = <span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>打印结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Predicted class: tensor([7], device = &#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><h5 id="模型层-Model-Layers"><a href="#模型层-Model-Layers" class="headerlink" title="模型层 Model Layers"></a>模型层 Model Layers</h5><p>分解 <code>FashionMNIST</code> 模型中的各层。为了说明这一点，我们通过获取一个包含 3 张大小为 28*28 的小批量图像样本，看看当数据传递到网络时会发生什么。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_image = torch.rand(<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(input_image.size())</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 28, 28])</span><br></pre></td></tr></table></figure><h5 id="nn-Flatten"><a href="#nn-Flatten" class="headerlink" title="nn.Flatten"></a>nn.Flatten</h5><p>初始化 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html">nn.Flatten</a> 层，将每个2D 28*28 图像转换成包含 784 个像素值的连续数组（保持小批量尺寸(dim &#x3D; 0)）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line"><span class="built_in">print</span>(flat_image.size())</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 784])</span><br></pre></td></tr></table></figure><h5 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear"></a>nn.Linear</h5><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">线性层模块</a>通过输入的权重w和偏差值b进行线性变换。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layer1 = nn.Linear(in_features = <span class="number">28</span>*<span class="number">28</span>, out_features = <span class="number">20</span>)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line"><span class="built_in">print</span>(hidden1.size())</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 20])</span><br></pre></td></tr></table></figure><h5 id="nn-ReLU"><a href="#nn-ReLU" class="headerlink" title="nn.ReLU"></a>nn.ReLU</h5><p>非线性激活函数可以在模型的输入输出之间创建复杂的映射关系。激活函数通过引入非线性的变换帮助神经网络学习各种现象。</p><p>在实例模型中，我们在线性层之间使用ReLU激活函数。但还有其他激活函数可以在模型的线性层中间作为激活函数使用，详情参考：<a href="https://zh.wikipedia.org/zh-cn/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">激活函数-wiki</a></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Before ReLU: <span class="subst">&#123;hidden1&#125;</span>\n\n&quot;</span>)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Aftr RelU: <span class="subst">&#123;hidden1&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Before ReLU: tensor([[ 0.4158, -0.0130, -0.1144,  0.3960,  0.1476, -0.0690, -0.0269,  0.2690,</span><br><span class="line">          0.1353,  0.1975,  0.4484,  0.0753,  0.4455,  0.5321, -0.1692,  0.4504,</span><br><span class="line">          0.2476, -0.1787, -0.2754,  0.2462],</span><br><span class="line">        [ 0.2326,  0.0623, -0.2984,  0.2878,  0.2767, -0.5434, -0.5051,  0.4339,</span><br><span class="line">          0.0302,  0.1634,  0.5649, -0.0055,  0.2025,  0.4473, -0.2333,  0.6611,</span><br><span class="line">          0.1883, -0.1250,  0.0820,  0.2778],</span><br><span class="line">        [ 0.3325,  0.2654,  0.1091,  0.0651,  0.3425, -0.3880, -0.0152,  0.2298,</span><br><span class="line">          0.3872,  0.0342,  0.8503,  0.0937,  0.1796,  0.5007, -0.1897,  0.4030,</span><br><span class="line">          0.1189, -0.3237,  0.2048,  0.4343]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">After ReLU: tensor([[0.4158, 0.0000, 0.0000, 0.3960, 0.1476, 0.0000, 0.0000, 0.2690, 0.1353,</span><br><span class="line">         0.1975, 0.4484, 0.0753, 0.4455, 0.5321, 0.0000, 0.4504, 0.2476, 0.0000,</span><br><span class="line">         0.0000, 0.2462],</span><br><span class="line">        [0.2326, 0.0623, 0.0000, 0.2878, 0.2767, 0.0000, 0.0000, 0.4339, 0.0302,</span><br><span class="line">         0.1634, 0.5649, 0.0000, 0.2025, 0.4473, 0.0000, 0.6611, 0.1883, 0.0000,</span><br><span class="line">         0.0820, 0.2778],</span><br><span class="line">        [0.3325, 0.2654, 0.1091, 0.0651, 0.3425, 0.0000, 0.0000, 0.2298, 0.3872,</span><br><span class="line">         0.0342, 0.8503, 0.0937, 0.1796, 0.5007, 0.0000, 0.4030, 0.1189, 0.0000,</span><br><span class="line">         0.2048, 0.4343]], grad_fn=&lt;ReluBackward0&gt;)</span><br></pre></td></tr></table></figure><h5 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h5><p>nn.Sequential是一个有序的模块容器。数据按照定义好的方式顺序的通过当前模块。<br>您可以使用顺序容器来组合一个“快捷网络” ，例如：<code>seq_modules</code> .</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">logits = seq_modules(input_image)</span><br></pre></td></tr></table></figure><h5 id="nn-Softmax"><a href="#nn-Softmax" class="headerlink" title="nn.Softmax"></a>nn.Softmax</h5><p>Softmax激活函数通常用在最后一个线性层，用来返回对数区间介于 [-infty, infty] 中的原始值，这些值最终被传递给 <code>nn.Softmax</code> 模块。</p><p>Softmax 激活函数将对应输出区间范围缩放在 [0, 1] 之间，<strong>表示模型对每个类别的预测概率</strong>。其中，<code>dim</code> 中所有参数指示值求和应该为 1 。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">softmax = nn.Softmax(dim = <span class="number">1</span>)</span><br><span class="line">pred_probab = softmax(logits)</span><br></pre></td></tr></table></figure><h5 id="模型的参数"><a href="#模型的参数" class="headerlink" title="模型的参数"></a>模型的参数</h5><p>神经网络往往非常的复杂，在整个网络的构建过程中，如果可以便捷的将每个部分表示出来，对于训练过程中的优化和修改相对的权重与偏差等都会有非常大的帮助。</p><p>子类化 <code>nn.Module</code> 模块可以帮助我们解决这个问题，该模块会自动跟踪模型对象中定义的所有字段，并使用模型的 <code>parameters()</code> 函数或 <code>named_parameters()</code> 函数方法访问所有参数。</p><p>在本次示例中，我们遍历每个参数，并预览它们的所有数值参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model structure: <span class="subst">&#123;model&#125;</span>\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span> | Size: <span class="subst">&#123;param.size()&#125;</span> | Values : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>打印输出结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Model structure: NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],</span><br><span class="line">        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0155, -0.0327], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0116,  0.0293, -0.0280,  ...,  0.0334, -0.0078,  0.0298],</span><br><span class="line">        [ 0.0095,  0.0038,  0.0009,  ..., -0.0365, -0.0011, -0.0221]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0148, -0.0256], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0147, -0.0229,  0.0180,  ..., -0.0013,  0.0177,  0.0070],</span><br><span class="line">        [-0.0202, -0.0417, -0.0279,  ..., -0.0441,  0.0185, -0.0268]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0070, -0.0411], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure><h2 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量 Tensor"></a>张量 Tensor</h2><p>通过一张图初步了解常见的多维空间数据的命名方式（来源：<a href="https://youtu.be/ORMx45xqWkA?si=Njw0z1RULmPeeit9&t=17">PyTorch in 100 Seconds</a>）<br><a href="https://imgse.com/i/pilKoHf"><img src="https://z1.ax1x.com/2023/11/06/pilKoHf.png" alt="不同维度参数的命名方式"></a></p><p>Tensor 又称 张量，是一种专门的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量对比模型的输入和输出以及模型的参数进行编码。</p><p>Tensors 类似于 NumPy 中的ndarrays，不同之处在于张量可以在 GPU 或其他硬件加速器上运行。事实上，张量和 NumPy 数组通常可以共享相同的底层内存，从而消除了复制数据的需要。</p><p>张量也针对自动微分进行了优化。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h3 id="初始化张量"><a href="#初始化张量" class="headerlink" title="初始化张量"></a>初始化张量</h3><p>一般可以通过如下方式初始化 <code>Tensor</code>:</p><ul><li>直接通过数据创建</li><li>通过NumPy创建</li><li>通过继承另一个Tensor的形状和数据类型</li><li>使用随机值或常量</li></ul><p>下面分别进行介绍：</p><h4 id="直接来自数据"><a href="#直接来自数据" class="headerlink" title="直接来自数据"></a>直接来自数据</h4><p>张量可以直接从已有的数据中创建，数据类型是自动推断的。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure><h4 id="来自另一个-Tensor"><a href="#来自另一个-Tensor" class="headerlink" title="来自另一个 Tensor"></a>来自另一个 Tensor</h4><p>新建的张量保留参考张量的部分参数（<strong>形状，数据类型</strong>），除非用显式的方式直接覆盖。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># retains the properties of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line">x_rand = torch.rand_like(x_data, dtype = torch.<span class="built_in">float</span>) <span class="comment"># overrides the datatype of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[1, 1],</span><br><span class="line">        [1, 1]])</span><br><span class="line"></span><br><span class="line">Random Tensor:</span><br><span class="line"> tensor([[0.8823, 0.9150],</span><br><span class="line">        [0.3829, 0.9593]])</span><br></pre></td></tr></table></figure><h4 id="使用随机值或常量"><a href="#使用随机值或常量" class="headerlink" title="使用随机值或常量"></a>使用随机值或常量</h4><p><code>shape</code> 是张量维度的元组表达式。在下面的函数中，它决定了输出张量的维度：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 Tensor 的维度</span></span><br><span class="line">shape = (<span class="number">2</span>, <span class="number">3</span>, )</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones_Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Zeros_Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>打印输出结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Random Tensor:</span><br><span class="line"> tensor([[0.3904, 0.6009, 0.2566],</span><br><span class="line">        [0.7936, 0.9408, 0.1332]])</span><br><span class="line"></span><br><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]])</span><br><span class="line"></span><br><span class="line">Zeros Tensor:</span><br><span class="line"> tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure><h3 id="张量的属性"><a href="#张量的属性" class="headerlink" title="张量的属性"></a>张量的属性</h3><p>Tensor 的属性描述了<u><strong>它们的形状</strong></u>、<u><strong>数据类型</strong></u> 和 <u><strong>存储它们的设备</strong></u>。</p><ul><li><code>tensor.shape</code></li><li><code>tensor.dtype</code></li><li><code>tensor.device</code></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Dtype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device of tensor: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Shape of tensor: torch.Size([3, 4])</span><br><span class="line">Datatype of tensor: torch.float32</span><br><span class="line">Device tensor is stored on: cpu</span><br></pre></td></tr></table></figure><h3 id="张量的操作"><a href="#张量的操作" class="headerlink" title="张量的操作"></a>张量的操作</h3><p>科学计算是深度学习领域的根本！PyTorch提供了 100+ 张量运算操作，包括算术运算、线性代数运算、矩阵运算（转职、索引、切片）、采样等。</p><p><u><strong><code>PyTorch</code> 中的所有逻辑运算都可以通过GPU进行加速运算</strong></u></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将运行设备选择为 GPU （如果你有的话）</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tensor = tensor.to(<span class="string">&quot;cuda&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="类似-NumPy-的索引和切片操作"><a href="#类似-NumPy-的索引和切片操作" class="headerlink" title="类似 NumPy 的索引和切片操作"></a>类似 NumPy 的索引和切片操作</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Tensor= torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First row: <span class="subst">&#123;tensor[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First column: <span class="subst">&#123;tensor[:, <span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;last column: <span class="subst">&#123;tensor[..., -<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line">tensor[:, <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">First row: tensor([1., 1., 1., 1.])</span><br><span class="line">First column: tensor([1., 1., 1., 1])</span><br><span class="line">Last column: tensor([1., 1., 1., 1])</span><br><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure><h4 id="Tensor-之间的连接"><a href="#Tensor-之间的连接" class="headerlink" title="Tensor 之间的连接"></a>Tensor 之间的连接</h4><p><code>torch.cat</code> 可以用于连接指定维度的张量，拥有同样功能的另一个算子是 <code>torch.stack_</code> （参考：<a href="https://pytorch.org/docs/stable/generated/torch.stack.html">torch.stack_</a>）。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim = <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure><h4 id="张量的算术运算"><a href="#张量的算术运算" class="headerlink" title="张量的算术运算"></a>张量的算术运算</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算两个张量之间的矩阵乘法。其中，y1, y2, y3 拥有相同的参数值</span></span><br><span class="line"><span class="comment"># `tensor.T` 返回张量的转置</span></span><br><span class="line">y1 = tensor @ tensor.T</span><br><span class="line">y2 = tensor.matmul(tensor.T)</span><br><span class="line"></span><br><span class="line">y3 = torch.rand_like(y1)</span><br><span class="line">torch.matmul(tensor, tensor.T, out = y3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将张量中的元素逐个相乘。z1, z2, z3 具有相同的值</span></span><br><span class="line">z1 = tensor * tensor</span><br><span class="line">z2 = tensor.mul(tensor)</span><br><span class="line"></span><br><span class="line">z3 = torch.rand_like(tensor)</span><br><span class="line">torch.mul(tensor, tensor, out = z3)</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1],</span><br><span class="line">        [1., 0., 1., 1],</span><br><span class="line">        [1., 0., 1., 1],</span><br><span class="line">        [1., 0., 1., 1]])</span><br></pre></td></tr></table></figure><h4 id="单一元素张量"><a href="#单一元素张量" class="headerlink" title="单一元素张量"></a>单一元素张量</h4><p>如果你有一个单一元素的张量，例如希望通过将张量的所有值聚合为一个值，那么可以使用 <code>item()</code> 将其转换为 python 数值：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">agg = tensor.<span class="built_in">sum</span>()</span><br><span class="line">agg_item = agg.item()</span><br><span class="line"><span class="built_in">print</span>(agg_item, <span class="built_in">type</span>(agg_item))</span><br></pre></td></tr></table></figure><p>打印输出结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12.0 &lt;class &#x27;float&#x27;&gt;</span><br></pre></td></tr></table></figure><h4 id="原地操作-不通过返回的方式进行原地修改"><a href="#原地操作-不通过返回的方式进行原地修改" class="headerlink" title="原地操作(不通过返回的方式进行原地修改)"></a>原地操作(不通过返回的方式进行原地修改)</h4><p>将结果存储在操作数中的操作成为原地操作。它们由 <code>_</code> 后缀表示。例如：<code>x.copy()</code> 、 <code>x.t_()</code> ，都将直接修改 <code>x</code>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;tensor&#125;</span> \n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br><span class="line"></span><br><span class="line">tensor([[6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.]])</span><br></pre></td></tr></table></figure><h3 id="Tensor-和-Numpy-二者转换"><a href="#Tensor-和-Numpy-二者转换" class="headerlink" title="Tensor 和 Numpy 二者转换"></a>Tensor 和 Numpy 二者转换</h3><p>位于 CPU 位置上的 Numpy 数组与 Tensor 可以共享同一个底层的内容空间，更改一个 tensor 会同时修改另一个 tensor 。</p><h4 id="Tensor2NumPy-array"><a href="#Tensor2NumPy-array" class="headerlink" title="Tensor2NumPy_array"></a>Tensor2NumPy_array</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>打印对比结果（t 代表 tensor；n 代表 numpy）:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([1., 1., 1., 1., 1.])</span><br><span class="line">n: [1. 1. 1. 1. 1.]</span><br></pre></td></tr></table></figure><p>下一步，改变 tensor 中的值，同时观察 NumPy 中值的变化：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: (n)&quot;</span>)</span><br></pre></td></tr></table></figure><p>打印对比结果（t 代表 tensor；n 代表 numpy）:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([2., 2., 2., 2., 2.])</span><br><span class="line">n: [2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure><h4 id="NumPy-array2Tensor"><a href="#NumPy-array2Tensor" class="headerlink" title="NumPy_array2Tensor"></a>NumPy_array2Tensor</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n = np.ones(<span class="number">5</span>)</span><br><span class="line">t = torch.from_numpy(n)</span><br></pre></td></tr></table></figure><p>NumPy 数组中的更改回反映在张量（tensor）中</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.add(n, <span class="number">1</span>, out = n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br><span class="line">n: [2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure><h2 id="数据集的相关操作"><a href="#数据集的相关操作" class="headerlink" title="数据集的相关操作"></a>数据集的相关操作</h2><p>从代码的架构设计上考虑，无论是出于可读性考虑还是出于代码逻辑的模块化管理考虑，我们都希望数据集代码与模型训练代码分离。</p><p>在数据预加载上，PyTorch 提供了两个功能函数 <code>torch.utils.data.DataLoader</code> 和 <code>torch.utils.data.Dataset</code> 分别读取预加载的数据和自己的数据。</p><p><code>Dataset</code> 存储样本和对应的标签，并在 <code>DataLoader</code> 范围内包装成一个可迭代对象 <code>Dataset</code> 以便轻松访问样本。</p><blockquote><p>在 PyTorch 函数库中预先提供好了很多可供预加载的数据集（例如：FashionMNIST），这些数据集借助 <code>torch.utils.data.Dataset</code> 子类化，并实现位于特定数据的函数。它们可用于对模型进行原型设计和基准测试。可以通过如下链接访问：<a href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/vision/stable/datasets.html">Image Datasets</a>，<a href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/text/stable/datasets.html">Text Datasets</a>，<a href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/audio/stable/datasets.html">Audio Datasets</a></p></blockquote><h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><p>下面例子是从 <code>TorchVision</code> 加载 <a href="https://colab.research.google.com/corgiredirector?site=https://research.zalando.com/project/fashion_mnist/fashion_mnist/">Fashion-MNIST</a> 数据集的示例。Fashion-MNIST 由 60000 个训练样本和 10000 个测试样本组成。每个示例都包含一个 28*28 灰度图像和一个来自 10 个类之一的关联标签。</p><p>我们通过如下几个参数，对 <a href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/vision/stable/datasets.html%23fashion-mnist">FashionMNIST Dataset</a> 数据集进行加载：</p><ul><li><code>root</code> 是存储训练&#x2F;测试数据的根目录</li><li><code>train</code> 指定训练或测试数据集</li><li><code>download = TRUE</code> 允许从互联网上搜索并下载数据集，前提是 root 路径下的数据集文件不存在</li><li><code>transform</code> 和 <code>target_transform</code> 分别执行 <em>标定特征</em> 和 <em>标注转换</em></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">True</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">False</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = ToTensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="数据集迭代和数据可视化"><a href="#数据集迭代和数据可视化" class="headerlink" title="数据集迭代和数据可视化"></a>数据集迭代和数据可视化</h3><p>使用 <code>Datasets</code> ，我们可以实现像 Python 中列表那样的手动索引 <code>training_data[index]</code>。</p><p>使用 <code>matplotlib</code> 可视化训练数据集中的样本进行展示。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">labels_map = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;T-Shirt&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;Trouser&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;Pullover&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;Dress&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;Coat&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;Sandal&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;Shirt&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;Sneaker&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;Bag&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;Ankle Boot&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">figure = plt.figure(figsize = (<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">cols, rows = <span class="number">4</span>, <span class="number">4</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, cols * rows + <span class="number">1</span>):</span><br><span class="line">    sample_idx = torch.randint(<span class="built_in">len</span>(training_data), size(<span class="number">1</span>,)).item()</span><br><span class="line">    image, lable = training_data[sample_idx]</span><br><span class="line">    figure.add_subplot(rows, cols, i)</span><br><span class="line">    plt.title(lables_map[label])</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(img.squeeze(), cmap = <span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()  <span class="comment"># 打印出来的输出为数据集图像示例展示</span></span><br></pre></td></tr></table></figure><p>打印输出：</p><p><a href="https://imgse.com/i/pilfxhV"><img src="https://z1.ax1x.com/2023/11/07/pilfxhV.png" alt="pilfxhV.png"></a></p><h3 id="创建自定义的数据集"><a href="#创建自定义的数据集" class="headerlink" title="创建自定义的数据集"></a>创建自定义的数据集</h3><p>自定义数据集必须含有三个函数：</p><ul><li><code>__init__</code></li><li><code>__len__</code></li><li><code>__gititem__</code></li></ul><p>通过下面这个示例实现了解相关函数的使用方法。</p><p>FashionMNIST 图像存储在目录 <code>img_dir</code>，其自身的标签单独存储在CSV文件 <code>annotations_file</code> 中</p><p>先看代码块部分：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torchvision.io <span class="keyword">as</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomImageDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, annotations_file, img_dir, transform = <span class="literal">None</span>, target_transform = <span class="literal">None</span></span>):</span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self</span>):</span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><h4 id="init"><a href="#init" class="headerlink" title="_init_"></a>_<em>init</em>_</h4><p>init 作为初始化函数，在实例化 Dataset 对象时运行一次。</p><p>我们初始化包含<strong>图像</strong>、<strong>注释文件</strong>和<strong>两个转换的目录</strong></p><p>labels.csv 文件的展示效果如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tshirt1.jpg, 0</span><br><span class="line">tshirt2.jpg, 0</span><br><span class="line">......</span><br><span class="line">ankleboot999.jpg, 9</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, annotations_file, img_dir, transform = <span class="literal">None</span>, target_transform = <span class="literal">None</span></span>):</span><br><span class="line">    self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">    self.img_dir = img_dir</span><br><span class="line">    self.transform = transform</span><br><span class="line">    self.target_transform = target_transform</span><br></pre></td></tr></table></figure><h4 id="len"><a href="#len" class="headerlink" title="_len_"></a>_<em>len</em>_</h4><p>len 函数返回数据集中的样本数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br></pre></td></tr></table></figure><h4 id="getitem"><a href="#getitem" class="headerlink" title="_getitem_"></a>_<em>getitem</em>_</h4><p>getitem 函数从给定索引目录的 <code>idx</code> 处返回数据集中的样本。</p><p>根据索引，它识别图像在磁盘上的位置，使用 <code>read_image</code>，从 csv 数据中检索相应的标签<code>self.img_labels</code>，调用它们的转换函数（前提是支持转换），并在元组中返回张量图像和相应的标签。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self</span>):</span><br><span class="line">    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">    image = read_image(img_path)</span><br><span class="line">    label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> self.transform:</span><br><span class="line">        image = self.transform(image)</span><br><span class="line">    <span class="keyword">if</span> self.target_transform:</span><br><span class="line">        label = self.target_transform(label)</span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><h3 id="准备数据并使用-DataLoader-进行训练"><a href="#准备数据并使用-DataLoader-进行训练" class="headerlink" title="准备数据并使用 DataLoader 进行训练"></a>准备数据并使用 DataLoader 进行训练</h3><p>检索 Dataset 数据集的特征，并一次标记一个样本。</p><p>在训练模型的过程中，我们通常希望通过“小批量”的方式传递样本，在新一轮 epoch 下数据 reshuffle(洗牌) 减少模型过拟合，并使用 Python 中的 <code>multiprocessing</code> 函数来加快数据检索的速度。</p><p><code>DataLoader</code> 是一个可迭代的对象，它通过一个简单的 API 为我们抽象了这种复杂性。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size = <span class="number">64</span>, shuffle = <span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size = <span class="number">64</span>, shuffle = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="遍历-DataLoader"><a href="#遍历-DataLoader" class="headerlink" title="遍历 DataLoader"></a>遍历 DataLoader</h3><p>通过上面的步骤，我们已经将数据集加载到了 <code>DataLoader</code> 并可以根据我们的需要来遍历该数据集。</p><p>后续程序中每一次迭代都会返回一批 <code>train_features</code> 和 <code>train_labels</code>，每批结果中都包含 <code>batch_size = 64</code> 特征和标签。</p><p>在上面代码块中，我们指定了 shuffle &#x3D; True ，在我们遍历了所有批次后，数据会被洗牌（目的是为了更细粒度地控制数据加载顺序，可参考<a href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/docs/stable/data.html%23data-loading-order-and-sampler">Samplers</a>）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Display image and label.</span></span><br><span class="line">train_features, train_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Feature batch shape: <span class="subst">&#123;train_features.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Labels batch shape: <span class="subst">&#123;train_labels.size()&#125;</span>&quot;</span>)</span><br><span class="line">img = train_features[<span class="number">0</span>].squeeze()</span><br><span class="line">label = train_labels[<span class="number">0</span>]</span><br><span class="line">plt.imshow(img, cmap = <span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()  <span class="comment"># 输出数据集中的图片</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h2><p>数据的格式不总是按照训练机器学习算法所需要的格式出现的，因此我们需要通过<u><em><strong>转换</strong></em></u>来对数据进行一系列操作使得其适合于机器学习任务。</p><p>所有 <code>TorchVision</code> 数据集都具有两个参数：</p><ul><li><code>transform</code> 用于修改标签</li><li><code>target_transform</code> 接受包含转换逻辑的可调用对象</li></ul><p>在 <a href="https://pytorch.org/vision/stable/transforms.html">torchvision.transforms</a> 中提供了几个开箱即用的转换格式。</p><p>FashionMNIST 特征采用 PIL Image 格式，标签为整数。</p><p>在训练任务开始前，我们需要将特征处理为归一化之后的张量。</p><p>为了进行这些转换，需要使用 <code>ToTensor</code> 和 <code>Lambda</code> 函数方法。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">True</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = ToTensor(),</span><br><span class="line">    target_transform = Lambda(<span class="keyword">lambda</span> y: torch.zero(<span class="number">10</span>, dtype = torch.<span class="built_in">float</span>).scatter_(<span class="number">0</span>, torch.tensor(y), value = <span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz</span><br><span class="line"></span><br><span class="line">  0%|          | 0/26421880 [00:00&lt;?, ?it/s]</span><br><span class="line">  0%|          | 65536/26421880 [00:00&lt;01:12, 362364.70it/s]</span><br><span class="line">  1%|          | 229376/26421880 [00:00&lt;00:38, 680532.51it/s]</span><br><span class="line">  3%|2         | 786432/26421880 [00:00&lt;00:11, 2194389.90it/s]</span><br><span class="line">  7%|7         | 1933312/26421880 [00:00&lt;00:05, 4185622.75it/s]</span><br><span class="line"> 17%|#6        | 4423680/26421880 [00:00&lt;00:02, 9599067.02it/s]</span><br><span class="line"> 25%|##5       | 6717440/26421880 [00:00&lt;00:01, 11175748.57it/s]</span><br><span class="line"> 34%|###4      | 9109504/26421880 [00:01&lt;00:01, 14174360.51it/s]</span><br><span class="line"> 44%|####3     | 11567104/26421880 [00:01&lt;00:01, 14358310.56it/s]</span><br><span class="line"> 53%|#####2    | 13959168/26421880 [00:01&lt;00:00, 16463421.66it/s]</span><br><span class="line"> 62%|######2   | 16449536/26421880 [00:01&lt;00:00, 15864345.49it/s]</span><br><span class="line"> 71%|#######1  | 18776064/26421880 [00:01&lt;00:00, 17449238.29it/s]</span><br><span class="line"> 81%|########  | 21397504/26421880 [00:01&lt;00:00, 16758523.84it/s]</span><br><span class="line"> 90%|########9 | 23691264/26421880 [00:01&lt;00:00, 18055860.39it/s]</span><br><span class="line">100%|##########| 26421880/26421880 [00:01&lt;00:00, 13728491.83it/s]</span><br><span class="line">Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw</span><br><span class="line"></span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz</span><br><span class="line"></span><br><span class="line">  0%|          | 0/29515 [00:00&lt;?, ?it/s]</span><br><span class="line">100%|##########| 29515/29515 [00:00&lt;00:00, 327895.25it/s]</span><br><span class="line">Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw</span><br><span class="line"></span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz</span><br><span class="line"></span><br><span class="line">  0%|          | 0/4422102 [00:00&lt;?, ?it/s]</span><br><span class="line">  1%|1         | 65536/4422102 [00:00&lt;00:11, 363267.41it/s]</span><br><span class="line">  5%|5         | 229376/4422102 [00:00&lt;00:06, 683985.17it/s]</span><br><span class="line"> 19%|#8        | 819200/4422102 [00:00&lt;00:01, 2304448.10it/s]</span><br><span class="line"> 33%|###3      | 1474560/4422102 [00:00&lt;00:00, 2999709.36it/s]</span><br><span class="line"> 83%|########2 | 3670016/4422102 [00:00&lt;00:00, 7976134.77it/s]</span><br><span class="line">100%|##########| 4422102/4422102 [00:00&lt;00:00, 5985529.02it/s]</span><br><span class="line">Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw</span><br><span class="line"></span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz</span><br><span class="line"></span><br><span class="line">  0%|          | 0/5148 [00:00&lt;?, ?it/s]</span><br><span class="line">100%|##########| 5148/5148 [00:00&lt;00:00, 39473998.16it/s]</span><br><span class="line">Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw</span><br></pre></td></tr></table></figure><h3 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor()"></a>ToTensor()</h3><p><a href="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor">ToTensor</a> 将 PIL 图像 或 NumPy <code>ndarray</code> 转换为 <code>FloatTensor</code>. 并在[0., 1.] 范围内缩放图像的像素空间。</p><h3 id="Lambda-转换"><a href="#Lambda-转换" class="headerlink" title="Lambda 转换"></a>Lambda 转换</h3><p>Lambda 函数允许任意用户定义 lambda 函数。在这里，我们定义了一个函数，将整数转换为 one-hot 编码的张量。</p><p>Lambda首先创建一个大小为10（我们数据集中的标签数量）的零向量，并调用 <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html">scatter_</a> 函数在索引 <code>y</code> 上分配标签 <code>value = 1</code>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">target_transform = Lambda(<span class="keyword">lambda</span> y:torch.zeros(</span><br><span class="line">    <span class="number">10</span>, dtype = torch.<span class="built_in">float</span>).scatter_(dim = <span class="number">0</span>, index = torch.tensor(y), value = <span class="number">1</span>))</span><br></pre></td></tr></table></figure><blockquote><p>阅读延伸：<a href="https://pytorch.org/vision/stable/transforms.html">torchvision.transforms API</a></p></blockquote><h2 id="（核心内容）自动微分-TORCH-AUTOGRAD"><a href="#（核心内容）自动微分-TORCH-AUTOGRAD" class="headerlink" title="（核心内容）自动微分(TORCH.AUTOGRAD)"></a>（核心内容）自动微分(<code>TORCH.AUTOGRAD</code>)</h2><p>在训练神经网络时，常用的算法是<strong>反向传播</strong>。在该算法中，参数（模型权重）根据损失函数相对于给定参数的<strong>梯度</strong>进行调整。</p><p>为了计算这些梯度，PyTorch 有一个内置的用于自动计算微分方程的引擎，称为 <code>torch.autograd</code> .</p><p>它支持任何计算图的梯度自动运算。</p><p>下面距离一个最简单的单层神经网络，其中包含输入 <code>x</code>、参数 <code>w</code> 和 <code>b</code>，以及一些损失函数。可以在 PyTorch 中按以下方式定义它：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">5</span>) <span class="comment"># input tensor</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>) <span class="comment"># expected output</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">z = torch.matmul(x, w) + b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) </span><br></pre></td></tr></table></figure><h3 id="张量、函数、计算图"><a href="#张量、函数、计算图" class="headerlink" title="张量、函数、计算图"></a>张量、函数、计算图</h3><p>上一小节的代码框中实现的损失函数计算流程如下图所示：</p><p><img src="https://pytorch.org/tutorials/_images/comp-graph.png" alt="计算损失函数"></p><p>在上面网络中，<code>w</code> 和 <code>b</code> 是我们需要优化的参数。因此，我们需要能够计算这些变量损失函数的梯度。</p><p>为了做到这些点，我们设置了这些张量的 <code>requires_grad</code> 函数来定义其属性。</p><blockquote><p>设置张量的值有两种方式：一种是在生成张量的时候使用 <code>requires_grad</code> 进行初始化设置；另一种是在后续使用 <code>x.requires_grad_(True)</code> 函数。</p></blockquote><p>用来构造计算图的函数实际上是类 <code>Function</code> 的对象。该对象指导如何计算正向函数，以及如何在反向传播步骤中计算其导数。对向后传播函数的引起存储在张量的属性中的 <code>grad_fn</code>。您可以在PyTorch的<a href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/docs/stable/autograd.html%23function">官方文档</a>中找到更多信息 <code>Function</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient function for z = <span class="subst">&#123;z.grad_fn&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient function for loss = <span class="subst">&#123;loss.grad_fn&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Gradient function for z = &lt;AddBackward0 object at 0x7d800ac85840&gt;</span><br><span class="line">Gradient function for loss = &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x7d800ac85ea0&gt;</span><br></pre></td></tr></table></figure><h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><p>为了优化神经网络中的参数权重，我们需要计算损失函数相对于参数的导数，即我们需要在固定 <code>x</code> 和 <code>y</code> 值的情况下，求出 $\frac{\partial loss}{\partial w}$ 和 $\frac{\partial loss}{\partial b}$。</p><p>为了求出上面的导数，我们需要调用函数 <code>loss.backward()</code>，然后从 <code>w.grad</code> 和 <code>b.grad</code> 中检索权重和偏置的数值。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure><p>打印输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.3287, 0.0101, 0.0988],</span><br><span class="line">        [0.3287, 0.0101, 0.0988],</span><br><span class="line">        [0.3287, 0.0101, 0.0988],</span><br><span class="line">        [0.3287, 0.0101, 0.0988],</span><br><span class="line">        [0.3287, 0.0101, 0.0988]])</span><br><span class="line">tensor([0.3287, 0.0101, 0.0988])</span><br></pre></td></tr></table></figure><blockquote><ul><li><p>可以通过将参数 <code>requires_grad</code> 的 <code>grad</code> 属性设置为 <code>True</code> 来或测计算图的叶节点属性。对于计算图中的所有其他节点，梯度将不可用。</p></li><li><p>PyTorch考虑性能上的原因，在给定图形上只能调用 <code>backward</code> 调用一次梯度计算。如果我们需要再同一个图上执行多个 <code>backward</code> 调用，我们需要参数传递给 <code>retain_graph = True</code>，再调用 <code>backward</code> 。</p></li></ul></blockquote><h3 id="禁用梯度追踪"><a href="#禁用梯度追踪" class="headerlink" title="禁用梯度追踪"></a>禁用梯度追踪</h3><p>默认情况下，所有张量都在 <code>require_grad = True</code> 跟踪其计算历史并支持梯度计算。但是，在某些情况下，只想将模型应用于默写输入数据时，即我们只想通过网络进行前向计算。我们可以通过将计算代码加上 <code>torch.no_grad()</code> 还书，用来停止跟踪计算。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w) +b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad) <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w) + b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad) <span class="comment"># False</span></span><br></pre></td></tr></table></figure><p>另一种方法是在张量上使用 <code>detach()</code> 方法：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w) + b</span><br><span class="line">z_det = z.detach()</span><br><span class="line"><span class="built_in">print</span>(z_det.requires_grad) <span class="comment"># False</span></span><br></pre></td></tr></table></figure><p>考虑要禁用梯度跟踪的情况：</p><ul><li>将神经网络中的某些参数标记为冻结参数</li><li>为了优化计算速度，在只进行前向传播时禁用梯度跟踪的选项</li></ul><h3 id="计算图部分扩展阅读"><a href="#计算图部分扩展阅读" class="headerlink" title="计算图部分扩展阅读"></a>计算图部分扩展阅读</h3><p>（留白，前面的区域以后再探索吧~）</p><h3 id="可选阅读：张量梯度-和-雅阁比积-Jacobian-Products"><a href="#可选阅读：张量梯度-和-雅阁比积-Jacobian-Products" class="headerlink" title="可选阅读：张量梯度 和 雅阁比积(Jacobian_Products)"></a>可选阅读：张量梯度 和 雅阁比积(Jacobian_Products)</h3><p>（留白，前面的区域以后再探索吧~）</p><h2 id="（重要）优化模型参数"><a href="#（重要）优化模型参数" class="headerlink" title="（重要）优化模型参数"></a>（重要）优化模型参数</h2><p>在拥有了模型和数据之后，就可以通过优化数据参数来训练、验证和测试我们的模型了。训练模型是一个带带过程，每次迭代中，模型都会对输出进行*<u>猜测</u>*，计算其猜测中的误差，计算其猜测中的误差（损失），收集误差对参数的导数（上一节中进行的工作），并使用梯度下降这些参数。</p><p>更详细的视频讲解，可以参考 <a href="https://www.youtube.com/watch?v=tIeHLnjs5U8">backpropagation from 3Blue1Brown</a></p><h3 id="先决条件代码"><a href="#先决条件代码" class="headerlink" title="先决条件代码"></a>先决条件代码</h3><h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><h3 id="优化循环"><a href="#优化循环" class="headerlink" title="优化循环"></a>优化循环</h3><h3 id="全过程数据跟踪"><a href="#全过程数据跟踪" class="headerlink" title="全过程数据跟踪"></a>全过程数据跟踪</h3><h2 id="保存、加载和使用模型"><a href="#保存、加载和使用模型" class="headerlink" title="保存、加载和使用模型"></a>保存、加载和使用模型</h2><h3 id="保存-x2F-加载模型权重"><a href="#保存-x2F-加载模型权重" class="headerlink" title="保存&#x2F;加载模型权重"></a>保存&#x2F;加载模型权重</h3><h3 id="通过模型的形状参数进行保存-x2F-加载"><a href="#通过模型的形状参数进行保存-x2F-加载" class="headerlink" title="通过模型的形状参数进行保存&#x2F;加载"></a>通过模型的形状参数进行保存&#x2F;加载</h3><h2 id="在-PyTorch-中保存和加载常规-Checkpoint"><a href="#在-PyTorch-中保存和加载常规-Checkpoint" class="headerlink" title="在 PyTorch 中保存和加载常规 Checkpoint"></a>在 PyTorch 中保存和加载常规 Checkpoint</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><h3 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h3><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><h4 id="1-导入加载数据所需要的库"><a href="#1-导入加载数据所需要的库" class="headerlink" title="1.导入加载数据所需要的库"></a>1.导入加载数据所需要的库</h4><h4 id="2-定义和初始化神经网络"><a href="#2-定义和初始化神经网络" class="headerlink" title="2.定义和初始化神经网络"></a>2.定义和初始化神经网络</h4><h4 id="3-初始化优化器"><a href="#3-初始化优化器" class="headerlink" title="3.初始化优化器"></a>3.初始化优化器</h4><h4 id="4-保存常规检查点"><a href="#4-保存常规检查点" class="headerlink" title="4.保存常规检查点"></a>4.保存常规检查点</h4><h4 id="5-加载常规检查点"><a href="#5-加载常规检查点" class="headerlink" title="5.加载常规检查点"></a>5.加载常规检查点</h4><h2 id="从-Checkpoint-中加载-nn-Module-的技巧"><a href="#从-Checkpoint-中加载-nn-Module-的技巧" class="headerlink" title="从 Checkpoint 中加载 nn.Module 的技巧"></a>从 Checkpoint 中加载 <code>nn.Module</code> 的技巧</h2><h3 id="活用-torch-load-mmap-True"><a href="#活用-torch-load-mmap-True" class="headerlink" title="活用 torch.load(mmap = True)"></a>活用 <code>torch.load(mmap = True)</code></h3><h3 id="活用-torch-device-quot-meta-quot"><a href="#活用-torch-device-quot-meta-quot" class="headerlink" title="活用 torch.device(&quot;meta&quot;)"></a>活用 <code>torch.device(&quot;meta&quot;)</code></h3><h3 id="活用-load-state-dict-assign-True"><a href="#活用-load-state-dict-assign-True" class="headerlink" title="活用 load_state_dict(assign = True)"></a>活用 <code>load_state_dict(assign = True)</code></h3><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#pytorch-%E5%9F%BA%E7%A1%80&quot;&gt;Pytorch 基础&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8&quot;&gt;快速入门&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a hr</summary>
      
    
    
    
    <category term="Deeplearning" scheme="https://zade23.github.io/categories/Deeplearning/"/>
    
    
    <category term="PyTorch" scheme="https://zade23.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>工具网站</title>
    <link href="https://zade23.github.io/2023/10/18/%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/"/>
    <id>https://zade23.github.io/2023/10/18/%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/</id>
    <published>2023-10-18T09:04:52.000Z</published>
    <updated>2024-03-21T06:41:41.700Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E5%86%85%E5%AE%B9%E5%88%86%E7%B1%BB">内容分类</a><ul><li><a href="#%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E7%BD%91%E7%AB%99">学习资源网站</a><ul><li><a href="#aigc">aigc</a></li><li><a href="#llm">llm</a></li><li><a href="#tts">tts</a></li><li><a href="#rl">rl</a></li><li><a href="#diffusion">diffusion</a></li><li><a href="#huggingface">huggingface</a></li><li><a href="#awesome">awesome</a></li><li><a href="#%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80">数学基础</a></li><li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a></li><li><a href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">深度学习&#x2F;模型训练</a></li><li><a href="#%E6%8A%BD%E8%B1%A1%E6%A6%82%E5%BF%B5%E5%8F%AF%E8%A7%86%E5%8C%96">抽象概念可视化</a></li><li><a href="#%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95">传统算法</a></li><li><a href="#%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80">汇编语言</a></li><li><a href="#%E7%94%B5%E5%AD%90%E4%B9%A6">电子书</a></li></ul></li><li><a href="#%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99">工具网站</a><ul><li><a href="#%E5%9C%A8%E7%BA%BF%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">在线大语言模型</a></li><li><a href="#%E5%9C%A8%E7%BA%BFapi">在线api</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86">数据集</a></li><li><a href="#%E9%9F%B3%E9%A2%91%E5%A4%84%E7%90%86">音频处理</a></li><li><a href="#%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86">视频处理</a></li><li><a href="#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86">图像处理</a></li><li><a href="#%E8%AE%BE%E8%AE%A1">设计</a></li><li><a href="#%E4%B8%8B%E8%BD%BD%E5%B7%A5%E5%85%B7">下载工具</a></li><li><a href="#%E6%A0%BC%E5%BC%8F%E5%8E%8B%E7%BC%A9">格式压缩</a></li></ul></li></ul></li></ul><hr><h2 id="内容分类"><a href="#内容分类" class="headerlink" title="内容分类"></a>内容分类</h2><h3 id="学习资源网站"><a href="#学习资源网站" class="headerlink" title="学习资源网站"></a>学习资源网站</h3><h4 id="aigc"><a href="#aigc" class="headerlink" title="aigc"></a>aigc</h4><ul><li>【关于AIGC的各种精选教程和资源，既适合初学者也适合进阶AI爱好者】<a href="github.com/luban-agi/Awesome-AIGC-Tutorials/blob/main/README_zh.md">github.com&#x2F;luban-agi&#x2F;Awesome-AIGC-Tutorials&#x2F;blob&#x2F;main&#x2F;README_zh.md</a></li></ul><h4 id="llm"><a href="#llm" class="headerlink" title="llm"></a>llm</h4><ul><li><p>【收集各种垂直领域的大语言模型】<a href="https://github.com/luban-agi/Awesome-Domain-LLM">Awesome Domain LLM</a></p></li><li><p>【关于大型语言模型(LLM)的一切，包括了LLMs的入门、微调方法、多模态模型、稳定扩散、注意力机制优化和数据效率等方面的信息，从nanoGPT到LoRA、QLoRA、RLHF到CLIP等多模态模型】<a href="github.com/tianlinxu312/Everything-about-LLMs">Everything-about-LLMs</a></p></li><li><p>【提示工程实例：用实际项目学习提示工程技术，从大型语言模型获得更好的结果，内容涵盖零样本和少样本提示，分隔符，步骤编号，角色提示，思维链(CoT)提示等】 <a href="https://realpython.com/practical-prompt-engineering/">《Prompt Engineering: A Practical Example – Real Python》</a></p></li></ul><h4 id="tts"><a href="#tts" class="headerlink" title="tts"></a>tts</h4><ul><li>【如何在浏览器中处理音视频】<a href="https://hughfenghen.github.io/tag/WebAV/">Web 音视频系列</a></li><li>【基于华为诺亚方舟实验室Grad-TTS的Grad-SVC】<a href="https://github.com/PlayVoice/Grad-SVC">https://github.com/PlayVoice/Grad-SVC</a></li><li>【台大课程《深度学习音乐分析与生成》资料】 <a href="http://github.com/affige/DeepMIR">github.com&#x2F;affige&#x2F;DeepMIR</a></li><li>【大型音频模型相关文献资源列表】 <a href="http://github.com/EmulationAI/awesome-large-audio-models">github.com&#x2F;EmulationAI&#x2F;awesome-large-audio-models</a></li></ul><h4 id="rl"><a href="#rl" class="headerlink" title="rl"></a>rl</h4><ul><li>【Generative Agents with Llama2】<a href="https://github.com/rlancemartin/generative_agents">https://github.com/rlancemartin/generative_agents</a></li></ul><h4 id="diffusion"><a href="#diffusion" class="headerlink" title="diffusion"></a>diffusion</h4><ul><li>【扩散模型相关论文资源列表，涵盖了文本到视频生成、文本引导视频编辑、个性化视频生成、视频预测等方面】 <a href="http://github.com/ChenHsing/Awesome-Video-Diffusion-Models">github.com&#x2F;ChenHsing&#x2F;Awesome-Video-Diffusion-Models</a></li><li>【3D Diffusion相关文献列表】 <a href="http://github.com/cwchenwang/awesome-3d-diffusion">github.com&#x2F;cwchenwang&#x2F;awesome-3d-diffusion</a></li></ul><h4 id="huggingface"><a href="#huggingface" class="headerlink" title="huggingface"></a>huggingface</h4><ul><li>【Hugging Face - Learn】<a href="https://huggingface.co/learn">https://huggingface.co/learn</a></li></ul><h4 id="awesome"><a href="#awesome" class="headerlink" title="awesome"></a>awesome</h4><ul><li>【收集各种生成式 AI 的教程】<a href="https://github.com/luban-agi/Awesome-AIGC-Tutorials">Awesome AIGC Tutorials</a></li></ul><h4 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h4><ul><li>【给程序员的线性代数指南】<a href="https://coffeemug.github.io/spakhm.com/posts/01-lingalg-p1/linalg-p1.html">Linear Algebra for programmers</a></li></ul><h4 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h4><ul><li>【ML Papers Explained：机器学习论文解析】 <a href="http://github.com/dair-ai/ML-Papers-Explained">github.com&#x2F;dair-ai&#x2F;ML-Papers-Explained</a></li></ul><h4 id="深度学习-x2F-模型训练"><a href="#深度学习-x2F-模型训练" class="headerlink" title="深度学习&#x2F;模型训练"></a>深度学习&#x2F;模型训练</h4><ul><li>【Batched LoRAs：通过同一批次的多个 LoRA 路由推理，最大化 GPU 利用率】<a href="github.com/sabetAI/BLoRA">Batched LoRAs - batched</a></li></ul><h4 id="抽象概念可视化"><a href="#抽象概念可视化" class="headerlink" title="抽象概念可视化"></a>抽象概念可视化</h4><ul><li>【机器学习MachineLearning】<a href="http://www.r2d3.us/">www.r2d3.us</a></li><li>【波Waveforms】<a href="https://pudding.cool/2018/02/waveforms/">Let’s Learn About Waveforms</a></li></ul><h4 id="传统算法"><a href="#传统算法" class="headerlink" title="传统算法"></a>传统算法</h4><ul><li>【开源算法库Algorithms】<a href="https://the-algorithms.com/">The Algorithms</a></li></ul><h4 id="汇编语言"><a href="#汇编语言" class="headerlink" title="汇编语言"></a>汇编语言</h4><ul><li><h5 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h5><ul><li>【Python官方文档】<a href="https://docs.python.org/zh-cn/3/tutorial/index.html">Python语法官方文档</a></li><li>【从Python内置函数理解Python】<a href="https://tushar.lol/post/builtins/">Understanding all of Python, through its builtins</a></li><li>【GraphLearn-for-PyTorch(GLT)：PyTorch图学习库，使分布式 GNN 训练和推理变得简单高效】’GraphLearn-for-PyTorch(GLT) <a href="github.com/alibaba/graphlearn-for-pytorch">A graph learning library for PyTorch that makes distributed GNN training and inference easy and efficient</a></li></ul></li><li><h5 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h5><ul><li>【C++在线学习】<a href="https://www.learncpp.com/">Learn C++ – Skill up with our free tutorials</a></li></ul></li></ul><h4 id="电子书"><a href="#电子书" class="headerlink" title="电子书"></a>电子书</h4><ul><li>(图书馆)【zlibrary】<a href="https://zh.zlibrary-china.se/">Z-Library – the world’s largest e-book library. Your gateway to knowledge and culture.</a></li><li>【免费书《人工智能：计算Agent基础，(第三版)》】<a href="https://artint.info/3e/html/ArtInt3e.html">《Artificial Intelligence: Foundations of Computational Agents,  3rd Edition》David L. Poole and Alan K. Mackworth (2023)</a></li><li>【配有动画的数据结构与算法电子书】<a href="https://github.com/krahets/hello-algo">Hello 算法</a></li></ul><h3 id="工具网站"><a href="#工具网站" class="headerlink" title="工具网站"></a>工具网站</h3><h4 id="在线大语言模型"><a href="#在线大语言模型" class="headerlink" title="在线大语言模型"></a>在线大语言模型</h4><ul><li><a href="http://wenxin.baidu.com/">百度（文心一言）</a></li><li><a href="http://doubao.com/">抖音（云雀大模型）</a></li><li><a href="http://chatglm.cn/">智谱AI（GLM大模型）</a></li><li><a href="http://xihe.mindspore.cn/">中科院（紫东太初大模型）</a></li><li><a href="http://baichuan-ai.com/">百川智能（百川大模型）</a></li><li><a href="https://chat.openai.com/">ChatGPT</a></li><li><a href="https://poe.com/">Poe</a></li><li><a href="https://passport.xfyun.cn/">讯飞星火</a></li><li><a href="https://bard.google.com/">谷歌bard</a></li><li><a href="https://qianwen.aliyun.com/">阿里通义千问</a></li></ul><h4 id="在线api"><a href="#在线api" class="headerlink" title="在线api"></a>在线api</h4><ul><li>【收集各种 AI 工具和资源】<a href="https://www.aihub.cn/">AIHub</a></li><li>【免费AI API列表】<a href="http://github.com/NovaOSS/free-ai-apis">free-ai-apis</a></li><li>【作文批改：使用GPT4对雅思托福作文判分和批改】<a href="https://www.essay.art/">https://www.essay.art/</a></li></ul><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li>【OpenDataLab 为国产大模型提供高质量的开放数据集】<a href="https://opendatalab.com/home">OpenDataLab</a></li></ul><h4 id="音频处理"><a href="#音频处理" class="headerlink" title="音频处理"></a>音频处理</h4><ul><li>【Whisper:英语音频转成文本的在线工具】<a href="https://huggingface.co/spaces/Xenova/whisper-web">Whisper Web</a></li></ul><h4 id="视频处理"><a href="#视频处理" class="headerlink" title="视频处理"></a>视频处理</h4><ul><li>【Spikes Studio】<a href="https://spikes.studio/">https://spikes.studio/</a></li></ul><h4 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h4><ul><li>【在线体验-图像分割算法Meta-SegementAnything】<a href="https://segment-anything.com/demo">Segment Anything</a></li><li>【SDXL在线体验】<a href="https://www.stablediffusionai.ai/">StableDiffusion XL 体验站</a></li><li>【nutsh：旨在通过人工反馈进行视觉学习的平台，具有用户友好的界面和 API，支持一系列视觉模式、多样化的人工输入方法以及基于人工反馈的学习机制】<a href="https://nutsh.ai/docs/">Nutsh</a></li></ul><h4 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h4><ul><li>【中文的图标搜索引擎，作者利用 ChatGPT 翻译了 Iconify 的 18 万个图标名】<a href="https://yesicon.app/">yesicon</a></li><li>【JupyterCAD - 用于3D几何建模的JupyterLab扩展】<a href="https://github.com/QuantStack/jupytercad">https://github.com/QuantStack/jupytercad</a></li><li>【图标搜索引擎收入了10万+的图标】<a href="https://iconbuddy.app/">Iconbuddy — 180K+ open source icons</a></li><li>【AI漫画|可选择漫画风格和页面布局】<a href="https://huggingface.co/spaces/jbilcke-hf/ai-comic-factory">AI Comic Factory - a Hugging Face Space by jbilcke-hf</a></li><li>【Figma在线版】<a href="https://www.figma.com/files/recents-and-sharing/recently-viewed">Figma</a></li></ul><h4 id="下载工具"><a href="#下载工具" class="headerlink" title="下载工具"></a>下载工具</h4><ul><li>【YouTube视频解析下载 】<a href="https://snapsave.io/zh-tw18">snapsave.io</a></li><li>【bilibili视频下载】<a href="https://bili.iiilab.com/">bili.iiilab</a></li></ul><h4 id="格式压缩"><a href="#格式压缩" class="headerlink" title="格式压缩"></a>格式压缩</h4><ul><li>【图片压缩软件】<a href="https://www.ticompressor.com/online/">Topspeed Image Compressor 在线压缩</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%86%85%E5%AE%B9%E5%88%86%E7%B1%BB&quot;&gt;内容分类&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E7%BD%91%E7%AB%99&quot;&gt;</summary>
      
    
    
    
    <category term="杂项" scheme="https://zade23.github.io/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
    <category term="工具网站" scheme="https://zade23.github.io/tags/%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>Python元组集合字典</title>
    <link href="https://zade23.github.io/2023/10/18/Python%E5%85%83%E7%BB%84%E9%9B%86%E5%90%88%E5%AD%97%E5%85%B8/"/>
    <id>https://zade23.github.io/2023/10/18/Python%E5%85%83%E7%BB%84%E9%9B%86%E5%90%88%E5%AD%97%E5%85%B8/</id>
    <published>2023-10-18T08:20:18.000Z</published>
    <updated>2024-03-14T02:59:36.156Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#1-%E5%85%83%E7%BB%84">1. 元组</a><ul><li><a href="#11-%E5%85%83%E7%BB%84%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96">1.1 元组的初始化</a></li><li><a href="#12-%E5%85%83%E7%BB%84%E7%9A%84%E8%A7%A3%E5%8C%85">1.2 元组的解包</a></li><li><a href="#13-%E5%85%83%E7%BB%84%E7%9A%84%E5%85%B6%E4%BB%96%E6%93%8D%E4%BD%9C">1.3 元组的其他操作</a></li></ul></li><li><a href="#2-%E9%9B%86%E5%90%88">2. 集合</a><ul><li><a href="#21-%E9%9B%86%E5%90%88%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96">2.1 集合的初始化</a></li><li><a href="#22-%E9%9B%86%E5%90%88%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C">2.2 集合的常用操作</a></li><li><a href="#23-%E4%BD%BF%E7%94%A8for%E5%BE%AA%E7%8E%AF%E9%81%8D%E5%8E%86%E9%9B%86%E5%90%88">2.3 使用<code>for</code>循环遍历集合</a></li></ul></li><li><a href="#3-%E5%AD%97%E5%85%B8">3. 字典</a><ul><li><a href="#31-%E5%AD%97%E5%85%B8%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96">3.1 字典的初始化</a></li><li><a href="#32-%E5%AD%97%E5%85%B8%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C">3.2 字典的常用操作</a></li><li><a href="#33-%E4%BD%BF%E7%94%A8for%E5%BE%AA%E7%8E%AF%E9%81%8D%E5%8E%86%E5%AD%97%E5%85%B8">3.3 使用<code>for</code>循环遍历字典</a></li></ul></li><li><a href="#4-%E4%BD%9C%E4%B8%9A%E6%89%A9%E5%B1%95">4. 作业扩展</a></li></ul><hr><h3 id="1-元组"><a href="#1-元组" class="headerlink" title="1. 元组"></a>1. 元组</h3><p>元组跟 <u>列表</u> 类似，只是不支持动态添加、删除元素，以及不能修改元素。</p><h4 id="1-1-元组的初始化"><a href="#1-1-元组的初始化" class="headerlink" title="1.1 元组的初始化"></a>1.1 元组的初始化</h4><p>元组需要用小括号括起来，中间的元素用逗号隔开。</p><p><strong>注意</strong>:如果初始化只包含一个元素的元组，需要在该元素后添加逗号。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = ()  <span class="comment"># 初始化一个空元组</span></span><br><span class="line">b = (<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 含有2个整数的元组</span></span><br><span class="line">c = <span class="number">6</span>, <span class="string">&quot;Python&quot;</span>, <span class="number">3.14</span>  <span class="comment"># 小括号可以省略，等价于(6, &quot;Python&quot;, 3.14)</span></span><br><span class="line">d = (<span class="number">5</span>,)  <span class="comment"># 注意不能写成(5)，(5)表示整数5</span></span><br><span class="line">e = <span class="number">5</span>,  <span class="comment"># 等价于(5,)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a, b, c, d, e)</span><br></pre></td></tr></table></figure><h4 id="1-2-元组的解包"><a href="#1-2-元组的解包" class="headerlink" title="1.2 元组的解包"></a>1.2 元组的解包</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = <span class="number">12345</span>, <span class="number">54321</span>, <span class="string">&quot;Hello!&quot;</span>  <span class="comment"># 初始化一个元组</span></span><br><span class="line">x, y, z = t  <span class="comment"># 将元组解包，将元组内的三个值按顺序赋值给x、y、z</span></span><br><span class="line"><span class="built_in">print</span>(x, y, z)</span><br></pre></td></tr></table></figure><p>所以，判断语句中的交换操作，本质上是元组的解包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a, b = <span class="number">3</span>, <span class="number">4</span>  <span class="comment"># 将元组(3, 4)解包，分别赋值给a、b</span></span><br><span class="line">a, b = b, a  <span class="comment"># 将元组(b, a)解包，分别赋值给a、b</span></span><br></pre></td></tr></table></figure><p>同样地，函数中函数返回多个值，本质上也是返回了一个元组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calc</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x + y, x * y  <span class="comment"># 等价于 return (x + y, x * y)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x, y = <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">s, p = calc(x, y)  <span class="comment"># 将(x + y, x * y)解包，分别赋值给s、p</span></span><br><span class="line"><span class="built_in">print</span>(s, p)</span><br></pre></td></tr></table></figure><h4 id="1-3-元组的其他操作"><a href="#1-3-元组的其他操作" class="headerlink" title="1.3 元组的其他操作"></a>1.3 元组的其他操作</h4><p>元组的下标访问元素、循环遍历、切片、加法和乘法运算等操作，都与列表相同。</p><h3 id="2-集合"><a href="#2-集合" class="headerlink" title="2. 集合"></a>2. 集合</h3><p>集合是Python中最常用的数据结构之一，用来存储不同元素。<br>注意，集合中的元素是无序的。</p><h4 id="2-1-集合的初始化"><a href="#2-1-集合的初始化" class="headerlink" title="2.1 集合的初始化"></a>2.1 集合的初始化</h4><p>创建集合用花括号或<code>set()</code>函数。注意：创建空集合只能用<code>set()</code>，不能用<code>&#123;&#125;</code>，因为<code>&#123;&#125;</code>创建的是空字典，会在下一小节里介绍字典。</p><p>集合常见的初始化方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">basket = &#123;<span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;orange&#x27;</span>, <span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;pear&#x27;</span>, <span class="string">&#x27;orange&#x27;</span>, <span class="string">&#x27;banana&#x27;</span>&#125;  <span class="comment"># 会自动去除重复元素</span></span><br><span class="line"><span class="built_in">print</span>(basket)  <span class="comment"># 重复的元素已经去除了</span></span><br><span class="line"></span><br><span class="line">s = <span class="built_in">set</span>()  <span class="comment"># 初始化一个空列表</span></span><br><span class="line"><span class="built_in">print</span>(s)</span><br><span class="line"></span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">b = <span class="built_in">set</span>(a)  <span class="comment"># 将列表转化成集合，一般是为了去重。</span></span><br><span class="line">c = <span class="built_in">list</span>(b)  <span class="comment"># 将集合转化回列表</span></span><br><span class="line"><span class="built_in">print</span>(b, c)</span><br><span class="line"></span><br><span class="line">x = <span class="string">&quot;abracadabra&quot;</span></span><br><span class="line">a = <span class="built_in">set</span>(x)  <span class="comment"># 将字符串中的每个字符存到集合中</span></span><br><span class="line">b = <span class="built_in">str</span>(a)  <span class="comment"># 注意，这里并不能将集合转化回原字符串，而是用格式化表示集合中的内容</span></span><br><span class="line"><span class="built_in">print</span>(a, b)</span><br></pre></td></tr></table></figure><h4 id="2-2-集合的常用操作"><a href="#2-2-集合的常用操作" class="headerlink" title="2.2 集合的常用操作"></a>2.2 集合的常用操作</h4><p>假设<code>a</code>表示一个集合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(a) 返回集合中包含的元素数量。</span><br><span class="line">a.add(x) 在集合中添加一个元素。</span><br><span class="line">a.remove(x) 删除集合中的x，如果x不存在，则报异常。</span><br><span class="line">a.discard(x) 删除集合中的x，如果x不存在，则不进行任何操作。</span><br><span class="line">x <span class="keyword">in</span> a 判断x是否在a中。</span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(a))  <span class="comment"># 输出3</span></span><br><span class="line"></span><br><span class="line">a.add(<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)  <span class="comment"># 输出 &#123;1, 2, 3, 4&#125;，注意集合中的元素是无序的。</span></span><br><span class="line"></span><br><span class="line">a.remove(<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(a)  <span class="comment"># 输出 &#123;1, 3, 4&#125;</span></span><br><span class="line"></span><br><span class="line">a.remove(<span class="number">5</span>)  <span class="comment"># 因为5不存在，所以会报异常</span></span><br><span class="line"></span><br><span class="line">a.discard(<span class="number">5</span>)  <span class="comment"># 因为5不存在，所以不进行任何操作</span></span><br><span class="line"><span class="built_in">print</span>(a)  <span class="comment"># &#123;1, 3, 4&#125;</span></span><br></pre></td></tr></table></figure><h4 id="2-3-使用for循环遍历集合"><a href="#2-3-使用for循环遍历集合" class="headerlink" title="2.3 使用for循环遍历集合"></a>2.3 使用<code>for</code>循环遍历集合</h4><p>类似于列表，集合也可以用<code>for ... in ...</code>的形式遍历。例如：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> a:  <span class="comment"># 循环遍历整个集合</span></span><br><span class="line">    <span class="built_in">print</span>(x, end=<span class="string">&#x27; &#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="3-字典"><a href="#3-字典" class="headerlink" title="3. 字典"></a>3. 字典</h3><p>字典是Python中最常用的数据结构之一，用来存储映射关系。<br>注意，字典中的元素是无序的。</p><p>不同于列表，字典是以<code>key</code>进行索引的，可以将每个<code>key</code>映射到某个<code>value</code>。<code>key</code>可以是任何不可变类型，常用可以作为<code>key</code>的类型有数字和字符串。列表因为是可变的，所以不能作为<code>key</code>。<code>value</code>可以是任意类型。</p><h4 id="3-1-字典的初始化"><a href="#3-1-字典的初始化" class="headerlink" title="3.1 字典的初始化"></a>3.1 字典的初始化</h4><p>创建字典用花括号或<code>dict()</code>函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tel = &#123;<span class="string">&#x27;jack&#x27;</span>: <span class="number">4098</span>, <span class="string">&#x27;sape&#x27;</span>: <span class="number">4139</span>&#125;  <span class="comment"># 创建一个字典</span></span><br><span class="line"><span class="built_in">print</span>(tel)  <span class="comment"># 输出 &#123;&#x27;jack&#x27;: 4098, &#x27;sape&#x27;: 4139&#125;</span></span><br><span class="line"></span><br><span class="line">a = <span class="built_in">dict</span>()  <span class="comment"># 创建一个空字典</span></span><br><span class="line">a[<span class="number">123</span>] = <span class="string">&quot;abc&quot;</span>  <span class="comment"># 在字典中插入一个key-value对</span></span><br><span class="line">a[<span class="number">456</span>] = <span class="string">&quot;def&quot;</span>  <span class="comment"># 在字典中插入一个key-value对</span></span><br><span class="line"><span class="built_in">print</span>(a)  <span class="comment"># 输出 &#123;123: &#x27;abc&#x27;, 456: &#x27;def&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line">b = <span class="built_in">list</span>(a)  <span class="comment"># 将字典的关键字转化成列表</span></span><br><span class="line"><span class="built_in">print</span>(b)  <span class="comment"># 输出[123, 456]</span></span><br></pre></td></tr></table></figure><h4 id="3-2-字典的常用操作"><a href="#3-2-字典的常用操作" class="headerlink" title="3.2 字典的常用操作"></a>3.2 字典的常用操作</h4><p>假设<code>a</code>表示一个字典。</p><ul><li><code>len(a)</code>：返回字典中的元素对数。</li><li><code>a[x]</code>：获取关键字<code>x</code>对应的值，如果<code>x</code>不存在，会报异常。</li><li><code>a.get(x)</code>：获取关键字<code>x</code>对应的值，如果<code>x</code>不存在，会返回<code>None</code>，不会报异常。</li><li><code>a.get(x, y)</code>：获取关键字<code>x</code>对应的值，如果<code>x</code>不存在，会返回默认值<code>y</code>，不会报异常。</li><li><code>a[x] = y</code>：在字典中插入一对元素，如果关键字<code>x</code>已存在，则将它之前映射的值覆盖掉。</li><li><code>del a[x]</code>：删除关键字<code>x</code>对应的元素对，如果<code>x</code>不存在，会报异常。</li><li><code>x in a</code>：检查字典中是否存在关键字<code>x</code>。</li><li><code>x not in a</code>：检查字典中是否不存在关键字<code>x</code>。</li><li><code>a.keys()</code>：返回字典的所有<code>key</code>。</li><li><code>a.values()</code>：返回字典的所有<code>value</code>。</li><li><code>a.items()</code>：返回字典的所有由<code>key</code>和<code>value</code>组成的元组。</li></ul><p>例如：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">a = &#123;<span class="string">&#x27;abc&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;def&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;python&#x27;</span>: <span class="number">3</span>&#125;  <span class="comment"># 初始化一个字典</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(a))  <span class="comment"># 输出3</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="string">&#x27;def&#x27;</span>])  <span class="comment"># 输出2</span></span><br><span class="line"><span class="built_in">print</span>(a.get(<span class="string">&#x27;def&#x27;</span>))  <span class="comment"># 输出2</span></span><br><span class="line"><span class="built_in">print</span>(a.get(<span class="string">&#x27;xyz&#x27;</span>, <span class="number">5</span>))  <span class="comment"># 因为&#x27;xyz&#x27;不存在，所以输出默认值5</span></span><br><span class="line"></span><br><span class="line">a[<span class="string">&#x27;hello&#x27;</span>] = <span class="number">4</span>  <span class="comment"># 插入一对元素 &#x27;hello&#x27; -&gt; 4</span></span><br><span class="line"><span class="built_in">print</span>(a)  <span class="comment"># 输出&#123;&#x27;abc&#x27;: 1, &#x27;def&#x27;: 2, &#x27;python&#x27;: 3, &#x27;hello&#x27;: 4&#125;</span></span><br><span class="line"></span><br><span class="line">a[<span class="string">&#x27;def&#x27;</span>] = <span class="number">5</span>  <span class="comment"># 更新&#x27;def&#x27;映射的值</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="string">&#x27;def&#x27;</span>])  <span class="comment"># 输出5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> a[<span class="string">&#x27;python&#x27;</span>]  <span class="comment"># 删除关键字&#x27;python&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(a)  <span class="comment"># 输出&#123;&#x27;abc&#x27;: 1, &#x27;def&#x27;: 5, &#x27;hello&#x27;: 4&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hello&#x27;</span> <span class="keyword">in</span> a)  <span class="comment"># 输出True</span></span><br><span class="line"><span class="built_in">print</span>(a.keys())  <span class="comment"># 输出dict_keys([&#x27;abc&#x27;, &#x27;def&#x27;, &#x27;hello&#x27;])</span></span><br><span class="line"><span class="built_in">print</span>(a.values())  <span class="comment"># 输出dict_values([1, 5, 4])</span></span><br><span class="line"><span class="built_in">print</span>(a.items())  <span class="comment"># 输出dict_items([(&#x27;abc&#x27;, 1), (&#x27;def&#x27;, 5), (&#x27;hello&#x27;, 4)])</span></span><br></pre></td></tr></table></figure><h4 id="3-3-使用for循环遍历字典"><a href="#3-3-使用for循环遍历字典" class="headerlink" title="3.3 使用for循环遍历字典"></a>3.3 使用<code>for</code>循环遍历字典</h4><p>类似于列表，字典也可以用<code>for ... in ...</code>的形式遍历。例如：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = &#123;<span class="string">&#x27;abc&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;def&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;python&#x27;</span>: <span class="number">3</span>&#125;  <span class="comment"># 初始化一个字典</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> a:  <span class="comment"># 遍历key</span></span><br><span class="line">    <span class="built_in">print</span>(k, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>()  <span class="comment"># 输出回车</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> a.keys():  <span class="comment"># 遍历key</span></span><br><span class="line">    <span class="built_in">print</span>(k, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>()  <span class="comment"># 输出回车</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> a.values():  <span class="comment"># 遍历value</span></span><br><span class="line">    <span class="built_in">print</span>(v, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>()  <span class="comment"># 输出回车</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> a.items():  <span class="comment"># 遍历key-value对</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;(%s, %d) &quot;</span> % (k, v), end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>()  <span class="comment"># 输出回车</span></span><br></pre></td></tr></table></figure><h3 id="4-作业扩展"><a href="#4-作业扩展" class="headerlink" title="4. 作业扩展"></a>4. 作业扩展</h3><p><code>map()</code>也可以用<code>for ... in ...</code>的形式遍历。例如：<code>for x in map(int, input().split())</code>可以遍历一行内用空格隔开的每个整数。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#1-%E5%85%83%E7%BB%84&quot;&gt;1. 元组&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#11-%E5%85%83%E7%BB%84%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96&quot;&gt;1.1 元组的初始化&lt;/</summary>
      
    
    
    
    
    <category term="Python" scheme="https://zade23.github.io/tags/Python/"/>
    
  </entry>
  
</feed>
