<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>pytorch-tutorial-official | ANdRoid&#39;s BLOG</title>
  <meta name="description" content="Pytorch 基础 https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;basics&#x2F;intro.html  大多数机器学习工作流：  处理数据 创建模型 优化模型参数 保存训练后模型  通过 Pytorch 基础部分的内容，读者可以完整的走完一整个MachineLearning的工作流，若读者对其中某个环节不理解或感兴趣，针对这些工作流中的每一个环节都有相关的扩展阅读">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch-tutorial-official">
<meta property="og:url" content="https://zade23.github.io/2023/10/31/pytorch-tutorial-official/index.html">
<meta property="og:site_name" content="ANdRoid&#39;s BLOG">
<meta property="og:description" content="Pytorch 基础 https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;basics&#x2F;intro.html  大多数机器学习工作流：  处理数据 创建模型 优化模型参数 保存训练后模型  通过 Pytorch 基础部分的内容，读者可以完整的走完一整个MachineLearning的工作流，若读者对其中某个环节不理解或感兴趣，针对这些工作流中的每一个环节都有相关的扩展阅读">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://z1.ax1x.com/2023/11/01/pin7WJf.md.jpg">
<meta property="og:image" content="https://z1.ax1x.com/2023/11/06/pilKoHf.png">
<meta property="article:published_time" content="2023-10-31T08:49:36.000Z">
<meta property="article:modified_time" content="2023-11-07T02:45:37.490Z">
<meta property="article:author" content="Android">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://z1.ax1x.com/2023/11/01/pin7WJf.md.jpg">
  <!-- Canonical links -->
  <link rel="canonical" href="https://zade23.github.io/2023/10/31/pytorch-tutorial-official/index.html">
  
    <link rel="alternate" href="/atom.xml" title="ANdRoid&#39;s BLOG" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 6.3.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/zade23" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Android</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">Student &amp; Coder</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> GuangZhou, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">项目</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">关于</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/zade23" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://weibo.com/u/5382156286" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>别想那么多，去码头整点儿薯条吧</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/whatever/">whatever</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/" rel="tag">Algorithm</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Go/" rel="tag">Go</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LeetCode/" rel="tag">LeetCode</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/test/" rel="tag">test</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/" rel="tag">工具网站</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%B0%8F%E7%9F%A5%E8%AF%86/" rel="tag">经济学小知识</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 14px;">Algorithm</a> <a href="/tags/Go/" style="font-size: 13px;">Go</a> <a href="/tags/Hexo/" style="font-size: 13px;">Hexo</a> <a href="/tags/LeetCode/" style="font-size: 13px;">LeetCode</a> <a href="/tags/test/" style="font-size: 13px;">test</a> <a href="/tags/%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/" style="font-size: 13px;">工具网站</a> <a href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%B0%8F%E7%9F%A5%E8%AF%86/" style="font-size: 13px;">经济学小知识</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a><span class="archive-list-count">3</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2023/10/31/pytorch-tutorial-official/" class="title">pytorch-tutorial-official</a>
              </p>
              <p class="item-date">
                <time datetime="2023-10-31T08:49:36.000Z" itemprop="datePublished">2023-10-31</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2023/10/23/Go%E8%AF%AD%E8%A8%80%E9%80%9F%E9%80%9A/" class="title">Go语言速通</a>
              </p>
              <p class="item-date">
                <time datetime="2023-10-23T07:43:29.000Z" itemprop="datePublished">2023-10-23</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2023/10/19/Python%E5%BC%82%E5%B8%B8%E7%9A%84%E5%A4%84%E7%90%86-try-except/" class="title">Python异常的处理(try...except)</a>
              </p>
              <p class="item-date">
                <time datetime="2023-10-19T08:46:45.000Z" itemprop="datePublished">2023-10-19</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2023/10/18/%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/" class="title">工具网站</a>
              </p>
              <p class="item-date">
                <time datetime="2023-10-18T09:04:52.000Z" itemprop="datePublished">2023-10-18</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2023/10/18/Python%E5%85%83%E7%BB%84%E9%9B%86%E5%90%88%E5%AD%97%E5%85%B8/" class="title">Python元组集合字典</a>
              </p>
              <p class="item-date">
                <time datetime="2023-10-18T08:20:18.000Z" itemprop="datePublished">2023-10-18</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-pytorch-tutorial-official" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      pytorch-tutorial-official
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2023/10/31/pytorch-tutorial-official/" class="article-date">
	  <time datetime="2023-10-31T08:49:36.000Z" itemprop="datePublished">2023-10-31</time>
	</a>
</span>
        
        

        
	<span class="article-read hidden-xs">
	    <i class="icon icon-eye-fill" aria-hidden="true"></i>
	    <span id="busuanzi_container_page_pv">
			<span id="busuanzi_value_page_pv">0</span>
		</span>
	</span>


        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2023/10/31/pytorch-tutorial-official/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h1 id="Pytorch-基础"><a href="#Pytorch-基础" class="headerlink" title="Pytorch 基础"></a>Pytorch 基础</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/intro.html">https://pytorch.org/tutorials/beginner/basics/intro.html</a></p>
</blockquote>
<p>大多数机器学习工作流：</p>
<ul>
<li>处理数据</li>
<li>创建模型</li>
<li>优化模型参数</li>
<li>保存训练后模型</li>
</ul>
<p>通过 Pytorch 基础部分的内容，读者可以完整的走完一整个MachineLearning的工作流，若读者对其中某个环节不理解或感兴趣，针对这些工作流中的每一个环节都有相关的扩展阅读链接。</p>
<p>我们将使用 FashionMNIST 数据集训练一个神经网络，该神经网络预测输入图像是否属于一下类别之一：T恤&#x2F;上衣、裤子、套头衫、连衣裙、外套、凉鞋、成山、运动鞋、包包、靴子。（是个多分类任务）</p>
<h2 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html</a></p>
</blockquote>
<p>本节会快速走完一个机器学习多分类的Demo，以此快速了解流程中必要的基本ML相关API。</p>
<h3 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h3><p>Pytorch 中有两个用于处理数据的子库 <code>torch.utils.data.DataLoader</code> 和 <code>torch.utils.data.Dataset.Dataset</code>。顾名思义，<code>Dataset</code> 存储样本及其相应的标签，并将 <code>DataLoader</code> 可迭代对象包装在 <code>Dataset</code> 中。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvison.transforms <span class="keyword">import</span> ToTensor</span><br></pre></td></tr></table></figure>

<p>通过上面的引用(import)，我们可以发现：Pytorch 中有非常多的子库，这些子库专注于某一特定的领域，例如： <a target="_blank" rel="noopener" href="https://pytorch.org/text/stable/index.html">TorchText</a>, <a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/index.html">TorchVision</a>, 和 <a target="_blank" rel="noopener" href="https://pytorch.org/audio/stable/index.html">TorchAudio</a>, 这些所有子库中都包含相应的数据集。</p>
<p>本次教程中我们使用 <code>TorchVision</code> 数据集。</p>
<p>该 <code>torchvision,.datasets</code> 模块包含 <code>Dataset</code> 来自现实世界中的视觉图像数据，最经典的有：CIFAT，COCO(<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html">full list here</a>)</p>
<p>本次教程中我们使用 <code>FashionMNIST</code> 数据集。每个 <code>TorchVison</code> 下的 <code>Dataset</code> 都包含两个参数：<code>transform</code> 和 <code>target_transform</code> 分别用来<strong>修改样本</strong>与<strong>打标签</strong>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从公开数据集中读取训练数据</span></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">True</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = ToTensor(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从公开数据集中读取测试数据</span></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">False</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = Totensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>下图是在 colab 中运行上面程序块的输出结果：<br><a target="_blank" rel="noopener" href="https://imgse.com/i/pin7WJf"><img src="https://z1.ax1x.com/2023/11/01/pin7WJf.md.jpg" alt="pin7WJf.md.jpg"></a></p>
<p>至此为止，通过上面的工作，我们将 <code>Dataset</code> 作为参数传递给了 <code>DataLoader</code> 。同时封装了相关数据集作为一个可迭代的对象，支持自动批处理、采样、洗牌(shuffling)、和多进程的数据加载。</p>
<hr>
<p>下一步中，我们定义 <code>batch_size = 64</code> ，即 <code>dataloader</code> 可迭代中的每个元素都将返回一批含有64个特征的标签。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据读取工具</span></span><br><span class="line">train_dataloader = DataLoader(tarain_data, batch_size = batch_size)</span><br><span class="line">test_dataloader = DataLoader(tast_data, batch_size = batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of X [N, C, H, W]: <span class="subst">&#123;X.shape&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of y []: <span class="subst">&#123;y.shape&#125;</span> <span class="subst">&#123;y.dtpye&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])</span><br><span class="line">Shape of y: torch.Size([64]) torch.int64</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">loading data in PyTorch</a> 的详细说明</p>
</blockquote>
<hr>
<h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><p>为了在 Pytorch 中定义神经网络，我们创建一个继承自  <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module</a> 的类。</p>
<p>我们通过 <code>__init__</code> 函数定义神经网络的层，并指明数据如何通过 <code>forward</code> 函数进入神经网络层。</p>
<blockquote>
<p>在设备允许的情况下，推荐使用GPU来加速神经网络的运算操作。</p>
</blockquote>
<p>代码实现：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取用于训练的设备（cpu/gpu/mps）</span></span><br><span class="line">device = (</span><br><span class="line">    <span class="string">&quot;cuda&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">else</span> <span class="string">&quot;mps&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.backends.mps.is_available()</span><br><span class="line">    <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using <span class="subst">&#123;device&#125;</span> device&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义神经网络的模型结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<p>打印结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Using cpu device</span><br><span class="line">NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="在-PyTorch-中构建神经网络"><a href="#在-PyTorch-中构建神经网络" class="headerlink" title="在 PyTorch 中构建神经网络"></a>在 PyTorch 中构建神经网络</h4><blockquote>
<p>这一部分是对‘构建模型’部分的一点补充说明，也是 PyTorch 官网教程中的扩展阅读部分</p>
</blockquote>
<p>神经网络是由多个对数据进行操作的层&#x2F;模型组合而成的。<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">torch.nn</a>  命名空间几乎已经提供了构建一个神经网络所需要用到的<strong>所有模块</strong>。</p>
<p>所有模块都在 PyTorch 下的子块  <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module</a> 中提供。</p>
<p>基于这样的结构化嵌套模块，整个神经网络可以自由的进行构建和管理复杂的架构。</p>
<p>在上面的代码块中，我们通过 <code>NeuralNetwork</code> 函数定义了一个神经网络模型 <code>model</code>。</p>
<p>为了使用该模型，我们将输入数据传递给它。这个操作将执行 <code>forward</code> 操作和一些<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866">后台操作</a>。</p>
<p>请记住：不要直接使用 <code>model.forward()</code> !</p>
<hr>
<p>通过输入操作调用模型，最后将返回一个二维张量，其中 dim &#x3D; 0 对应于每个类别的 10 个原始预测输出，dim &#x3D; 1 对应与每个输出的单个值。</p>
<p>我们可以通过 <code>nn.Softmax</code> 模块实例传递对结果预测的概率来进行最终预测概率的判断。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device = device)</span><br><span class="line">logits = model(X)</span><br><span class="line">pred_probab = nn.Softmax(dim = <span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Predicted class: tensor([7], device = &#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>
<h5 id="模型层-Model-Layers"><a href="#模型层-Model-Layers" class="headerlink" title="模型层 Model Layers"></a>模型层 Model Layers</h5><p>分解 <code>FashionMNIST</code> 模型中的各层。为了说明这一点，我们通过获取一个包含 3 张大小为 28*28 的小批量图像样本，看看当数据传递到网络时会发生什么。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_image = torch.rand(<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(input_image.size())</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 28, 28])</span><br></pre></td></tr></table></figure>

<h5 id="nn-Flatten"><a href="#nn-Flatten" class="headerlink" title="nn.Flatten"></a>nn.Flatten</h5><p>初始化 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html">nn.Flatten</a> 层，将每个2D 28*28 图像转换成包含 784 个像素值的连续数组（保持小批量尺寸(dim &#x3D; 0)）</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line"><span class="built_in">print</span>(flat_image.size())</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 784])</span><br></pre></td></tr></table></figure>

<h5 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear"></a>nn.Linear</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">线性层模块</a>通过输入的权重w和偏差值b进行线性变换。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layer1 = nn.Linear(in_features = <span class="number">28</span>*<span class="number">28</span>, out_features = <span class="number">20</span>)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line"><span class="built_in">print</span>(hidden1.size())</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 20])</span><br></pre></td></tr></table></figure>

<h5 id="nn-ReLU"><a href="#nn-ReLU" class="headerlink" title="nn.ReLU"></a>nn.ReLU</h5><p>非线性激活函数可以在模型的输入输出之间创建复杂的映射关系。激活函数通过引入非线性的变换帮助神经网络学习各种现象。</p>
<p>在实例模型中，我们在线性层之间使用ReLU激活函数。但还有其他激活函数可以在模型的线性层中间作为激活函数使用，详情参考：<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-cn/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">激活函数-wiki</a></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Before ReLU: <span class="subst">&#123;hidden1&#125;</span>\n\n&quot;</span>)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Aftr RelU: <span class="subst">&#123;hidden1&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Before ReLU: tensor([[ 0.4158, -0.0130, -0.1144,  0.3960,  0.1476, -0.0690, -0.0269,  0.2690,</span><br><span class="line">          0.1353,  0.1975,  0.4484,  0.0753,  0.4455,  0.5321, -0.1692,  0.4504,</span><br><span class="line">          0.2476, -0.1787, -0.2754,  0.2462],</span><br><span class="line">        [ 0.2326,  0.0623, -0.2984,  0.2878,  0.2767, -0.5434, -0.5051,  0.4339,</span><br><span class="line">          0.0302,  0.1634,  0.5649, -0.0055,  0.2025,  0.4473, -0.2333,  0.6611,</span><br><span class="line">          0.1883, -0.1250,  0.0820,  0.2778],</span><br><span class="line">        [ 0.3325,  0.2654,  0.1091,  0.0651,  0.3425, -0.3880, -0.0152,  0.2298,</span><br><span class="line">          0.3872,  0.0342,  0.8503,  0.0937,  0.1796,  0.5007, -0.1897,  0.4030,</span><br><span class="line">          0.1189, -0.3237,  0.2048,  0.4343]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">After ReLU: tensor([[0.4158, 0.0000, 0.0000, 0.3960, 0.1476, 0.0000, 0.0000, 0.2690, 0.1353,</span><br><span class="line">         0.1975, 0.4484, 0.0753, 0.4455, 0.5321, 0.0000, 0.4504, 0.2476, 0.0000,</span><br><span class="line">         0.0000, 0.2462],</span><br><span class="line">        [0.2326, 0.0623, 0.0000, 0.2878, 0.2767, 0.0000, 0.0000, 0.4339, 0.0302,</span><br><span class="line">         0.1634, 0.5649, 0.0000, 0.2025, 0.4473, 0.0000, 0.6611, 0.1883, 0.0000,</span><br><span class="line">         0.0820, 0.2778],</span><br><span class="line">        [0.3325, 0.2654, 0.1091, 0.0651, 0.3425, 0.0000, 0.0000, 0.2298, 0.3872,</span><br><span class="line">         0.0342, 0.8503, 0.0937, 0.1796, 0.5007, 0.0000, 0.4030, 0.1189, 0.0000,</span><br><span class="line">         0.2048, 0.4343]], grad_fn=&lt;ReluBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h5 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h5><p>nn.Sequential是一个有序的模块容器。数据按照定义好的方式顺序的通过当前模块。<br>您可以使用顺序容器来组合一个“快捷网络” ，例如：<code>seq_modules</code> .</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">logits = seq_modules(input_image)</span><br></pre></td></tr></table></figure>

<h5 id="nn-Softmax"><a href="#nn-Softmax" class="headerlink" title="nn.Softmax"></a>nn.Softmax</h5><p>Softmax激活函数通常用在最后一个线性层，用来返回对数区间介于 [-infty, infty] 中的原始值，这些值最终被传递给 <code>nn.Softmax</code> 模块。</p>
<p>Softmax 激活函数将对应输出区间范围缩放在 [0, 1] 之间，<strong>表示模型对每个类别的预测概率</strong>。其中，<code>dim</code> 中所有参数指示值求和应该为 1 。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">softmax = nn.Softmax(dim = <span class="number">1</span>)</span><br><span class="line">pred_probab = softmax(logits)</span><br></pre></td></tr></table></figure>

<h5 id="模型的参数"><a href="#模型的参数" class="headerlink" title="模型的参数"></a>模型的参数</h5><p>神经网络往往非常的复杂，在整个网络的构建过程中，如果可以便捷的将每个部分表示出来，对于训练过程中的优化和修改相对的权重与偏差等都会有非常大的帮助。</p>
<p>子类化 <code>nn.Module</code> 模块可以帮助我们解决这个问题，该模块会自动跟踪模型对象中定义的所有字段，并使用模型的 <code>parameters()</code> 函数或 <code>named_parameters()</code> 函数方法访问所有参数。</p>
<p>在本次示例中，我们遍历每个参数，并预览它们的所有数值参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model structure: <span class="subst">&#123;model&#125;</span>\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span> | Size: <span class="subst">&#123;param.size()&#125;</span> | Values : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Model structure: NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],</span><br><span class="line">        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0155, -0.0327], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0116,  0.0293, -0.0280,  ...,  0.0334, -0.0078,  0.0298],</span><br><span class="line">        [ 0.0095,  0.0038,  0.0009,  ..., -0.0365, -0.0011, -0.0221]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0148, -0.0256], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0147, -0.0229,  0.0180,  ..., -0.0013,  0.0177,  0.0070],</span><br><span class="line">        [-0.0202, -0.0417, -0.0279,  ..., -0.0441,  0.0185, -0.0268]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0070, -0.0411], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h2 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量 Tensor"></a>张量 Tensor</h2><p>通过一张图初步了解常见的多维空间数据的命名方式（来源：<a target="_blank" rel="noopener" href="https://youtu.be/ORMx45xqWkA?si=Njw0z1RULmPeeit9&t=17">PyTorch in 100 Seconds</a>）<br><a target="_blank" rel="noopener" href="https://imgse.com/i/pilKoHf"><img src="https://z1.ax1x.com/2023/11/06/pilKoHf.png" alt="不同维度参数的命名方式"></a></p>
<p>Tensor 又称 张量，是一种专门的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量对比模型的输入和输出以及模型的参数进行编码。</p>
<p>Tensors 类似于 NumPy 中的ndarrays，不同之处在于张量可以在 GPU 或其他硬件加速器上运行。事实上，张量和 NumPy 数组通常可以共享相同的底层内存，从而消除了复制数据的需要。</p>
<p>张量也针对自动微分进行了优化。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h3 id="初始化张量"><a href="#初始化张量" class="headerlink" title="初始化张量"></a>初始化张量</h3><p>一般可以通过如下方式初始化 <code>Tensor</code>:</p>
<ul>
<li>直接通过数据创建</li>
<li>通过NumPy创建</li>
<li>通过继承另一个Tensor的形状和数据类型</li>
<li>使用随机值或常量</li>
</ul>
<p>下面分别进行介绍：</p>
<h4 id="直接来自数据"><a href="#直接来自数据" class="headerlink" title="直接来自数据"></a>直接来自数据</h4><p>张量可以直接从已有的数据中创建，数据类型是自动推断的。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure>
<h4 id="来自另一个-Tensor"><a href="#来自另一个-Tensor" class="headerlink" title="来自另一个 Tensor"></a>来自另一个 Tensor</h4><p>新建的张量保留参考张量的部分参数（<strong>形状，数据类型</strong>），除非用显式的方式直接覆盖。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># retains the properties of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line">x_rand = torch.rand_like(x_data, dtype = torch.<span class="built_in">float</span>) <span class="comment"># overrides the datatype of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[1, 1],</span><br><span class="line">        [1, 1]])</span><br><span class="line"></span><br><span class="line">Random Tensor:</span><br><span class="line"> tensor([[0.8823, 0.9150],</span><br><span class="line">        [0.3829, 0.9593]])</span><br></pre></td></tr></table></figure>

<h4 id="使用随机值或常量"><a href="#使用随机值或常量" class="headerlink" title="使用随机值或常量"></a>使用随机值或常量</h4><p><code>shape</code> 是张量维度的元组表达式。在下面的函数中，它决定了输出张量的维度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 Tensor 的维度</span></span><br><span class="line">shape = (<span class="number">2</span>, <span class="number">3</span>, )</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones_Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Zeros_Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Random Tensor:</span><br><span class="line"> tensor([[0.3904, 0.6009, 0.2566],</span><br><span class="line">        [0.7936, 0.9408, 0.1332]])</span><br><span class="line"></span><br><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]])</span><br><span class="line"></span><br><span class="line">Zeros Tensor:</span><br><span class="line"> tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure>

<h3 id="张量的属性"><a href="#张量的属性" class="headerlink" title="张量的属性"></a>张量的属性</h3><p>Tensor 的属性描述了<u><strong>它们的形状</strong></u>、<u><strong>数据类型</strong></u> 和 <u><strong>存储它们的设备</strong></u>。</p>
<ul>
<li><code>tensor.shape</code></li>
<li><code>tensor.dtype</code></li>
<li><code>tensor.device</code></li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Dtype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device of tensor: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Shape of tensor: torch.Size([3, 4])</span><br><span class="line">Datatype of tensor: torch.float32</span><br><span class="line">Device tensor is stored on: cpu</span><br></pre></td></tr></table></figure>

<h3 id="张量的操作"><a href="#张量的操作" class="headerlink" title="张量的操作"></a>张量的操作</h3><p>科学计算是深度学习领域的根本！PyTorch提供了 100+ 张量运算操作，包括算术运算、线性代数运算、矩阵运算（转职、索引、切片）、采样等。</p>
<p><u><strong><code>PyTorch</code> 中的所有逻辑运算都可以通过GPU进行加速运算</strong></u></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将运行设备选择为 GPU （如果你有的话）</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tensor = tensor.to(<span class="string">&quot;cuda&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="类似-NumPy-的索引和切片操作"><a href="#类似-NumPy-的索引和切片操作" class="headerlink" title="类似 NumPy 的索引和切片操作"></a>类似 NumPy 的索引和切片操作</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Tensor= torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First row: <span class="subst">&#123;tensor[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First column: <span class="subst">&#123;tensor[:, <span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;last column: <span class="subst">&#123;tensor[..., -<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line">tensor[:, <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">First row: tensor([1., 1., 1., 1.])</span><br><span class="line">First column: tensor([1., 1., 1., 1])</span><br><span class="line">Last column: tensor([1., 1., 1., 1])</span><br><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure>

<h4 id="Tensor-之间的连接"><a href="#Tensor-之间的连接" class="headerlink" title="Tensor 之间的连接"></a>Tensor 之间的连接</h4><p><code>torch.cat</code> 可以用于连接指定维度的张量，拥有同样功能的另一个算子是 <code>torch.stack_</code> （参考：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.stack.html">torch.stack_</a>）。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim = <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure>

<h4 id="张量的算术运算"><a href="#张量的算术运算" class="headerlink" title="张量的算术运算"></a>张量的算术运算</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算两个张量之间的矩阵乘法。其中，y1, y2, y3 拥有相同的参数值</span></span><br><span class="line"><span class="comment"># `tensor.T` 返回张量的转置</span></span><br><span class="line">y1 = tensor @ tensor.T</span><br><span class="line">y2 = tensor.matmul(tensor.T)</span><br><span class="line"></span><br><span class="line">y3 = torch.rand_like(y1)</span><br><span class="line">torch.matmul(tensor, tensor.T, out = y3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将张量中的元素逐个相乘。z1, z2, z3 具有相同的值</span></span><br><span class="line">z1 = tensor * tensor</span><br><span class="line">z2 = tensor.mul(tensor)</span><br><span class="line"></span><br><span class="line">z3 = torch.rand_like(tensor)</span><br><span class="line">torch.mul(tensor, tensor, out = z3)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1],</span><br><span class="line">        [1., 0., 1., 1],</span><br><span class="line">        [1., 0., 1., 1],</span><br><span class="line">        [1., 0., 1., 1]])</span><br></pre></td></tr></table></figure>

<h4 id="单一元素张量"><a href="#单一元素张量" class="headerlink" title="单一元素张量"></a>单一元素张量</h4><p>如果你有一个单一元素的张量，例如希望通过将张量的所有值聚合为一个值，那么可以使用 <code>item()</code> 将其转换为 python 数值：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">agg = tensor.<span class="built_in">sum</span>()</span><br><span class="line">agg_item = agg.item()</span><br><span class="line"><span class="built_in">print</span>(agg_item, <span class="built_in">type</span>(agg_item))</span><br></pre></td></tr></table></figure>

<p>打印输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12.0 &lt;class &#x27;float&#x27;&gt;</span><br></pre></td></tr></table></figure>

<h4 id="原地操作-不通过返回的方式进行原地修改"><a href="#原地操作-不通过返回的方式进行原地修改" class="headerlink" title="原地操作(不通过返回的方式进行原地修改)"></a>原地操作(不通过返回的方式进行原地修改)</h4><p>将结果存储在操作数中的操作成为原地操作。它们由 <code>_</code> 后缀表示。例如：<code>x.copy()</code> 、 <code>x.t_()</code> ，都将直接修改 <code>x</code>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;tensor&#125;</span> \n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br><span class="line"></span><br><span class="line">tensor([[6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.]])</span><br></pre></td></tr></table></figure>

<h3 id="Tensor-和-Numpy-二者转换"><a href="#Tensor-和-Numpy-二者转换" class="headerlink" title="Tensor 和 Numpy 二者转换"></a>Tensor 和 Numpy 二者转换</h3><p>位于 CPU 位置上的 Numpy 数组与 Tensor 可以共享同一个底层的内容空间，更改一个 tensor 会同时修改另一个 tensor 。</p>
<h4 id="Tensor2NumPy-array"><a href="#Tensor2NumPy-array" class="headerlink" title="Tensor2NumPy_array"></a>Tensor2NumPy_array</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印对比结果（t 代表 tensor；n 代表 numpy）:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([1., 1., 1., 1., 1.])</span><br><span class="line">n: [1. 1. 1. 1. 1.]</span><br></pre></td></tr></table></figure>

<p>下一步，改变 tensor 中的值，同时观察 NumPy 中值的变化：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: (n)&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印对比结果（t 代表 tensor；n 代表 numpy）:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([2., 2., 2., 2., 2.])</span><br><span class="line">n: [2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure>

<h4 id="NumPy-array2Tensor"><a href="#NumPy-array2Tensor" class="headerlink" title="NumPy_array2Tensor"></a>NumPy_array2Tensor</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n = np.ones(<span class="number">5</span>)</span><br><span class="line">t = torch.from_numpy(n)</span><br></pre></td></tr></table></figure>

<p>NumPy 数组中的更改回反映在张量（tensor）中</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.add(n, <span class="number">1</span>, out = n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br><span class="line">n: [2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure>

<h2 id="数据集的相关操作"><a href="#数据集的相关操作" class="headerlink" title="数据集的相关操作"></a>数据集的相关操作</h2><p>从代码的架构设计上考虑，无论是出于可读性考虑还是出于代码逻辑的模块化管理考虑，我们都希望数据集代码与模型训练代码分离。</p>
<p>在数据预加载上，PyTorch 提供了两个功能函数 <code>torch.utils.data.DataLoader</code> 和 <code>torch.utils.data.Dataset</code> 分别读取预加载的数据和自己的数据。</p>
<p><code>Dataset</code> 存储样本和对应的标签，并在 <code>DataLoader</code> 范围内包装成一个可迭代对象 <code>Dataset</code> 以便轻松访问样本。</p>
<blockquote>
<p>在 PyTorch 函数库中预先提供好了很多可供预加载的数据集（例如：FashionMNIST），这些数据集借助 <code>torch.utils.data.Dataset</code> 子类化，并实现位于特定数据的函数。它们可用于对模型进行原型设计和基准测试。可以通过如下链接访问：<a target="_blank" rel="noopener" href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/vision/stable/datasets.html">Image Datasets</a>，<a target="_blank" rel="noopener" href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/text/stable/datasets.html">Text Datasets</a>，<a target="_blank" rel="noopener" href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/audio/stable/datasets.html">Audio Datasets</a></p>
</blockquote>
<h2 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h2><h2 id="构建模型-1"><a href="#构建模型-1" class="headerlink" title="构建模型"></a>构建模型</h2><h2 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h2><h2 id="优化循环"><a href="#优化循环" class="headerlink" title="优化循环"></a>优化循环</h2><h2 id="保存、加载和使用模型"><a href="#保存、加载和使用模型" class="headerlink" title="保存、加载和使用模型"></a>保存、加载和使用模型</h2>
      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://zade23.github.io/2023/10/31/pytorch-tutorial-official/" title="pytorch-tutorial-official" target="_blank" rel="external">https://zade23.github.io/2023/10/31/pytorch-tutorial-official/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/zade23" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/zade23" target="_blank"><span class="text-dark">Android</span><small class="ml-1x">Student &amp; Coder</small></a></h3>
        <div>Happy Coding!</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    
    <li class="next">
      <a href="/2023/10/23/Go%E8%AF%AD%E8%A8%80%E9%80%9F%E9%80%9A/" title="Go语言速通"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/zade23" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://weibo.com/u/5382156286" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: '',
    appKey: '',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script>

     







</body>
</html>