<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>pytorch-tutorial-official | ANdRoid&#39;s BLOG</title>
  <meta name="description" content="Pytorch 基础 快速入门 处理数据 构建模型 在 PyTorch 中构建神经网络 模型层 Model Layers nn.Flatten nn.Linear nn.ReLU nn.Sequential nn.Softmax 模型的参数       张量 Tensor 初始化张量 直接来自数据 来自另一个 Tensor 使用随机值或常量   张量的属性 张量的操作 类似 NumPy 的索引和">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch-tutorial-official">
<meta property="og:url" content="https://zade23.github.io/2023/10/31/pytorch-tutorial-official/index.html">
<meta property="og:site_name" content="ANdRoid&#39;s BLOG">
<meta property="og:description" content="Pytorch 基础 快速入门 处理数据 构建模型 在 PyTorch 中构建神经网络 模型层 Model Layers nn.Flatten nn.Linear nn.ReLU nn.Sequential nn.Softmax 模型的参数       张量 Tensor 初始化张量 直接来自数据 来自另一个 Tensor 使用随机值或常量   张量的属性 张量的操作 类似 NumPy 的索引和">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://z1.ax1x.com/2023/11/01/pin7WJf.md.jpg">
<meta property="og:image" content="https://z1.ax1x.com/2023/11/06/pilKoHf.png">
<meta property="og:image" content="https://z1.ax1x.com/2023/11/07/pilfxhV.png">
<meta property="og:image" content="https://pytorch.org/tutorials/_images/comp-graph.png">
<meta property="article:published_time" content="2023-10-31T08:49:36.000Z">
<meta property="article:modified_time" content="2024-03-14T03:02:15.673Z">
<meta property="article:author" content="Android">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://z1.ax1x.com/2023/11/01/pin7WJf.md.jpg">
  <!-- Canonical links -->
  <link rel="canonical" href="https://zade23.github.io/2023/10/31/pytorch-tutorial-official/index.html">
  
    <link rel="alternate" href="/atom.xml" title="ANdRoid&#39;s BLOG" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 6.3.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/zade23" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Android</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">Student &amp; Coder</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> GuangZhou, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">项目</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">关于</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/zade23" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://weibo.com/u/5382156286" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>别想那么多，去码头整点儿薯条吧</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deeplearning/">Deeplearning</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git-Workflow/">Git Workflow</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LLM/">LLM</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/whatever/">whatever</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/" rel="tag">C++</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/" rel="tag">Docker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyTorch/" rel="tag">PyTorch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Training/" rel="tag">Training</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformers/" rel="tag">Transformers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/conda/" rel="tag">conda</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/environment/" rel="tag">environment</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/" rel="tag">工具网站</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%B0%8F%E7%9F%A5%E8%AF%86/" rel="tag">经济学小知识</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/C/" style="font-size: 13px;">C++</a> <a href="/tags/Docker/" style="font-size: 13px;">Docker</a> <a href="/tags/Git/" style="font-size: 13.5px;">Git</a> <a href="/tags/Hexo/" style="font-size: 13px;">Hexo</a> <a href="/tags/PyTorch/" style="font-size: 13px;">PyTorch</a> <a href="/tags/Python/" style="font-size: 14px;">Python</a> <a href="/tags/Training/" style="font-size: 13px;">Training</a> <a href="/tags/Transformers/" style="font-size: 13px;">Transformers</a> <a href="/tags/conda/" style="font-size: 13px;">conda</a> <a href="/tags/environment/" style="font-size: 13px;">environment</a> <a href="/tags/%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/" style="font-size: 13px;">工具网站</a> <a href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%B0%8F%E7%9F%A5%E8%AF%86/" style="font-size: 13px;">经济学小知识</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">三月 2024</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Deeplearning/">Deeplearning</a>
              </p>
              <p class="item-title">
                <a href="/2024/03/21/Transformers-FlashBack/" class="title">Transformers_FlashBack</a>
              </p>
              <p class="item-date">
                <time datetime="2024-03-21T02:10:26.000Z" itemprop="datePublished">2024-03-21</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a>
              </p>
              <p class="item-title">
                <a href="/2024/03/20/%E3%80%8A%E5%81%A5%E5%A3%AE%E7%9A%84Python%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-Day1/" class="title">《健壮的Python》读书笔记-Day1</a>
              </p>
              <p class="item-date">
                <time datetime="2024-03-20T13:45:05.000Z" itemprop="datePublished">2024-03-20</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/LLM/">LLM</a>
              </p>
              <p class="item-title">
                <a href="/2024/03/19/LLaMA-Factory%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0/" class="title">LLaMA-Factory原理与底层实现</a>
              </p>
              <p class="item-date">
                <time datetime="2024-03-19T02:53:47.000Z" itemprop="datePublished">2024-03-19</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2024/03/13/Python%E6%A8%A1%E5%9D%97%E5%B0%81%E8%A3%85%E5%AF%BC%E5%85%A5%E5%92%8C%E5%8C%85%E7%9A%84%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/" class="title">Python模块封装导入和包的相关知识</a>
              </p>
              <p class="item-date">
                <time datetime="2024-03-13T06:12:24.000Z" itemprop="datePublished">2024-03-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/Git-Workflow/">Git Workflow</a>
              </p>
              <p class="item-title">
                <a href="/2024/03/11/%E5%90%88%E4%BD%9C%E5%BC%80%E5%8F%91%E4%B8%ADGit%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E7%9A%84%E7%BB%86%E8%8A%82/" class="title">合作开发中Git工作流程的细节</a>
              </p>
              <p class="item-date">
                <time datetime="2024-03-11T08:39:05.000Z" itemprop="datePublished">2024-03-11</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-pytorch-tutorial-official" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      pytorch-tutorial-official
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2023/10/31/pytorch-tutorial-official/" class="article-date">
	  <time datetime="2023-10-31T08:49:36.000Z" itemprop="datePublished">2023-10-31</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/Deeplearning/">Deeplearning</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/PyTorch/" rel="tag">PyTorch</a>
  </span>


        
	<span class="article-read hidden-xs">
	    <i class="icon icon-eye-fill" aria-hidden="true"></i>
	    <span id="busuanzi_container_page_pv">
			<span id="busuanzi_value_page_pv">0</span>
		</span>
	</span>


        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2023/10/31/pytorch-tutorial-official/#comments" class="article-comment-link">评论</a></span>
        
	
		<span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 7.7k(字)</span>
	
	
		<span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 34(分)</span>
	

      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <ul>
<li><a href="#pytorch-%E5%9F%BA%E7%A1%80">Pytorch 基础</a><ul>
<li><a href="#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8">快速入门</a><ul>
<li><a href="#%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE">处理数据</a></li>
<li><a href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B">构建模型</a><ul>
<li><a href="#%E5%9C%A8-pytorch-%E4%B8%AD%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">在 PyTorch 中构建神经网络</a><ul>
<li><a href="#%E6%A8%A1%E5%9E%8B%E5%B1%82-model-layers">模型层 Model Layers</a></li>
<li><a href="#nnflatten">nn.Flatten</a></li>
<li><a href="#nnlinear">nn.Linear</a></li>
<li><a href="#nnrelu">nn.ReLU</a></li>
<li><a href="#nnsequential">nn.Sequential</a></li>
<li><a href="#nnsoftmax">nn.Softmax</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0">模型的参数</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%BC%A0%E9%87%8F-tensor">张量 Tensor</a><ul>
<li><a href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%BC%A0%E9%87%8F">初始化张量</a><ul>
<li><a href="#%E7%9B%B4%E6%8E%A5%E6%9D%A5%E8%87%AA%E6%95%B0%E6%8D%AE">直接来自数据</a></li>
<li><a href="#%E6%9D%A5%E8%87%AA%E5%8F%A6%E4%B8%80%E4%B8%AA-tensor">来自另一个 Tensor</a></li>
<li><a href="#%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BA%E5%80%BC%E6%88%96%E5%B8%B8%E9%87%8F">使用随机值或常量</a></li>
</ul>
</li>
<li><a href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7">张量的属性</a></li>
<li><a href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E6%93%8D%E4%BD%9C">张量的操作</a><ul>
<li><a href="#%E7%B1%BB%E4%BC%BC-numpy-%E7%9A%84%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87%E6%93%8D%E4%BD%9C">类似 NumPy 的索引和切片操作</a></li>
<li><a href="#tensor-%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BF%9E%E6%8E%A5">Tensor 之间的连接</a></li>
<li><a href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97">张量的算术运算</a></li>
<li><a href="#%E5%8D%95%E4%B8%80%E5%85%83%E7%B4%A0%E5%BC%A0%E9%87%8F">单一元素张量</a></li>
<li><a href="#%E5%8E%9F%E5%9C%B0%E6%93%8D%E4%BD%9C%E4%B8%8D%E9%80%9A%E8%BF%87%E8%BF%94%E5%9B%9E%E7%9A%84%E6%96%B9%E5%BC%8F%E8%BF%9B%E8%A1%8C%E5%8E%9F%E5%9C%B0%E4%BF%AE%E6%94%B9">原地操作(不通过返回的方式进行原地修改)</a></li>
</ul>
</li>
<li><a href="#tensor-%E5%92%8C-numpy-%E4%BA%8C%E8%80%85%E8%BD%AC%E6%8D%A2">Tensor 和 Numpy 二者转换</a><ul>
<li><a href="#tensor2numpy_array">Tensor2NumPy_array</a></li>
<li><a href="#numpy_array2tensor">NumPy_array2Tensor</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C">数据集的相关操作</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD">数据加载</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%AD%E4%BB%A3%E5%92%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96">数据集迭代和数据可视化</a></li>
<li><a href="#%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86">创建自定义的数据集</a><ul>
<li><a href="#_init_">_<em>init</em>_</a></li>
<li><a href="#_len_">_<em>len</em>_</a></li>
<li><a href="#_getitem_">_<em>getitem</em>_</a></li>
</ul>
</li>
<li><a href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E5%B9%B6%E4%BD%BF%E7%94%A8-dataloader-%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83">准备数据并使用 DataLoader 进行训练</a></li>
<li><a href="#%E9%81%8D%E5%8E%86-dataloader">遍历 DataLoader</a></li>
</ul>
</li>
<li><a href="#%E8%BD%AC%E6%8D%A2">转换</a><ul>
<li><a href="#totensor">ToTensor()</a></li>
<li><a href="#lambda-%E8%BD%AC%E6%8D%A2">Lambda 转换</a></li>
</ul>
</li>
<li><a href="#%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86torchautograd">（核心内容）自动微分(<code>TORCH.AUTOGRAD</code>)</a><ul>
<li><a href="#%E5%BC%A0%E9%87%8F%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97%E5%9B%BE">张量、函数、计算图</a></li>
<li><a href="#%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97">梯度计算</a></li>
<li><a href="#%E7%A6%81%E7%94%A8%E6%A2%AF%E5%BA%A6%E8%BF%BD%E8%B8%AA">禁用梯度追踪</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E9%83%A8%E5%88%86%E6%89%A9%E5%B1%95%E9%98%85%E8%AF%BB">计算图部分扩展阅读</a></li>
<li><a href="#%E5%8F%AF%E9%80%89%E9%98%85%E8%AF%BB%E5%BC%A0%E9%87%8F%E6%A2%AF%E5%BA%A6-%E5%92%8C-%E9%9B%85%E9%98%81%E6%AF%94%E7%A7%AFjacobian_products">可选阅读：张量梯度 和 雅阁比积(Jacobian_Products)</a></li>
</ul>
</li>
<li><a href="#%E9%87%8D%E8%A6%81%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0">（重要）优化模型参数</a><ul>
<li><a href="#%E5%85%88%E5%86%B3%E6%9D%A1%E4%BB%B6%E4%BB%A3%E7%A0%81">先决条件代码</a></li>
<li><a href="#%E8%B6%85%E5%8F%82%E6%95%B0">超参数</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E5%BE%AA%E7%8E%AF">优化循环</a></li>
<li><a href="#%E5%85%A8%E8%BF%87%E7%A8%8B%E6%95%B0%E6%8D%AE%E8%B7%9F%E8%B8%AA">全过程数据跟踪</a></li>
</ul>
</li>
<li><a href="#%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B">保存、加载和使用模型</a><ul>
<li><a href="#%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D">保存&#x2F;加载模型权重</a></li>
<li><a href="#%E9%80%9A%E8%BF%87%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BD%A2%E7%8A%B6%E5%8F%82%E6%95%B0%E8%BF%9B%E8%A1%8C%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD">通过模型的形状参数进行保存&#x2F;加载</a></li>
</ul>
</li>
<li><a href="#%E5%9C%A8-pytorch-%E4%B8%AD%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E5%B8%B8%E8%A7%84-checkpoint">在 PyTorch 中保存和加载常规 Checkpoint</a><ul>
<li><a href="#%E4%BB%8B%E7%BB%8D">介绍</a></li>
<li><a href="#%E8%AE%BE%E7%BD%AE">设置</a></li>
<li><a href="#%E6%AD%A5%E9%AA%A4">步骤</a><ul>
<li><a href="#1%E5%AF%BC%E5%85%A5%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E6%89%80%E9%9C%80%E8%A6%81%E7%9A%84%E5%BA%93">1.导入加载数据所需要的库</a></li>
<li><a href="#2%E5%AE%9A%E4%B9%89%E5%92%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">2.定义和初始化神经网络</a></li>
<li><a href="#3%E5%88%9D%E5%A7%8B%E5%8C%96%E4%BC%98%E5%8C%96%E5%99%A8">3.初始化优化器</a></li>
<li><a href="#4%E4%BF%9D%E5%AD%98%E5%B8%B8%E8%A7%84%E6%A3%80%E6%9F%A5%E7%82%B9">4.保存常规检查点</a></li>
<li><a href="#5%E5%8A%A0%E8%BD%BD%E5%B8%B8%E8%A7%84%E6%A3%80%E6%9F%A5%E7%82%B9">5.加载常规检查点</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E4%BB%8E-checkpoint-%E4%B8%AD%E5%8A%A0%E8%BD%BD-nnmodule-%E7%9A%84%E6%8A%80%E5%B7%A7">从 Checkpoint 中加载 <code>nn.Module</code> 的技巧</a><ul>
<li><a href="#%E6%B4%BB%E7%94%A8-torchloadmmap--true">活用 <code>torch.load(mmap = True)</code></a></li>
<li><a href="#%E6%B4%BB%E7%94%A8-torchdevicemeta">活用 <code>torch.device(&quot;meta&quot;)</code></a></li>
<li><a href="#%E6%B4%BB%E7%94%A8-load_state_dictassign--true">活用 <code>load_state_dict(assign = True)</code></a></li>
<li><a href="#%E7%BB%93%E8%AE%BA">结论</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="Pytorch-基础"><a href="#Pytorch-基础" class="headerlink" title="Pytorch 基础"></a>Pytorch 基础</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/intro.html">https://pytorch.org/tutorials/beginner/basics/intro.html</a></p>
</blockquote>
<p>大多数机器学习工作流：</p>
<ul>
<li>处理数据</li>
<li>创建模型</li>
<li>优化模型参数</li>
<li>保存训练后模型</li>
</ul>
<p>通过 Pytorch 基础部分的内容，读者可以完整的走完一整个MachineLearning的工作流，若读者对其中某个环节不理解或感兴趣，针对这些工作流中的每一个环节都有相关的扩展阅读链接。</p>
<p>我们将使用 FashionMNIST 数据集训练一个神经网络，该神经网络预测输入图像是否属于一下类别之一：T恤&#x2F;上衣、裤子、套头衫、连衣裙、外套、凉鞋、成山、运动鞋、包包、靴子。（是个多分类任务）</p>
<h2 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html</a></p>
</blockquote>
<p>本节会快速走完一个机器学习多分类的Demo，以此快速了解流程中必要的基本ML相关API。</p>
<h3 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h3><p>Pytorch 中有两个用于处理数据的子库 <code>torch.utils.data.DataLoader</code> 和 <code>torch.utils.data.Dataset.Dataset</code>。顾名思义，<code>Dataset</code> 存储样本及其相应的标签，并将 <code>DataLoader</code> 可迭代对象包装在 <code>Dataset</code> 中。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvison.transforms <span class="keyword">import</span> ToTensor</span><br></pre></td></tr></table></figure>

<p>通过上面的引用(import)，我们可以发现：Pytorch 中有非常多的子库，这些子库专注于某一特定的领域，例如： <a target="_blank" rel="noopener" href="https://pytorch.org/text/stable/index.html">TorchText</a>, <a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/index.html">TorchVision</a>, 和 <a target="_blank" rel="noopener" href="https://pytorch.org/audio/stable/index.html">TorchAudio</a>, 这些所有子库中都包含相应的数据集。</p>
<p>本次教程中我们使用 <code>TorchVision</code> 数据集。</p>
<p>该 <code>torchvision,.datasets</code> 模块包含 <code>Dataset</code> 来自现实世界中的视觉图像数据，最经典的有：CIFAT，COCO(<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html">full list here</a>)</p>
<p>本次教程中我们使用 <code>FashionMNIST</code> 数据集。每个 <code>TorchVison</code> 下的 <code>Dataset</code> 都包含两个参数：<code>transform</code> 和 <code>target_transform</code> 分别用来<strong>修改样本</strong>与<strong>打标签</strong>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从公开数据集中读取训练数据</span></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">True</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = ToTensor(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从公开数据集中读取测试数据</span></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">False</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = Totensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>下图是在 colab 中运行上面程序块的输出结果：<br><a target="_blank" rel="noopener" href="https://imgse.com/i/pin7WJf"><img src="https://z1.ax1x.com/2023/11/01/pin7WJf.md.jpg" alt="pin7WJf.md.jpg"></a></p>
<p>至此为止，通过上面的工作，我们将 <code>Dataset</code> 作为参数传递给了 <code>DataLoader</code> 。同时封装了相关数据集作为一个可迭代的对象，支持自动批处理、采样、洗牌(shuffling)、和多进程的数据加载。</p>
<hr>
<p>下一步中，我们定义 <code>batch_size = 64</code> ，即 <code>dataloader</code> 可迭代中的每个元素都将返回一批含有64个特征的标签。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据读取工具</span></span><br><span class="line">train_dataloader = DataLoader(tarain_data, batch_size = batch_size)</span><br><span class="line">test_dataloader = DataLoader(tast_data, batch_size = batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of X [N, C, H, W]: <span class="subst">&#123;X.shape&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Shape of y []: <span class="subst">&#123;y.shape&#125;</span> <span class="subst">&#123;y.dtpye&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])</span><br><span class="line">Shape of y: torch.Size([64]) torch.int64</span><br></pre></td></tr></table></figure>

<blockquote>
<p>关于 <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">loading data in PyTorch</a> 的详细说明</p>
</blockquote>
<hr>
<h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><p>为了在 Pytorch 中定义神经网络，我们创建一个继承自  <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module</a> 的类。</p>
<p>我们通过 <code>__init__</code> 函数定义神经网络的层，并指明数据如何通过 <code>forward</code> 函数进入神经网络层。</p>
<blockquote>
<p>在设备允许的情况下，推荐使用GPU来加速神经网络的运算操作。</p>
</blockquote>
<p>代码实现：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取用于训练的设备（cpu/gpu/mps）</span></span><br><span class="line">device = (</span><br><span class="line">    <span class="string">&quot;cuda&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">else</span> <span class="string">&quot;mps&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.backends.mps.is_available()</span><br><span class="line">    <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Using <span class="subst">&#123;device&#125;</span> device&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义神经网络的模型结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<p>打印结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Using cpu device</span><br><span class="line">NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="在-PyTorch-中构建神经网络"><a href="#在-PyTorch-中构建神经网络" class="headerlink" title="在 PyTorch 中构建神经网络"></a>在 PyTorch 中构建神经网络</h4><blockquote>
<p>这一部分是对‘构建模型’部分的一点补充说明，也是 PyTorch 官网教程中的扩展阅读部分</p>
</blockquote>
<p>神经网络是由多个对数据进行操作的层&#x2F;模型组合而成的。<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">torch.nn</a>  命名空间几乎已经提供了构建一个神经网络所需要用到的<strong>所有模块</strong>。</p>
<p>所有模块都在 PyTorch 下的子块  <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module</a> 中提供。</p>
<p>基于这样的结构化嵌套模块，整个神经网络可以自由的进行构建和管理复杂的架构。</p>
<p>在上面的代码块中，我们通过 <code>NeuralNetwork</code> 函数定义了一个神经网络模型 <code>model</code>。</p>
<p>为了使用该模型，我们将输入数据传递给它。这个操作将执行 <code>forward</code> 操作和一些<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866">后台操作</a>。</p>
<p>请记住：不要直接使用 <code>model.forward()</code> !</p>
<hr>
<p>通过输入操作调用模型，最后将返回一个二维张量，其中 dim &#x3D; 0 对应于每个类别的 10 个原始预测输出，dim &#x3D; 1 对应与每个输出的单个值。</p>
<p>我们可以通过 <code>nn.Softmax</code> 模块实例传递对结果预测的概率来进行最终预测概率的判断。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device = device)</span><br><span class="line">logits = model(X)</span><br><span class="line">pred_probab = nn.Softmax(dim = <span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Predicted class: tensor([7], device = &#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>

<h5 id="模型层-Model-Layers"><a href="#模型层-Model-Layers" class="headerlink" title="模型层 Model Layers"></a>模型层 Model Layers</h5><p>分解 <code>FashionMNIST</code> 模型中的各层。为了说明这一点，我们通过获取一个包含 3 张大小为 28*28 的小批量图像样本，看看当数据传递到网络时会发生什么。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_image = torch.rand(<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(input_image.size())</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 28, 28])</span><br></pre></td></tr></table></figure>

<h5 id="nn-Flatten"><a href="#nn-Flatten" class="headerlink" title="nn.Flatten"></a>nn.Flatten</h5><p>初始化 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html">nn.Flatten</a> 层，将每个2D 28*28 图像转换成包含 784 个像素值的连续数组（保持小批量尺寸(dim &#x3D; 0)）</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line"><span class="built_in">print</span>(flat_image.size())</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 784])</span><br></pre></td></tr></table></figure>

<h5 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear"></a>nn.Linear</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">线性层模块</a>通过输入的权重w和偏差值b进行线性变换。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layer1 = nn.Linear(in_features = <span class="number">28</span>*<span class="number">28</span>, out_features = <span class="number">20</span>)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line"><span class="built_in">print</span>(hidden1.size())</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 20])</span><br></pre></td></tr></table></figure>

<h5 id="nn-ReLU"><a href="#nn-ReLU" class="headerlink" title="nn.ReLU"></a>nn.ReLU</h5><p>非线性激活函数可以在模型的输入输出之间创建复杂的映射关系。激活函数通过引入非线性的变换帮助神经网络学习各种现象。</p>
<p>在实例模型中，我们在线性层之间使用ReLU激活函数。但还有其他激活函数可以在模型的线性层中间作为激活函数使用，详情参考：<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-cn/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">激活函数-wiki</a></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Before ReLU: <span class="subst">&#123;hidden1&#125;</span>\n\n&quot;</span>)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Aftr RelU: <span class="subst">&#123;hidden1&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Before ReLU: tensor([[ 0.4158, -0.0130, -0.1144,  0.3960,  0.1476, -0.0690, -0.0269,  0.2690,</span><br><span class="line">          0.1353,  0.1975,  0.4484,  0.0753,  0.4455,  0.5321, -0.1692,  0.4504,</span><br><span class="line">          0.2476, -0.1787, -0.2754,  0.2462],</span><br><span class="line">        [ 0.2326,  0.0623, -0.2984,  0.2878,  0.2767, -0.5434, -0.5051,  0.4339,</span><br><span class="line">          0.0302,  0.1634,  0.5649, -0.0055,  0.2025,  0.4473, -0.2333,  0.6611,</span><br><span class="line">          0.1883, -0.1250,  0.0820,  0.2778],</span><br><span class="line">        [ 0.3325,  0.2654,  0.1091,  0.0651,  0.3425, -0.3880, -0.0152,  0.2298,</span><br><span class="line">          0.3872,  0.0342,  0.8503,  0.0937,  0.1796,  0.5007, -0.1897,  0.4030,</span><br><span class="line">          0.1189, -0.3237,  0.2048,  0.4343]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">After ReLU: tensor([[0.4158, 0.0000, 0.0000, 0.3960, 0.1476, 0.0000, 0.0000, 0.2690, 0.1353,</span><br><span class="line">         0.1975, 0.4484, 0.0753, 0.4455, 0.5321, 0.0000, 0.4504, 0.2476, 0.0000,</span><br><span class="line">         0.0000, 0.2462],</span><br><span class="line">        [0.2326, 0.0623, 0.0000, 0.2878, 0.2767, 0.0000, 0.0000, 0.4339, 0.0302,</span><br><span class="line">         0.1634, 0.5649, 0.0000, 0.2025, 0.4473, 0.0000, 0.6611, 0.1883, 0.0000,</span><br><span class="line">         0.0820, 0.2778],</span><br><span class="line">        [0.3325, 0.2654, 0.1091, 0.0651, 0.3425, 0.0000, 0.0000, 0.2298, 0.3872,</span><br><span class="line">         0.0342, 0.8503, 0.0937, 0.1796, 0.5007, 0.0000, 0.4030, 0.1189, 0.0000,</span><br><span class="line">         0.2048, 0.4343]], grad_fn=&lt;ReluBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h5 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h5><p>nn.Sequential是一个有序的模块容器。数据按照定义好的方式顺序的通过当前模块。<br>您可以使用顺序容器来组合一个“快捷网络” ，例如：<code>seq_modules</code> .</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(<span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">logits = seq_modules(input_image)</span><br></pre></td></tr></table></figure>

<h5 id="nn-Softmax"><a href="#nn-Softmax" class="headerlink" title="nn.Softmax"></a>nn.Softmax</h5><p>Softmax激活函数通常用在最后一个线性层，用来返回对数区间介于 [-infty, infty] 中的原始值，这些值最终被传递给 <code>nn.Softmax</code> 模块。</p>
<p>Softmax 激活函数将对应输出区间范围缩放在 [0, 1] 之间，<strong>表示模型对每个类别的预测概率</strong>。其中，<code>dim</code> 中所有参数指示值求和应该为 1 。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">softmax = nn.Softmax(dim = <span class="number">1</span>)</span><br><span class="line">pred_probab = softmax(logits)</span><br></pre></td></tr></table></figure>

<h5 id="模型的参数"><a href="#模型的参数" class="headerlink" title="模型的参数"></a>模型的参数</h5><p>神经网络往往非常的复杂，在整个网络的构建过程中，如果可以便捷的将每个部分表示出来，对于训练过程中的优化和修改相对的权重与偏差等都会有非常大的帮助。</p>
<p>子类化 <code>nn.Module</code> 模块可以帮助我们解决这个问题，该模块会自动跟踪模型对象中定义的所有字段，并使用模型的 <code>parameters()</code> 函数或 <code>named_parameters()</code> 函数方法访问所有参数。</p>
<p>在本次示例中，我们遍历每个参数，并预览它们的所有数值参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model structure: <span class="subst">&#123;model&#125;</span>\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span> | Size: <span class="subst">&#123;param.size()&#125;</span> | Values : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Model structure: NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],</span><br><span class="line">        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0155, -0.0327], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0116,  0.0293, -0.0280,  ...,  0.0334, -0.0078,  0.0298],</span><br><span class="line">        [ 0.0095,  0.0038,  0.0009,  ..., -0.0365, -0.0011, -0.0221]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0148, -0.0256], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0147, -0.0229,  0.0180,  ..., -0.0013,  0.0177,  0.0070],</span><br><span class="line">        [-0.0202, -0.0417, -0.0279,  ..., -0.0441,  0.0185, -0.0268]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0070, -0.0411], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h2 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量 Tensor"></a>张量 Tensor</h2><p>通过一张图初步了解常见的多维空间数据的命名方式（来源：<a target="_blank" rel="noopener" href="https://youtu.be/ORMx45xqWkA?si=Njw0z1RULmPeeit9&t=17">PyTorch in 100 Seconds</a>）<br><a target="_blank" rel="noopener" href="https://imgse.com/i/pilKoHf"><img src="https://z1.ax1x.com/2023/11/06/pilKoHf.png" alt="不同维度参数的命名方式"></a></p>
<p>Tensor 又称 张量，是一种专门的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量对比模型的输入和输出以及模型的参数进行编码。</p>
<p>Tensors 类似于 NumPy 中的ndarrays，不同之处在于张量可以在 GPU 或其他硬件加速器上运行。事实上，张量和 NumPy 数组通常可以共享相同的底层内存，从而消除了复制数据的需要。</p>
<p>张量也针对自动微分进行了优化。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h3 id="初始化张量"><a href="#初始化张量" class="headerlink" title="初始化张量"></a>初始化张量</h3><p>一般可以通过如下方式初始化 <code>Tensor</code>:</p>
<ul>
<li>直接通过数据创建</li>
<li>通过NumPy创建</li>
<li>通过继承另一个Tensor的形状和数据类型</li>
<li>使用随机值或常量</li>
</ul>
<p>下面分别进行介绍：</p>
<h4 id="直接来自数据"><a href="#直接来自数据" class="headerlink" title="直接来自数据"></a>直接来自数据</h4><p>张量可以直接从已有的数据中创建，数据类型是自动推断的。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure>

<h4 id="来自另一个-Tensor"><a href="#来自另一个-Tensor" class="headerlink" title="来自另一个 Tensor"></a>来自另一个 Tensor</h4><p>新建的张量保留参考张量的部分参数（<strong>形状，数据类型</strong>），除非用显式的方式直接覆盖。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># retains the properties of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line">x_rand = torch.rand_like(x_data, dtype = torch.<span class="built_in">float</span>) <span class="comment"># overrides the datatype of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[1, 1],</span><br><span class="line">        [1, 1]])</span><br><span class="line"></span><br><span class="line">Random Tensor:</span><br><span class="line"> tensor([[0.8823, 0.9150],</span><br><span class="line">        [0.3829, 0.9593]])</span><br></pre></td></tr></table></figure>

<h4 id="使用随机值或常量"><a href="#使用随机值或常量" class="headerlink" title="使用随机值或常量"></a>使用随机值或常量</h4><p><code>shape</code> 是张量维度的元组表达式。在下面的函数中，它决定了输出张量的维度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 Tensor 的维度</span></span><br><span class="line">shape = (<span class="number">2</span>, <span class="number">3</span>, )</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones_Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Zeros_Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Random Tensor:</span><br><span class="line"> tensor([[0.3904, 0.6009, 0.2566],</span><br><span class="line">        [0.7936, 0.9408, 0.1332]])</span><br><span class="line"></span><br><span class="line">Ones Tensor:</span><br><span class="line"> tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]])</span><br><span class="line"></span><br><span class="line">Zeros Tensor:</span><br><span class="line"> tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure>

<h3 id="张量的属性"><a href="#张量的属性" class="headerlink" title="张量的属性"></a>张量的属性</h3><p>Tensor 的属性描述了<u><strong>它们的形状</strong></u>、<u><strong>数据类型</strong></u> 和 <u><strong>存储它们的设备</strong></u>。</p>
<ul>
<li><code>tensor.shape</code></li>
<li><code>tensor.dtype</code></li>
<li><code>tensor.device</code></li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Dtype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device of tensor: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Shape of tensor: torch.Size([3, 4])</span><br><span class="line">Datatype of tensor: torch.float32</span><br><span class="line">Device tensor is stored on: cpu</span><br></pre></td></tr></table></figure>

<h3 id="张量的操作"><a href="#张量的操作" class="headerlink" title="张量的操作"></a>张量的操作</h3><p>科学计算是深度学习领域的根本！PyTorch提供了 100+ 张量运算操作，包括算术运算、线性代数运算、矩阵运算（转职、索引、切片）、采样等。</p>
<p><u><strong><code>PyTorch</code> 中的所有逻辑运算都可以通过GPU进行加速运算</strong></u></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将运行设备选择为 GPU （如果你有的话）</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tensor = tensor.to(<span class="string">&quot;cuda&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="类似-NumPy-的索引和切片操作"><a href="#类似-NumPy-的索引和切片操作" class="headerlink" title="类似 NumPy 的索引和切片操作"></a>类似 NumPy 的索引和切片操作</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Tensor= torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First row: <span class="subst">&#123;tensor[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First column: <span class="subst">&#123;tensor[:, <span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;last column: <span class="subst">&#123;tensor[..., -<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line">tensor[:, <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">First row: tensor([1., 1., 1., 1.])</span><br><span class="line">First column: tensor([1., 1., 1., 1])</span><br><span class="line">Last column: tensor([1., 1., 1., 1])</span><br><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure>

<h4 id="Tensor-之间的连接"><a href="#Tensor-之间的连接" class="headerlink" title="Tensor 之间的连接"></a>Tensor 之间的连接</h4><p><code>torch.cat</code> 可以用于连接指定维度的张量，拥有同样功能的另一个算子是 <code>torch.stack_</code> （参考：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.stack.html">torch.stack_</a>）。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim = <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure>

<h4 id="张量的算术运算"><a href="#张量的算术运算" class="headerlink" title="张量的算术运算"></a>张量的算术运算</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算两个张量之间的矩阵乘法。其中，y1, y2, y3 拥有相同的参数值</span></span><br><span class="line"><span class="comment"># `tensor.T` 返回张量的转置</span></span><br><span class="line">y1 = tensor @ tensor.T</span><br><span class="line">y2 = tensor.matmul(tensor.T)</span><br><span class="line"></span><br><span class="line">y3 = torch.rand_like(y1)</span><br><span class="line">torch.matmul(tensor, tensor.T, out = y3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将张量中的元素逐个相乘。z1, z2, z3 具有相同的值</span></span><br><span class="line">z1 = tensor * tensor</span><br><span class="line">z2 = tensor.mul(tensor)</span><br><span class="line"></span><br><span class="line">z3 = torch.rand_like(tensor)</span><br><span class="line">torch.mul(tensor, tensor, out = z3)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1],</span><br><span class="line">        [1., 0., 1., 1],</span><br><span class="line">        [1., 0., 1., 1],</span><br><span class="line">        [1., 0., 1., 1]])</span><br></pre></td></tr></table></figure>

<h4 id="单一元素张量"><a href="#单一元素张量" class="headerlink" title="单一元素张量"></a>单一元素张量</h4><p>如果你有一个单一元素的张量，例如希望通过将张量的所有值聚合为一个值，那么可以使用 <code>item()</code> 将其转换为 python 数值：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">agg = tensor.<span class="built_in">sum</span>()</span><br><span class="line">agg_item = agg.item()</span><br><span class="line"><span class="built_in">print</span>(agg_item, <span class="built_in">type</span>(agg_item))</span><br></pre></td></tr></table></figure>

<p>打印输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12.0 &lt;class &#x27;float&#x27;&gt;</span><br></pre></td></tr></table></figure>

<h4 id="原地操作-不通过返回的方式进行原地修改"><a href="#原地操作-不通过返回的方式进行原地修改" class="headerlink" title="原地操作(不通过返回的方式进行原地修改)"></a>原地操作(不通过返回的方式进行原地修改)</h4><p>将结果存储在操作数中的操作成为原地操作。它们由 <code>_</code> 后缀表示。例如：<code>x.copy()</code> 、 <code>x.t_()</code> ，都将直接修改 <code>x</code>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;tensor&#125;</span> \n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br><span class="line"></span><br><span class="line">tensor([[6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.],</span><br><span class="line">        [6., 5., 6., 6.]])</span><br></pre></td></tr></table></figure>

<h3 id="Tensor-和-Numpy-二者转换"><a href="#Tensor-和-Numpy-二者转换" class="headerlink" title="Tensor 和 Numpy 二者转换"></a>Tensor 和 Numpy 二者转换</h3><p>位于 CPU 位置上的 Numpy 数组与 Tensor 可以共享同一个底层的内容空间，更改一个 tensor 会同时修改另一个 tensor 。</p>
<h4 id="Tensor2NumPy-array"><a href="#Tensor2NumPy-array" class="headerlink" title="Tensor2NumPy_array"></a>Tensor2NumPy_array</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印对比结果（t 代表 tensor；n 代表 numpy）:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([1., 1., 1., 1., 1.])</span><br><span class="line">n: [1. 1. 1. 1. 1.]</span><br></pre></td></tr></table></figure>

<p>下一步，改变 tensor 中的值，同时观察 NumPy 中值的变化：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: (n)&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印对比结果（t 代表 tensor；n 代表 numpy）:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([2., 2., 2., 2., 2.])</span><br><span class="line">n: [2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure>

<h4 id="NumPy-array2Tensor"><a href="#NumPy-array2Tensor" class="headerlink" title="NumPy_array2Tensor"></a>NumPy_array2Tensor</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n = np.ones(<span class="number">5</span>)</span><br><span class="line">t = torch.from_numpy(n)</span><br></pre></td></tr></table></figure>

<p>NumPy 数组中的更改回反映在张量（tensor）中</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.add(n, <span class="number">1</span>, out = n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span><br><span class="line">n: [2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure>

<h2 id="数据集的相关操作"><a href="#数据集的相关操作" class="headerlink" title="数据集的相关操作"></a>数据集的相关操作</h2><p>从代码的架构设计上考虑，无论是出于可读性考虑还是出于代码逻辑的模块化管理考虑，我们都希望数据集代码与模型训练代码分离。</p>
<p>在数据预加载上，PyTorch 提供了两个功能函数 <code>torch.utils.data.DataLoader</code> 和 <code>torch.utils.data.Dataset</code> 分别读取预加载的数据和自己的数据。</p>
<p><code>Dataset</code> 存储样本和对应的标签，并在 <code>DataLoader</code> 范围内包装成一个可迭代对象 <code>Dataset</code> 以便轻松访问样本。</p>
<blockquote>
<p>在 PyTorch 函数库中预先提供好了很多可供预加载的数据集（例如：FashionMNIST），这些数据集借助 <code>torch.utils.data.Dataset</code> 子类化，并实现位于特定数据的函数。它们可用于对模型进行原型设计和基准测试。可以通过如下链接访问：<a target="_blank" rel="noopener" href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/vision/stable/datasets.html">Image Datasets</a>，<a target="_blank" rel="noopener" href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/text/stable/datasets.html">Text Datasets</a>，<a target="_blank" rel="noopener" href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/audio/stable/datasets.html">Audio Datasets</a></p>
</blockquote>
<h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><p>下面例子是从 <code>TorchVision</code> 加载 <a target="_blank" rel="noopener" href="https://colab.research.google.com/corgiredirector?site=https://research.zalando.com/project/fashion_mnist/fashion_mnist/">Fashion-MNIST</a> 数据集的示例。Fashion-MNIST 由 60000 个训练样本和 10000 个测试样本组成。每个示例都包含一个 28*28 灰度图像和一个来自 10 个类之一的关联标签。</p>
<p>我们通过如下几个参数，对 <a target="_blank" rel="noopener" href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/vision/stable/datasets.html%23fashion-mnist">FashionMNIST Dataset</a> 数据集进行加载：</p>
<ul>
<li><code>root</code> 是存储训练&#x2F;测试数据的根目录</li>
<li><code>train</code> 指定训练或测试数据集</li>
<li><code>download = TRUE</code> 允许从互联网上搜索并下载数据集，前提是 root 路径下的数据集文件不存在</li>
<li><code>transform</code> 和 <code>target_transform</code> 分别执行 <em>标定特征</em> 和 <em>标注转换</em></li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">True</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">False</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = ToTensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="数据集迭代和数据可视化"><a href="#数据集迭代和数据可视化" class="headerlink" title="数据集迭代和数据可视化"></a>数据集迭代和数据可视化</h3><p>使用 <code>Datasets</code> ，我们可以实现像 Python 中列表那样的手动索引 <code>training_data[index]</code>。</p>
<p>使用 <code>matplotlib</code> 可视化训练数据集中的样本进行展示。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">labels_map = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;T-Shirt&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;Trouser&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;Pullover&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;Dress&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;Coat&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;Sandal&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;Shirt&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;Sneaker&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;Bag&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;Ankle Boot&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">figure = plt.figure(figsize = (<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">cols, rows = <span class="number">4</span>, <span class="number">4</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, cols * rows + <span class="number">1</span>):</span><br><span class="line">    sample_idx = torch.randint(<span class="built_in">len</span>(training_data), size(<span class="number">1</span>,)).item()</span><br><span class="line">    image, lable = training_data[sample_idx]</span><br><span class="line">    figure.add_subplot(rows, cols, i)</span><br><span class="line">    plt.title(lables_map[label])</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(img.squeeze(), cmap = <span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()  <span class="comment"># 打印出来的输出为数据集图像示例展示</span></span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pilfxhV"><img src="https://z1.ax1x.com/2023/11/07/pilfxhV.png" alt="pilfxhV.png"></a></p>
<h3 id="创建自定义的数据集"><a href="#创建自定义的数据集" class="headerlink" title="创建自定义的数据集"></a>创建自定义的数据集</h3><p>自定义数据集必须含有三个函数：</p>
<ul>
<li><code>__init__</code></li>
<li><code>__len__</code></li>
<li><code>__gititem__</code></li>
</ul>
<p>通过下面这个示例实现了解相关函数的使用方法。</p>
<p>FashionMNIST 图像存储在目录 <code>img_dir</code>，其自身的标签单独存储在CSV文件 <code>annotations_file</code> 中</p>
<p>先看代码块部分：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torchvision.io <span class="keyword">as</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomImageDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, annotations_file, img_dir, transform = <span class="literal">None</span>, target_transform = <span class="literal">None</span></span>):</span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self</span>):</span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure>

<h4 id="init"><a href="#init" class="headerlink" title="_init_"></a>_<em>init</em>_</h4><p>init 作为初始化函数，在实例化 Dataset 对象时运行一次。</p>
<p>我们初始化包含<strong>图像</strong>、<strong>注释文件</strong>和<strong>两个转换的目录</strong></p>
<p>labels.csv 文件的展示效果如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tshirt1.jpg, 0</span><br><span class="line">tshirt2.jpg, 0</span><br><span class="line">......</span><br><span class="line">ankleboot999.jpg, 9</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, annotations_file, img_dir, transform = <span class="literal">None</span>, target_transform = <span class="literal">None</span></span>):</span><br><span class="line">    self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">    self.img_dir = img_dir</span><br><span class="line">    self.transform = transform</span><br><span class="line">    self.target_transform = target_transform</span><br></pre></td></tr></table></figure>

<h4 id="len"><a href="#len" class="headerlink" title="_len_"></a>_<em>len</em>_</h4><p>len 函数返回数据集中的样本数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br></pre></td></tr></table></figure>

<h4 id="getitem"><a href="#getitem" class="headerlink" title="_getitem_"></a>_<em>getitem</em>_</h4><p>getitem 函数从给定索引目录的 <code>idx</code> 处返回数据集中的样本。</p>
<p>根据索引，它识别图像在磁盘上的位置，使用 <code>read_image</code>，从 csv 数据中检索相应的标签<code>self.img_labels</code>，调用它们的转换函数（前提是支持转换），并在元组中返回张量图像和相应的标签。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self</span>):</span><br><span class="line">    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">    image = read_image(img_path)</span><br><span class="line">    label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> self.transform:</span><br><span class="line">        image = self.transform(image)</span><br><span class="line">    <span class="keyword">if</span> self.target_transform:</span><br><span class="line">        label = self.target_transform(label)</span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure>

<h3 id="准备数据并使用-DataLoader-进行训练"><a href="#准备数据并使用-DataLoader-进行训练" class="headerlink" title="准备数据并使用 DataLoader 进行训练"></a>准备数据并使用 DataLoader 进行训练</h3><p>检索 Dataset 数据集的特征，并一次标记一个样本。</p>
<p>在训练模型的过程中，我们通常希望通过“小批量”的方式传递样本，在新一轮 epoch 下数据 reshuffle(洗牌) 减少模型过拟合，并使用 Python 中的 <code>multiprocessing</code> 函数来加快数据检索的速度。</p>
<p><code>DataLoader</code> 是一个可迭代的对象，它通过一个简单的 API 为我们抽象了这种复杂性。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size = <span class="number">64</span>, shuffle = <span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size = <span class="number">64</span>, shuffle = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="遍历-DataLoader"><a href="#遍历-DataLoader" class="headerlink" title="遍历 DataLoader"></a>遍历 DataLoader</h3><p>通过上面的步骤，我们已经将数据集加载到了 <code>DataLoader</code> 并可以根据我们的需要来遍历该数据集。</p>
<p>后续程序中每一次迭代都会返回一批 <code>train_features</code> 和 <code>train_labels</code>，每批结果中都包含 <code>batch_size = 64</code> 特征和标签。</p>
<p>在上面代码块中，我们指定了 shuffle &#x3D; True ，在我们遍历了所有批次后，数据会被洗牌（目的是为了更细粒度地控制数据加载顺序，可参考<a target="_blank" rel="noopener" href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/docs/stable/data.html%23data-loading-order-and-sampler">Samplers</a>）</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Display image and label.</span></span><br><span class="line">train_features, train_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Feature batch shape: <span class="subst">&#123;train_features.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Labels batch shape: <span class="subst">&#123;train_labels.size()&#125;</span>&quot;</span>)</span><br><span class="line">img = train_features[<span class="number">0</span>].squeeze()</span><br><span class="line">label = train_labels[<span class="number">0</span>]</span><br><span class="line">plt.imshow(img, cmap = <span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()  <span class="comment"># 输出数据集中的图片</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h2><p>数据的格式不总是按照训练机器学习算法所需要的格式出现的，因此我们需要通过<u><em><strong>转换</strong></em></u>来对数据进行一系列操作使得其适合于机器学习任务。</p>
<p>所有 <code>TorchVision</code> 数据集都具有两个参数：</p>
<ul>
<li><code>transform</code> 用于修改标签</li>
<li><code>target_transform</code> 接受包含转换逻辑的可调用对象</li>
</ul>
<p>在 <a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/transforms.html">torchvision.transforms</a> 中提供了几个开箱即用的转换格式。</p>
<p>FashionMNIST 特征采用 PIL Image 格式，标签为整数。</p>
<p>在训练任务开始前，我们需要将特征处理为归一化之后的张量。</p>
<p>为了进行这些转换，需要使用 <code>ToTensor</code> 和 <code>Lambda</code> 函数方法。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root = <span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train = <span class="literal">True</span>,</span><br><span class="line">    download = <span class="literal">True</span>,</span><br><span class="line">    transform = ToTensor(),</span><br><span class="line">    target_transform = Lambda(<span class="keyword">lambda</span> y: torch.zero(<span class="number">10</span>, dtype = torch.<span class="built_in">float</span>).scatter_(<span class="number">0</span>, torch.tensor(y), value = <span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz</span><br><span class="line"></span><br><span class="line">  0%|          | 0/26421880 [00:00&lt;?, ?it/s]</span><br><span class="line">  0%|          | 65536/26421880 [00:00&lt;01:12, 362364.70it/s]</span><br><span class="line">  1%|          | 229376/26421880 [00:00&lt;00:38, 680532.51it/s]</span><br><span class="line">  3%|2         | 786432/26421880 [00:00&lt;00:11, 2194389.90it/s]</span><br><span class="line">  7%|7         | 1933312/26421880 [00:00&lt;00:05, 4185622.75it/s]</span><br><span class="line"> 17%|#6        | 4423680/26421880 [00:00&lt;00:02, 9599067.02it/s]</span><br><span class="line"> 25%|##5       | 6717440/26421880 [00:00&lt;00:01, 11175748.57it/s]</span><br><span class="line"> 34%|###4      | 9109504/26421880 [00:01&lt;00:01, 14174360.51it/s]</span><br><span class="line"> 44%|####3     | 11567104/26421880 [00:01&lt;00:01, 14358310.56it/s]</span><br><span class="line"> 53%|#####2    | 13959168/26421880 [00:01&lt;00:00, 16463421.66it/s]</span><br><span class="line"> 62%|######2   | 16449536/26421880 [00:01&lt;00:00, 15864345.49it/s]</span><br><span class="line"> 71%|#######1  | 18776064/26421880 [00:01&lt;00:00, 17449238.29it/s]</span><br><span class="line"> 81%|########  | 21397504/26421880 [00:01&lt;00:00, 16758523.84it/s]</span><br><span class="line"> 90%|########9 | 23691264/26421880 [00:01&lt;00:00, 18055860.39it/s]</span><br><span class="line">100%|##########| 26421880/26421880 [00:01&lt;00:00, 13728491.83it/s]</span><br><span class="line">Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw</span><br><span class="line"></span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz</span><br><span class="line"></span><br><span class="line">  0%|          | 0/29515 [00:00&lt;?, ?it/s]</span><br><span class="line">100%|##########| 29515/29515 [00:00&lt;00:00, 327895.25it/s]</span><br><span class="line">Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw</span><br><span class="line"></span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz</span><br><span class="line"></span><br><span class="line">  0%|          | 0/4422102 [00:00&lt;?, ?it/s]</span><br><span class="line">  1%|1         | 65536/4422102 [00:00&lt;00:11, 363267.41it/s]</span><br><span class="line">  5%|5         | 229376/4422102 [00:00&lt;00:06, 683985.17it/s]</span><br><span class="line"> 19%|#8        | 819200/4422102 [00:00&lt;00:01, 2304448.10it/s]</span><br><span class="line"> 33%|###3      | 1474560/4422102 [00:00&lt;00:00, 2999709.36it/s]</span><br><span class="line"> 83%|########2 | 3670016/4422102 [00:00&lt;00:00, 7976134.77it/s]</span><br><span class="line">100%|##########| 4422102/4422102 [00:00&lt;00:00, 5985529.02it/s]</span><br><span class="line">Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw</span><br><span class="line"></span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz</span><br><span class="line">Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz</span><br><span class="line"></span><br><span class="line">  0%|          | 0/5148 [00:00&lt;?, ?it/s]</span><br><span class="line">100%|##########| 5148/5148 [00:00&lt;00:00, 39473998.16it/s]</span><br><span class="line">Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw</span><br></pre></td></tr></table></figure>

<h3 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor()"></a>ToTensor()</h3><p><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor">ToTensor</a> 将 PIL 图像 或 NumPy <code>ndarray</code> 转换为 <code>FloatTensor</code>. 并在[0., 1.] 范围内缩放图像的像素空间。</p>
<h3 id="Lambda-转换"><a href="#Lambda-转换" class="headerlink" title="Lambda 转换"></a>Lambda 转换</h3><p>Lambda 函数允许任意用户定义 lambda 函数。在这里，我们定义了一个函数，将整数转换为 one-hot 编码的张量。</p>
<p>Lambda首先创建一个大小为10（我们数据集中的标签数量）的零向量，并调用 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html">scatter_</a> 函数在索引 <code>y</code> 上分配标签 <code>value = 1</code>。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">target_transform = Lambda(<span class="keyword">lambda</span> y:torch.zeros(</span><br><span class="line">    <span class="number">10</span>, dtype = torch.<span class="built_in">float</span>).scatter_(dim = <span class="number">0</span>, index = torch.tensor(y), value = <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>阅读延伸：<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/transforms.html">torchvision.transforms API</a></p>
</blockquote>
<h2 id="（核心内容）自动微分-TORCH-AUTOGRAD"><a href="#（核心内容）自动微分-TORCH-AUTOGRAD" class="headerlink" title="（核心内容）自动微分(TORCH.AUTOGRAD)"></a>（核心内容）自动微分(<code>TORCH.AUTOGRAD</code>)</h2><p>在训练神经网络时，常用的算法是<strong>反向传播</strong>。在该算法中，参数（模型权重）根据损失函数相对于给定参数的<strong>梯度</strong>进行调整。</p>
<p>为了计算这些梯度，PyTorch 有一个内置的用于自动计算微分方程的引擎，称为 <code>torch.autograd</code> .</p>
<p>它支持任何计算图的梯度自动运算。</p>
<p>下面距离一个最简单的单层神经网络，其中包含输入 <code>x</code>、参数 <code>w</code> 和 <code>b</code>，以及一些损失函数。可以在 PyTorch 中按以下方式定义它：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">5</span>) <span class="comment"># input tensor</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>) <span class="comment"># expected output</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">z = torch.matmul(x, w) + b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) </span><br></pre></td></tr></table></figure>

<h3 id="张量、函数、计算图"><a href="#张量、函数、计算图" class="headerlink" title="张量、函数、计算图"></a>张量、函数、计算图</h3><p>上一小节的代码框中实现的损失函数计算流程如下图所示：</p>
<p><img src="https://pytorch.org/tutorials/_images/comp-graph.png" alt="计算损失函数"></p>
<p>在上面网络中，<code>w</code> 和 <code>b</code> 是我们需要优化的参数。因此，我们需要能够计算这些变量损失函数的梯度。</p>
<p>为了做到这些点，我们设置了这些张量的 <code>requires_grad</code> 函数来定义其属性。</p>
<blockquote>
<p>设置张量的值有两种方式：一种是在生成张量的时候使用 <code>requires_grad</code> 进行初始化设置；另一种是在后续使用 <code>x.requires_grad_(True)</code> 函数。</p>
</blockquote>
<p>用来构造计算图的函数实际上是类 <code>Function</code> 的对象。该对象指导如何计算正向函数，以及如何在反向传播步骤中计算其导数。对向后传播函数的引起存储在张量的属性中的 <code>grad_fn</code>。您可以在PyTorch的<a target="_blank" rel="noopener" href="https://colab.research.google.com/corgiredirector?site=https://pytorch.org/docs/stable/autograd.html%23function">官方文档</a>中找到更多信息 <code>Function</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient function for z = <span class="subst">&#123;z.grad_fn&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Gradient function for loss = <span class="subst">&#123;loss.grad_fn&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Gradient function for z = &lt;AddBackward0 object at 0x7d800ac85840&gt;</span><br><span class="line">Gradient function for loss = &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x7d800ac85ea0&gt;</span><br></pre></td></tr></table></figure>

<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><p>为了优化神经网络中的参数权重，我们需要计算损失函数相对于参数的导数，即我们需要在固定 <code>x</code> 和 <code>y</code> 值的情况下，求出 $\frac{\partial loss}{\partial w}$ 和 $\frac{\partial loss}{\partial b}$。</p>
<p>为了求出上面的导数，我们需要调用函数 <code>loss.backward()</code>，然后从 <code>w.grad</code> 和 <code>b.grad</code> 中检索权重和偏置的数值。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.3287, 0.0101, 0.0988],</span><br><span class="line">        [0.3287, 0.0101, 0.0988],</span><br><span class="line">        [0.3287, 0.0101, 0.0988],</span><br><span class="line">        [0.3287, 0.0101, 0.0988],</span><br><span class="line">        [0.3287, 0.0101, 0.0988]])</span><br><span class="line">tensor([0.3287, 0.0101, 0.0988])</span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li><p>可以通过将参数 <code>requires_grad</code> 的 <code>grad</code> 属性设置为 <code>True</code> 来或测计算图的叶节点属性。对于计算图中的所有其他节点，梯度将不可用。</p>
</li>
<li><p>PyTorch考虑性能上的原因，在给定图形上只能调用 <code>backward</code> 调用一次梯度计算。如果我们需要再同一个图上执行多个 <code>backward</code> 调用，我们需要参数传递给 <code>retain_graph = True</code>，再调用 <code>backward</code> 。</p>
</li>
</ul>
</blockquote>
<h3 id="禁用梯度追踪"><a href="#禁用梯度追踪" class="headerlink" title="禁用梯度追踪"></a>禁用梯度追踪</h3><p>默认情况下，所有张量都在 <code>require_grad = True</code> 跟踪其计算历史并支持梯度计算。但是，在某些情况下，只想将模型应用于默写输入数据时，即我们只想通过网络进行前向计算。我们可以通过将计算代码加上 <code>torch.no_grad()</code> 还书，用来停止跟踪计算。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w) +b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad) <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w) + b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad) <span class="comment"># False</span></span><br></pre></td></tr></table></figure>

<p>另一种方法是在张量上使用 <code>detach()</code> 方法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w) + b</span><br><span class="line">z_det = z.detach()</span><br><span class="line"><span class="built_in">print</span>(z_det.requires_grad) <span class="comment"># False</span></span><br></pre></td></tr></table></figure>

<p>考虑要禁用梯度跟踪的情况：</p>
<ul>
<li>将神经网络中的某些参数标记为冻结参数</li>
<li>为了优化计算速度，在只进行前向传播时禁用梯度跟踪的选项</li>
</ul>
<h3 id="计算图部分扩展阅读"><a href="#计算图部分扩展阅读" class="headerlink" title="计算图部分扩展阅读"></a>计算图部分扩展阅读</h3><p>（留白，前面的区域以后再探索吧~）</p>
<h3 id="可选阅读：张量梯度-和-雅阁比积-Jacobian-Products"><a href="#可选阅读：张量梯度-和-雅阁比积-Jacobian-Products" class="headerlink" title="可选阅读：张量梯度 和 雅阁比积(Jacobian_Products)"></a>可选阅读：张量梯度 和 雅阁比积(Jacobian_Products)</h3><p>（留白，前面的区域以后再探索吧~）</p>
<h2 id="（重要）优化模型参数"><a href="#（重要）优化模型参数" class="headerlink" title="（重要）优化模型参数"></a>（重要）优化模型参数</h2><p>在拥有了模型和数据之后，就可以通过优化数据参数来训练、验证和测试我们的模型了。训练模型是一个带带过程，每次迭代中，模型都会对输出进行*<u>猜测</u>*，计算其猜测中的误差，计算其猜测中的误差（损失），收集误差对参数的导数（上一节中进行的工作），并使用梯度下降这些参数。</p>
<p>更详细的视频讲解，可以参考 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=tIeHLnjs5U8">backpropagation from 3Blue1Brown</a></p>
<h3 id="先决条件代码"><a href="#先决条件代码" class="headerlink" title="先决条件代码"></a>先决条件代码</h3><h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><h3 id="优化循环"><a href="#优化循环" class="headerlink" title="优化循环"></a>优化循环</h3><h3 id="全过程数据跟踪"><a href="#全过程数据跟踪" class="headerlink" title="全过程数据跟踪"></a>全过程数据跟踪</h3><h2 id="保存、加载和使用模型"><a href="#保存、加载和使用模型" class="headerlink" title="保存、加载和使用模型"></a>保存、加载和使用模型</h2><h3 id="保存-x2F-加载模型权重"><a href="#保存-x2F-加载模型权重" class="headerlink" title="保存&#x2F;加载模型权重"></a>保存&#x2F;加载模型权重</h3><h3 id="通过模型的形状参数进行保存-x2F-加载"><a href="#通过模型的形状参数进行保存-x2F-加载" class="headerlink" title="通过模型的形状参数进行保存&#x2F;加载"></a>通过模型的形状参数进行保存&#x2F;加载</h3><h2 id="在-PyTorch-中保存和加载常规-Checkpoint"><a href="#在-PyTorch-中保存和加载常规-Checkpoint" class="headerlink" title="在 PyTorch 中保存和加载常规 Checkpoint"></a>在 PyTorch 中保存和加载常规 Checkpoint</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><h3 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h3><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><h4 id="1-导入加载数据所需要的库"><a href="#1-导入加载数据所需要的库" class="headerlink" title="1.导入加载数据所需要的库"></a>1.导入加载数据所需要的库</h4><h4 id="2-定义和初始化神经网络"><a href="#2-定义和初始化神经网络" class="headerlink" title="2.定义和初始化神经网络"></a>2.定义和初始化神经网络</h4><h4 id="3-初始化优化器"><a href="#3-初始化优化器" class="headerlink" title="3.初始化优化器"></a>3.初始化优化器</h4><h4 id="4-保存常规检查点"><a href="#4-保存常规检查点" class="headerlink" title="4.保存常规检查点"></a>4.保存常规检查点</h4><h4 id="5-加载常规检查点"><a href="#5-加载常规检查点" class="headerlink" title="5.加载常规检查点"></a>5.加载常规检查点</h4><h2 id="从-Checkpoint-中加载-nn-Module-的技巧"><a href="#从-Checkpoint-中加载-nn-Module-的技巧" class="headerlink" title="从 Checkpoint 中加载 nn.Module 的技巧"></a>从 Checkpoint 中加载 <code>nn.Module</code> 的技巧</h2><h3 id="活用-torch-load-mmap-True"><a href="#活用-torch-load-mmap-True" class="headerlink" title="活用 torch.load(mmap = True)"></a>活用 <code>torch.load(mmap = True)</code></h3><h3 id="活用-torch-device-quot-meta-quot"><a href="#活用-torch-device-quot-meta-quot" class="headerlink" title="活用 torch.device(&quot;meta&quot;)"></a>活用 <code>torch.device(&quot;meta&quot;)</code></h3><h3 id="活用-load-state-dict-assign-True"><a href="#活用-load-state-dict-assign-True" class="headerlink" title="活用 load_state_dict(assign = True)"></a>活用 <code>load_state_dict(assign = True)</code></h3><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3>
      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://zade23.github.io/2023/10/31/pytorch-tutorial-official/" title="pytorch-tutorial-official" target="_blank" rel="external">https://zade23.github.io/2023/10/31/pytorch-tutorial-official/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/zade23" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/zade23" target="_blank"><span class="text-dark">Android</span><small class="ml-1x">Student &amp; Coder</small></a></h3>
        <div>Happy Coding!</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2023/11/07/%E7%8E%A9%E8%BD%ACDocker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="玩转Docker学习笔记"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2023/10/18/%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/" title="工具网站"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,wechat,facebook,twitter"></div>
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/zade23" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://weibo.com/u/5382156286" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        &copy; 2024 Android
        
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: '',
    appKey: '',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script>

     







</body>
</html>